[{"authors":["翟本祥"],"categories":null,"content":"","date":1757376000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1757376000,"objectID":"4a375a3af28d91dd09d2955f42507cf1","permalink":"http://localhost:1313/author/benxiang-zhai%E7%BF%9F%E6%9C%AC%E7%A5%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/benxiang-zhai%E7%BF%9F%E6%9C%AC%E7%A5%A5/","section":"authors","summary":"","tags":null,"title":"Benxiang Zhai(翟本祥)","type":"authors"},{"authors":["都思丹"],"categories":null,"content":"个人简历： 1997年毕业于南京大学，获博士学位。南京大学电子科学与工程学院电子工程系教授，图像工程实验室主任。电路与系统学科带头人，南京市321工程有突出贡献中青年专家。先后在日本富士通、美国西北大学访问研究， 曾挂职宜兴市科技副市长。主持承担了自然科学基金、国际合作重大项目，以及江苏省科技支撑计划等项目。发表SCI、EI学术论文100余篇。获授权国家发明专利20余项，江苏省科学技术三等奖、江苏省优秀软件产品奖，中国计算机行业发展成就奖（团体），日本富士通优秀业绩奖，南京大学青年骨干教师奖。\n研究方向： 图像处理与控制 机器学习与算法 密码与信息安全 主要课程： 本科生：概率论与随机过程，图像处理 研究生：信号处理中的数学方法，现代图像工程 ","date":1757376000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1757376000,"objectID":"aec4f54067fa5320f4f03fe6a507c3f7","permalink":"http://localhost:1313/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/","section":"authors","summary":"个人简历： 1997年毕业于南京大学，获博士学位。南京大学电子科学与工程学院电子工程系教授，图像工程实验室主任。电路与系统学科带头人，南京市321工程有突出贡献中青年专家。先后在日本富士通、美国西北大学访问研究， 曾挂职宜兴市科技副市长。主持承担了自然科学基金、国际合作重大项目，以及江苏省科技支撑计划等项目。发表SCI、EI学术论文100余篇。获授权国家发明专利20余项，江苏省科学技术三等奖、江苏省优秀软件产品奖，中国计算机行业发展成就奖（团体），日本富士通优秀业绩奖，南京大学青年骨干教师奖。\n研究方向： 图像处理与控制 机器学习与算法 密码与信息安全 主要课程： 本科生：概率论与随机过程，图像处理 研究生：信号处理中的数学方法，现代图像工程 ","tags":null,"title":"Sidan Du(都思丹)","type":"authors"},{"authors":["李杨"],"categories":null,"content":"在计算机视觉领域。我们的研究兴趣集中利用计算机视觉技术对环境进行三维感知和对环境图像进行语义理解。我们围绕着机器人导航、自动驾驶等应用场景下的多种三维感知技术进行研究。在双目及多目立体视觉被动感知技术方面，我们首次提出了利用帧间相关性加速视频序列中的立体匹配问题，该方法在不降低视差计算精度的前提可以显著提升基于深度学习和非深度学习方法的立体匹配速度，我们在嵌入式系统中实现的双目立体视觉系统达到了国际先进水平。我们同时在激光雷达点云的语义分割、配准上也取得众多成果，利用算法优化和GPU加速我们实现在低功耗嵌入式平台上的实时三维重建。针对机器人导航、自动驾驶等应用场景，我们进一步发展和完善了双目视觉的同步定位与地图构建问题，提出并实现了利用深度图像加速地图构建。此外，我们在目标识别、图像语义分析等领域也取得若干成果，例如我们针对多模态图像中的目标识别问题提出了全新的深度网络架构和数据增强及训练方法，显著提升了算法的性能。\n","date":1757376000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1757376000,"objectID":"f288ec240bf05ad2f6ed15f7234831d0","permalink":"http://localhost:1313/author/yang-li%E6%9D%8E%E6%9D%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yang-li%E6%9D%8E%E6%9D%A8/","section":"authors","summary":"在计算机视觉领域。我们的研究兴趣集中利用计算机视觉技术对环境进行三维感知和对环境图像进行语义理解。我们围绕着机器人导航、自动驾驶等应用场景下的多种三维感知技术进行研究。在双目及多目立体视觉被动感知技术方面，我们首次提出了利用帧间相关性加速视频序列中的立体匹配问题，该方法在不降低视差计算精度的前提可以显著提升基于深度学习和非深度学习方法的立体匹配速度，我们在嵌入式系统中实现的双目立体视觉系统达到了国际先进水平。我们同时在激光雷达点云的语义分割、配准上也取得众多成果，利用算法优化和GPU加速我们实现在低功耗嵌入式平台上的实时三维重建。针对机器人导航、自动驾驶等应用场景，我们进一步发展和完善了双目视觉的同步定位与地图构建问题，提出并实现了利用深度图像加速地图构建。此外，我们在目标识别、图像语义分析等领域也取得若干成果，例如我们针对多模态图像中的目标识别问题提出了全新的深度网络架构和数据增强及训练方法，显著提升了算法的性能。","tags":null,"title":"Yang Li(李杨)","type":"authors"},{"authors":["徐一舫"],"categories":null,"content":"这里是个人介绍\n","date":1757376000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1757376000,"objectID":"bce2f47b2f7cdd5915fb051fcf786371","permalink":"http://localhost:1313/author/yifang-xu%E5%BE%90%E4%B8%80%E8%88%AB/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yifang-xu%E5%BE%90%E4%B8%80%E8%88%AB/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Yifang Xu(徐一舫)","type":"authors"},{"authors":["李明"],"categories":null,"content":"","date":1749600000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1749600000,"objectID":"926f2044e1048ef84ac37cc73a1bf8ff","permalink":"http://localhost:1313/author/ming-li%E6%9D%8E%E6%98%8E/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ming-li%E6%9D%8E%E6%98%8E/","section":"authors","summary":"","tags":null,"title":"Ming Li(李明)","type":"authors"},{"authors":["武超凡"],"categories":null,"content":"","date":1744243200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1744243200,"objectID":"b8b1ecd5414e7585506248eb6669b394","permalink":"http://localhost:1313/author/chaofan-wu%E6%AD%A6%E8%B6%85%E5%87%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chaofan-wu%E6%AD%A6%E8%B6%85%E5%87%A1/","section":"authors","summary":"","tags":null,"title":"Chaofan Wu(武超凡)","type":"authors"},{"authors":["曹靖豪"],"categories":null,"content":"这里是个人介绍\n","date":1744243200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1744243200,"objectID":"da4ca16fe10d4dfa0e9d04f2315b9cb8","permalink":"http://localhost:1313/author/jinghao-cao%E6%9B%B9%E9%9D%96%E8%B1%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jinghao-cao%E6%9B%B9%E9%9D%96%E8%B1%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jinghao Cao(曹靖豪)","type":"authors"},{"authors":["刘晟"],"categories":null,"content":"这里是个人介绍\n","date":1744243200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1744243200,"objectID":"0e126dad231a8b33624f1ff0ad18c1a4","permalink":"http://localhost:1313/author/sheng-liu%E5%88%98%E6%99%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sheng-liu%E5%88%98%E6%99%9F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Sheng Liu(刘晟)","type":"authors"},{"authors":["戴京昭"],"categories":null,"content":"这里是个人介绍\n","date":1738195200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1738195200,"objectID":"3cfa55795f69a5a08c4fddfe7da5203f","permalink":"http://localhost:1313/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jingzhao Dai(戴京昭)","type":"authors"},{"authors":["胡雪娇"],"categories":null,"content":"","date":1735344000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1735344000,"objectID":"c1943d631536dada07c9deecb391dee1","permalink":"http://localhost:1313/author/xuejiao-hu%E8%83%A1%E9%9B%AA%E5%A8%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xuejiao-hu%E8%83%A1%E9%9B%AA%E5%A8%87/","section":"authors","summary":"","tags":null,"title":"Xuejiao Hu(胡雪娇)","type":"authors"},{"authors":["杨雄"],"categories":null,"content":"","date":1728950400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1728950400,"objectID":"cf8d3df0151b6542686b80091698c9a0","permalink":"http://localhost:1313/author/xiong-yang%E6%9D%A8%E9%9B%84/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiong-yang%E6%9D%A8%E9%9B%84/","section":"authors","summary":"","tags":null,"title":"Xiong Yang(杨雄)","type":"authors"},{"authors":["王师捷"],"categories":null,"content":"","date":1725148800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1725148800,"objectID":"4de19d0a657045458a77ad2bcde5977f","permalink":"http://localhost:1313/author/shijie-wang%E7%8E%8B%E5%B8%88%E6%8D%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shijie-wang%E7%8E%8B%E5%B8%88%E6%8D%B7/","section":"authors","summary":"","tags":null,"title":"Shijie Wang(王师捷)","type":"authors"},{"authors":["唐铁健"],"categories":null,"content":"喜欢羽毛球，游泳，看动漫，湖南伢子，特别能吃辣\n","date":1715558400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1715558400,"objectID":"e7488a3910324de9929d24ca2288bbc7","permalink":"http://localhost:1313/author/tiejian-tang%E5%94%90%E9%93%81%E5%81%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tiejian-tang%E5%94%90%E9%93%81%E5%81%A5/","section":"authors","summary":"喜欢羽毛球，游泳，看动漫，湖南伢子，特别能吃辣","tags":null,"title":"Tiejian Tang(唐铁健)","type":"authors"},{"authors":["王品智"],"categories":null,"content":"这里是个人介绍\n","date":1704499200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704499200,"objectID":"73214478b073efac6dd77142f312f503","permalink":"http://localhost:1313/author/pinzhi-wang%E7%8E%8B%E5%93%81%E6%99%BA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pinzhi-wang%E7%8E%8B%E5%93%81%E6%99%BA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Pinzhi Wang(王品智)","type":"authors"},{"authors":["谢子恩"],"categories":null,"content":"这里是个人介绍\n","date":1701648000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1701648000,"objectID":"b44123dade96c394f3042dcca6ad619e","permalink":"http://localhost:1313/author/zien-xie%E8%B0%A2%E5%AD%90%E6%81%A9/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zien-xie%E8%B0%A2%E5%AD%90%E6%81%A9/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zien Xie(谢子恩)","type":"authors"},{"authors":["帅江海"],"categories":null,"content":"这里是个人介绍\n","date":1700179200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1700179200,"objectID":"046f43ce15a0832fa8303702f4caa410","permalink":"http://localhost:1313/author/jianghai-shuai%E5%B8%85%E6%B1%9F%E6%B5%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianghai-shuai%E5%B8%85%E6%B1%9F%E6%B5%B7/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jianghai Shuai(帅江海)","type":"authors"},{"authors":["冯永康"],"categories":null,"content":"Feel free to contact me.\n","date":1700179200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1700179200,"objectID":"b48fa1359440151b6c79c3ea1b02bec2","permalink":"http://localhost:1313/author/yongkang-feng%E5%86%AF%E6%B0%B8%E5%BA%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yongkang-feng%E5%86%AF%E6%B0%B8%E5%BA%B7/","section":"authors","summary":"Feel free to contact me.","tags":null,"title":"Yongkang Feng(冯永康)","type":"authors"},{"authors":["朱治亦"],"categories":null,"content":"这里是个人介绍\n","date":1690416000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1690416000,"objectID":"262c22709d80c22fea8d72432065c3f0","permalink":"http://localhost:1313/author/zhiyi-zhu%E6%9C%B1%E6%B2%BB%E4%BA%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiyi-zhu%E6%9C%B1%E6%B2%BB%E4%BA%A6/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhiyi Zhu(朱治亦)","type":"authors"},{"authors":["靳学乾"],"categories":null,"content":"这里是个人介绍\n","date":1656806400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1656806400,"objectID":"d3f1f0cdfcb9e2e97fc2490fb6509516","permalink":"http://localhost:1313/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Xueqian Jin(靳学乾)","type":"authors"},{"authors":["王汉镕"],"categories":null,"content":"这里是个人介绍\n","date":1653177600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1653177600,"objectID":"4813d0567e2be557ca0e7e9adc939800","permalink":"http://localhost:1313/author/hanrong-wang%E7%8E%8B%E6%B1%89%E9%95%95/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hanrong-wang%E7%8E%8B%E6%B1%89%E9%95%95/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Hanrong Wang(王汉镕)","type":"authors"},{"authors":["王杰"],"categories":null,"content":"","date":1653177600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1653177600,"objectID":"5a4344f2cc5135e24172554221ed61e0","permalink":"http://localhost:1313/author/jie-wang%E7%8E%8B%E6%9D%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jie-wang%E7%8E%8B%E6%9D%B0/","section":"authors","summary":"","tags":null,"title":"Jie Wang(王杰)","type":"authors"},{"authors":["曹静怡"],"categories":null,"content":"这里是个人介绍\n","date":1651795200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1651795200,"objectID":"f6ff1b89fdfdcc22f3c6363d4db78b29","permalink":"http://localhost:1313/author/jingyi-cao%E6%9B%B9%E9%9D%99%E6%80%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingyi-cao%E6%9B%B9%E9%9D%99%E6%80%A1/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jingyi Cao(曹静怡)","type":"authors"},{"authors":["白珏"],"categories":null,"content":"这里是个人介绍\n","date":1646352000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1646352000,"objectID":"9bf3914ddb531aa38f7f52088e14376e","permalink":"http://localhost:1313/author/jue-bai%E7%99%BD%E7%8F%8F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jue-bai%E7%99%BD%E7%8F%8F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jue Bai(白珏)","type":"authors"},{"authors":["李兆旭"],"categories":null,"content":"这里是个人介绍\n","date":1646352000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1646352000,"objectID":"c7b764560c4c9d4d73658b276b5e454b","permalink":"http://localhost:1313/author/zhaoxu-li%E6%9D%8E%E5%85%86%E6%97%AD/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhaoxu-li%E6%9D%8E%E5%85%86%E6%97%AD/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhaoxu Li(李兆旭)","type":"authors"},{"authors":["黎琪"],"categories":null,"content":"这里是个人介绍\n","date":1617840000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1617840000,"objectID":"f1ca98276535d17eb6f055f3c436b962","permalink":"http://localhost:1313/author/qi-li%E9%BB%8E%E7%90%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qi-li%E9%BB%8E%E7%90%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Qi Li(黎琪)","type":"authors"},{"authors":["陈佟"],"categories":null,"content":"这里是个人介绍\n","date":1617840000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1617840000,"objectID":"2b82ab97fce568f00e121b3821dc3ca8","permalink":"http://localhost:1313/author/tong-chen%E9%99%88%E4%BD%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tong-chen%E9%99%88%E4%BD%9F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Tong Chen(陈佟)","type":"authors"},{"authors":["陈旭东"],"categories":null,"content":"经历了紫荆 pt、GoAgent 时代，开眼看世界。 经过几年不务正业，精通各式语言和框架，如 C++、Makefile、CMake、Bash、Python、Latex、Markdown、HTML、CSS、JavaScript、Lua、Java、Hive SQL，以及各样系统和软件，如 Windows、Ubuntu、Microsoft Office、Microsoft Visio、Adobe Acrobat、Adobe Photoshop、SAI，的擅长领域和文档查询。\n最近觉得 NAS 和 pt 有点意思，可惜紫荆已经关站。 强烈安利 BYRPT ，这是我有账号的几个 pt 站里还没关的，欢迎大家找我玩~~\n","date":1617840000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1617840000,"objectID":"4426c4f7800d9d6ba95dcc000a955da9","permalink":"http://localhost:1313/author/xudong-chen%E9%99%88%E6%97%AD%E4%B8%9C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xudong-chen%E9%99%88%E6%97%AD%E4%B8%9C/","section":"authors","summary":"经历了紫荆 pt、GoAgent 时代，开眼看世界。 经过几年不务正业，精通各式语言和框架，如 C++、Makefile、CMake、Bash、Python、Latex、Markdown、HTML、CSS、JavaScript、Lua、Java、Hive SQL，以及各样系统和软件，如 Windows、Ubuntu、Microsoft Office、Microsoft Visio、Adobe Acrobat、Adobe Photoshop、SAI，的擅长领域和文档查询。\n最近觉得 NAS 和 pt 有点意思，可惜紫荆已经关站。 强烈安利 BYRPT ，这是我有账号的几个 pt 站里还没关的，欢迎大家找我玩~~","tags":null,"title":"Xudong Chen(陈旭东)","type":"authors"},{"authors":["周子豪"],"categories":null,"content":"","date":1613952000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1613952000,"objectID":"0029828d3e9d8497ac33da8f4419d0e1","permalink":"http://localhost:1313/author/zihao-zhou%E5%91%A8%E5%AD%90%E8%B1%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zihao-zhou%E5%91%A8%E5%AD%90%E8%B1%AA/","section":"authors","summary":"","tags":null,"title":"Zihao Zhou(周子豪)","type":"authors"},{"authors":["董晨"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"a7697d83fcea15e78896b80c27fe2534","permalink":"http://localhost:1313/author/chen-dong%E8%91%A3%E6%99%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chen-dong%E8%91%A3%E6%99%A8/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Chen Dong(董晨)","type":"authors"},{"authors":["杨帆"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"08232687745d8a4d0349d036c8c5fcf3","permalink":"http://localhost:1313/author/fan-yang%E6%9D%A8%E5%B8%86/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/fan-yang%E6%9D%A8%E5%B8%86/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Fan Yang(杨帆)","type":"authors"},{"authors":["李嘉恒"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"27879f078458565628243f2aaf610067","permalink":"http://localhost:1313/author/jiaheng-li%E6%9D%8E%E5%98%89%E6%81%92/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaheng-li%E6%9D%8E%E5%98%89%E6%81%92/","section":"authors","summary":"","tags":null,"title":"Jiaheng Li(李嘉恒)","type":"authors"},{"authors":["郑嘉璇"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"c9ef1dac8c4b1e372ccc0cde72333de4","permalink":"http://localhost:1313/author/jiaxuan-zheng%E9%83%91%E5%98%89%E7%92%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaxuan-zheng%E9%83%91%E5%98%89%E7%92%87/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jiaxuan Zheng(郑嘉璇)","type":"authors"},{"authors":["吴佳昱"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"65834053bfc801d06ba84965cabacfc0","permalink":"http://localhost:1313/author/jiayu-wu%E5%90%B4%E4%BD%B3%E6%98%B1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiayu-wu%E5%90%B4%E4%BD%B3%E6%98%B1/","section":"authors","summary":"","tags":null,"title":"Jiayu Wu(吴佳昱)","type":"authors"},{"authors":["石立"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"7dc83e81e202bdc33b04b4500b2f77ab","permalink":"http://localhost:1313/author/li-shi%E7%9F%B3%E7%AB%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-shi%E7%9F%B3%E7%AB%8B/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Li Shi(石立)","type":"authors"},{"authors":["赵芮"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b3cc0ba2cc650ae09b0712998d97a0c3","permalink":"http://localhost:1313/author/rui-zhao%E8%B5%B5%E8%8A%AE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rui-zhao%E8%B5%B5%E8%8A%AE/","section":"authors","summary":"","tags":null,"title":"Rui Zhao(赵芮)","type":"authors"},{"authors":["陆胜"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"3e403ff20ed614d1376c068f35edcafb","permalink":"http://localhost:1313/author/sheng-lu%E9%99%86%E8%83%9C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sheng-lu%E9%99%86%E8%83%9C/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Sheng Lu(陆胜)","type":"authors"},{"authors":["许薯文"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"56100b1701554d958fb2e4677d931f4c","permalink":"http://localhost:1313/author/shuwen-xu%E8%AE%B8%E8%96%AF%E6%96%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuwen-xu%E8%AE%B8%E8%96%AF%E6%96%87/","section":"authors","summary":"","tags":null,"title":"Shuwen Xu(许薯文)","type":"authors"},{"authors":["陆天昊"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"0700c0266630f9e8f4f9e155cd550b42","permalink":"http://localhost:1313/author/tianhao-lu%E9%99%86%E5%A4%A9%E6%98%8A/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianhao-lu%E9%99%86%E5%A4%A9%E6%98%8A/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Tianhao Lu(陆天昊)","type":"authors"},{"authors":["蔡文聪"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"47e5a4405627ad6402085ce1bf9b43b1","permalink":"http://localhost:1313/author/wencong-cai%E8%94%A1%E6%96%87%E8%81%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wencong-cai%E8%94%A1%E6%96%87%E8%81%AA/","section":"authors","summary":"","tags":null,"title":"Wencong Cai(蔡文聪)","type":"authors"},{"authors":["王祥祥"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"6c3e5110295a92172e68ef64afccbd71","permalink":"http://localhost:1313/author/xiangxiang-wang%E7%8E%8B%E7%A5%A5%E7%A5%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiangxiang-wang%E7%8E%8B%E7%A5%A5%E7%A5%A5/","section":"authors","summary":"","tags":null,"title":"Xiangxiang Wang(王祥祥)","type":"authors"},{"authors":["陈叶朦"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"3d41a0d98761fb8f5c83f8dfe252ef6b","permalink":"http://localhost:1313/author/yemeng-chen%E9%99%88%E5%8F%B6%E6%9C%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yemeng-chen%E9%99%88%E5%8F%B6%E6%9C%A6/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"yemeng Chen(陈叶朦)","type":"authors"},{"authors":["马依航"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"70eff90bc88e20830640ad5e398071bd","permalink":"http://localhost:1313/author/yihang-ma%E9%A9%AC%E4%BE%9D%E8%88%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yihang-ma%E9%A9%AC%E4%BE%9D%E8%88%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Yihang Ma(马依航)","type":"authors"},{"authors":["侯悦"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"d9c52025fb4cd7669dca17895012e9be","permalink":"http://localhost:1313/author/yue-hou%E4%BE%AF%E6%82%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yue-hou%E4%BE%AF%E6%82%A6/","section":"authors","summary":"","tags":null,"title":"Yue Hou(侯悦)","type":"authors"},{"authors":["凌煜清"],"categories":null,"content":"本人性格开朗，热爱探索，从事视频AI算法工作数年，对视觉AI方面的前沿技术抱有浓厚兴趣。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"40ddcfad620fcd4960329a95986e940d","permalink":"http://localhost:1313/author/yuqing-ling%E5%87%8C%E7%85%9C%E6%B8%85/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuqing-ling%E5%87%8C%E7%85%9C%E6%B8%85/","section":"authors","summary":"本人性格开朗，热爱探索，从事视频AI算法工作数年，对视觉AI方面的前沿技术抱有浓厚兴趣。","tags":null,"title":"Yuqing Ling(凌煜清)","type":"authors"},{"authors":["朱雨秋"],"categories":null,"content":"南京大学电子科学与工程学院2024级研究生\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"2a8cdc1f77113e2dab663bdc2a6857c5","permalink":"http://localhost:1313/author/yuqiu-zhu%E6%9C%B1%E9%9B%A8%E7%A7%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuqiu-zhu%E6%9C%B1%E9%9B%A8%E7%A7%8B/","section":"authors","summary":"南京大学电子科学与工程学院2024级研究生","tags":null,"title":"Yuqiu Zhu(朱雨秋)","type":"authors"},{"authors":["王之渊"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b6bb3ab316146419e470e248d397a13f","permalink":"http://localhost:1313/author/zhiyuan-wang%E7%8E%8B%E4%B9%8B%E6%B8%8A/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiyuan-wang%E7%8E%8B%E4%B9%8B%E6%B8%8A/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhiyuan Wang(王之渊)","type":"authors"},{"authors":["高梓航"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"01d774f2f2748cdea7dc9aa1e5e9f3e1","permalink":"http://localhost:1313/author/zihang-gao%E9%AB%98%E6%A2%93%E8%88%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zihang-gao%E9%AB%98%E6%A2%93%E8%88%AA/","section":"authors","summary":"","tags":null,"title":"Zihang Gao(高梓航)","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"http://localhost:1313/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["翟本祥","徐一舫","Guofeng Zhang","李杨","都思丹"],"categories":null,"content":"","date":1757376000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1757376000,"objectID":"fe00340b955da7bce0f382e53dc6d90c","permalink":"http://localhost:1313/publication/facesnap/","publishdate":"2025-09-09T00:00:00Z","relpermalink":"/publication/facesnap/","section":"publication","summary":"Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.","tags":["Source Themes"],"title":"FaceSnap: Enhanced ID-Fidelity Network forTuning-Free Portrait Customization","type":"publication"},{"authors":["徐一舫","翟本祥","孙运卓","李明","李杨","都思丹"],"categories":null,"content":"","date":1749600000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1749600000,"objectID":"2c6e33b9c3604a3e5ee30e3db7d4cd0d","permalink":"http://localhost:1313/publication/hifi-portrait/","publishdate":"2025-06-11T00:00:00Z","relpermalink":"/publication/hifi-portrait/","section":"publication","summary":"Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.","tags":["Source Themes"],"title":"HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion","type":"publication"},{"authors":["曹靖豪","刘晟","武超凡","李杨","都思丹"],"categories":null,"content":"","date":1744243200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1744243200,"objectID":"74c435639e41ec56db2b3b2f39feb5a0","permalink":"http://localhost:1313/publication/athena/","publishdate":"2025-04-10T00:00:00Z","relpermalink":"/publication/athena/","section":"publication","summary":"Large language models have brought revolutionary changes to autonomous driving algorithms, ushering them into the era of multimodality. However, existing vehicle trajectory planning methods primarily focus on obstacle avoidance in autonomous driving scenarios, overlooking interactions with entities within the scene, such as humans. In this letter, we propose a new research direction:vehicle trajectory planning that takes into account human actions. We establish ATHENA, the first autonomous driving dataset that integrates multimodal human actions, comprising 33,855 scenarios. Each scenario contains status information about ego vehicle, as well as pedestrian actions that actively or passively interact with the vehicle, such as signaling the vehicle to proceed by waving and the unexpected falls by pedestrians that force the vehicle to stop. Based on each type of interaction, ATHENA also provides the corresponding driving suggestions and the reasons behind them. Moreover, we present an LLM-based baseline that consists of two agents:the Action Understanding Agent and the Vehicle Control Agent. Our baseline implements the generation of driving recommendations and vehicle control functions, which are guided by pedestrian actions. Experiments demonstrate the effectiveness and strong performance of our method.","tags":["Source Themes"],"title":"ATHENA - Autonomous Vehicle Trajectory Planning Considered Human Action Awareness","type":"publication"},{"authors":["徐一舫","孙运卓","翟本祥","Wenxin Liang","李杨","都思丹"],"categories":null,"content":"","date":1740441600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1740441600,"objectID":"c6e1d619e1c4c6b09fcdc6bdd5331e77","permalink":"http://localhost:1313/publication/zero-shot/","publishdate":"2025-02-25T00:00:00Z","relpermalink":"/publication/zero-shot/","section":"publication","summary":"The target of video moment retrieval (VMR) is predicting temporal spans within a video that semantically match a given linguistic query. Existing VMR methods based on multimodal large language models (MLLMs) overly rely on expensive high-quality datasets and time-consuming fine-tuning. Although some recent studies introduce a zero-shot setting to avoid fine-tuning, they overlook inherent language bias in the query, leading to erroneous localization. To tackle the aforementioned challenges, this paper proposes Moment-GPT, a tuning-free pipeline for zero-shot VMR utilizing frozen MLLMs. Specifically, we first employ LLaMA-3 to correct and rephrase the query to mitigate language bias. Subsequently, we design a span generator combined with MiniGPT-v2 to produce candidate spans adaptively. Finally, to leverage the video comprehension capabilities of MLLMs, we apply VideoChatGPT and span scorer to select the most appropriate spans. Our proposed method substantially outperforms the state-ofthe-art MLLM-based and zero-shot models on several public datasets, including QVHighlights, ActivityNet-Captions, and Charades-STA.","tags":["Source Themes"],"title":" Zero-shot Video Moment Retrieval via Off-the-shelf Multimodal Large Language Models","type":"publication"},{"authors":["戴京昭","李杨","都思丹"],"categories":null,"content":"","date":1738195200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1738195200,"objectID":"4bd4f7ec81892a8dcb54b4c45b44b26a","permalink":"http://localhost:1313/publication/the-patch-based-multi-task/","publishdate":"2025-01-30T00:00:00Z","relpermalink":"/publication/the-patch-based-multi-task/","section":"publication","summary":"In this paper, we propose a global gaze following method using the patched‐based multi‐task multi‐scale reborn network (MMRGaze360) specifically designed for panorama images. Unlike existing approaches that rely on spherical networks or process only local regions, our architecture thoroughly accounts for the distortions introduced by the sphere‐to‐plane projection, enabling gaze following in comprehensive 360‐degree images. MMRGaze360 incorporates field‐of‐view (360‐FoV) and sight line (360‐Gaze) generators to model gaze behaviours and scene information in 360‐degree images. A multi‐task multi‐scale module is introduced to capture features from multiple patches centred around the estimated points located in the 360‐Gaze, using multi‐scale attention maps. These features, along with the 360‐FoV, are fused to produce a final heatmap. Additionally, we employ multi‐layer perceptions and convolutional networks using the reborn mechanism to enhance information usage and feature representation. Moreover, we establish a novel dataset, SRGaze360, which contains more conditions of the sphere‐to‐plane distortion. Experimental results on the GazeFollow360 and SRGaze360 datasets demonstrate the superiority of our method over previous works. It can be validated that our approach effectively addresses the limitations of 2D gaze following in handling out‐of‐frame gaze positions and distortions in 360‐degree images.","tags":["Source Themes"],"title":"HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion","type":"publication"},{"authors":["徐一舫","Chenyu Zhang","翟本祥","都思丹"],"categories":null,"content":"","date":1737936000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1737936000,"objectID":"2a4d51cacd71875b26183cffed344d74","permalink":"http://localhost:1313/publication/hp3/","publishdate":"2025-01-27T00:00:00Z","relpermalink":"/publication/hp3/","section":"publication","summary":"Portrait personalization (PP) has garnered considerable attention recently due to its potential applications. However, existing methods only preserve the face region, with limited capability to customize head attributes, which restricts their practicality. To address these challenges, we introduce HP3, the first head-preserving framework for zero-shot PP. It can generate realistic portraits that preserve the source head, while controlling head expression and pose through the driving image. To accomplish this, we first design the head encoder and 3D reconstruction module to obtain head features and 3D priors. Next, the head controller is devised to align them, producing 3D-aware head conditions. These conditions are injected into UNet via the adaptive connector to achieve superior head preservation and control. Qualitative and quantitative experiments demonstrate that HP3 significantly outperforms SOTA methods. Our project is available at https://github.com/YoucanBaby/HP3.","tags":["Source Themes"],"title":"HP3: Tuning-Free Head-Preserving Portrait Personalization Via 3D-Controlled Diffusion Models","type":"publication"},{"authors":["胡雪娇","戴京昭","李明","李杨","都思丹"],"categories":null,"content":"","date":1735344000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1735344000,"objectID":"eeac99db29f9c2806d7d29f064700403","permalink":"http://localhost:1313/publication/temporal-action-detection/","publishdate":"2024-12-28T00:00:00Z","relpermalink":"/publication/temporal-action-detection/","section":"publication","summary":"Temporal action detection is a fundamental yet challenging task in video understanding. It is important to process the action proposals for action classification and temporal boundary localization. Some methods process action proposals by exploiting the relations between them. However, learning the relations between numerous action proposals is time-consuming and requires huge computation and memory storage. Each proposal contains contextual information extracted from video segments, and redundant information aggregation has a negative impact on the final detection performance. In this paper, we exploit an efficient model which processes each proposal individually and learn intra-proposal features adequately, avoiding the interference of redundant information to achieve more effective detection. We also design relational learning models based on mean pooling, self-attention, and temporal convolution to compare with the intra-proposal learning model. Extensive experiments show that our method outperforms the relation learning models and achieves competitive performance on the two standard benchmarks. Moreover, efficiency experiments also verify that our model is more efficient than the relation learning methods.","tags":["Source Themes"],"title":"An efficient action proposal processing approach for temporal action detection","type":"publication"},{"authors":["曹靖豪","李明","刘晟","李杨","都思丹"],"categories":null,"content":"","date":1732147200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1732147200,"objectID":"d5d3af87b26a658156da8d083013b751","permalink":"http://localhost:1313/publication/cassc/","publishdate":"2024-11-21T00:00:00Z","relpermalink":"/publication/cassc/","section":"publication","summary":"Semantic scene completion is a crucial end-to-end 3D perception task, and the 3D information perception subjects is vital for autonomous driving. This paper presents CASSC, a novel adaptive context-aware method based on Transformer networks, aimed at realizing camera-based semantic scene completion algorithms. The key idea is to leverage rich context information from images to obtain pixel-level label proposals, followed by designing a multiscale fusion mechanism to merge this information and match it with voxel space. A weakly supervised training strategy is proposed to obtain semantic label distribution features from images and introduce an adaptive multiscale fusion module to fuse and adaptively match these features with voxel space. Here, CASSC achieves state-of-the-art performance on the SemanticKITTI dataset and demonstrates excellent performance on the SSC-Bench dataset. Ablation experiments validate the rationality and effectiveness of our design, and the model and code of CASSC will be open-sourced on https://github.com/dogooooo/CASSC.","tags":["Source Themes"],"title":"CASSC: Context-aware method for depth guided semantic scene completion","type":"publication"},{"authors":null,"categories":null,"content":"","date":1730160000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1730160000,"objectID":"efb4f93d86e7ae8f84dfe52fdffa3267","permalink":"http://localhost:1313/post/25-01-03-nianhui/","publishdate":"2024-10-29T00:00:00Z","relpermalink":"/post/25-01-03-nianhui/","section":"post","summary":"","tags":null,"title":"我们实验室在2025年1月3日举办实验室年会！","type":"post"},{"authors":null,"categories":null,"content":"科研之余注意身体健康\n","date":1730160000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1730160000,"objectID":"d5d983bf2b8dd158db0968e24a189f87","permalink":"http://localhost:1313/post/24-10-29-sports-meeting/","publishdate":"2024-10-29T00:00:00Z","relpermalink":"/post/24-10-29-sports-meeting/","section":"post","summary":"科研之余注意身体健康\n","tags":null,"title":"我们实验室在方肇周体育场举办趣味运动会！","type":"post"},{"authors":["曹靖豪","刘晟","杨雄","李杨","都思丹"],"categories":null,"content":"","date":1728950400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1728950400,"objectID":"931ed0c8acaf63ea4e4bcd4da76cfb0a","permalink":"http://localhost:1313/publication/ares/","publishdate":"2024-10-15T00:00:00Z","relpermalink":"/publication/ares/","section":"publication","summary":"The large-scale generation of real-world scenario datasets is a pivotal task in the field of autonomous driving. Existing methods emphasize solely on single-frame rendering, which need complex inputs for continuous scenario rendering. In this letter, ARES:a text-driven automatic realistic simulator is proposed, which can generate extensive realistic datasets with just a single text input. Its core idea is to generate vehicle trajectories based on the textual description, and then render the scenario by vehicle attributes associated with these trajectories. For learning trajectories generating, supervisory signal temporal logic is proposed to assist conditional diffusion model, which incorporates prior physical information. We annotate textual descriptions for KITTI-MOT dataset and establish an objective quantitative evaluation system. The superiority of our method is demonstrated by its high performance, which is reflected in a matching score of 3.54 and an FID of 8.93in the trajectory reconstruction task, along with a speed accuracy of 0.99 and a direction accuracy of 0.93in the trajectory editing task. The scenarios rendered by the proposed method exhibit high quality and realism, which indicates its great potential in testing of autonomous driving algorithms with vehicle-in-the-loop simulations.","tags":["Source Themes"],"title":"ARES: Text-Driven Automatic Realistic Simulator for Autonomous Traffic","type":"publication"},{"authors":["王师捷","胡雪娇","刘晟","李明","李杨","都思丹"],"categories":null,"content":"","date":1725148800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1725148800,"objectID":"2b6979e2bcf58bd60f29ccb2917a2d78","permalink":"http://localhost:1313/publication/tig/","publishdate":"2024-09-01T00:00:00Z","relpermalink":"/publication/tig/","section":"publication","summary":"Detecting key frames in videos has garnered substantial attention in recent years, it is a point-level task and has deep research value and application prospect in daily life. For instances, video surveillance system, video cover generation and highlight moment flashback all demands the technique of key frame detection. However, the task is beset by challenges such as the sparsity of key frame instances, imbalances between target frames and background frames, and the absence of post-processing method. In response to these problems, we introduce a novel and effective Temporal Interval Guided (TIG) framework to precisely localize specific frames. The framework is incorporated with a proposed Point-Level-Soft non-maximum suppression (PLS-NMS) post-processing algorithm which is suitable for point-level task, facilitated by the well-designed confidence score decay function. Furthermore, we propose a TIG-loss, exhibiting sensitivity to temporal interval from target frame, to optimize the two-stage framework. The proposed method can be broadly applied to key frame detection in video understanding, including action start detection and static video summarization. Extensive experimentation validates the efficacy of our approach on action start detection benchmark datasets:THUMOS’14 and Activitynet v1.3, and we have reached state-of-the-art performance. Competitive results are also demonstrated on SumMe and TVSum datasets for deep learning based static video summarization.","tags":["Source Themes"],"title":"TIG: A Multitask Temporal Interval Guided Framework for Key Frame Detection","type":"publication"},{"authors":["徐一舫","Yunzhuo Sun","翟本祥","Youyao Jia","都思丹"],"categories":null,"content":"","date":1719705600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1719705600,"objectID":"e8de2fd3a85298788674dee7d0e6c7d7","permalink":"http://localhost:1313/publication/mh-detr/","publishdate":"2024-06-30T00:00:00Z","relpermalink":"/publication/mh-detr/","section":"publication","summary":"With the increasing demand for video understanding, video moment and highlight detection (MHD) has emerged as a critical research topic. MHD aims to localize all moments and predict clip-wise saliency scores simultaneously. Despite progress made by existing DETR-based methods, we observe that these methods coarsely fuse features from different modalities, which weakens the temporal intra-modal context and results in insufficient cross-modal interaction. To address this issue, we propose MH-DETR (Moment and Highlight DEtection TRansformer) tailored for MHD. Specifically, we introduce a simple yet efficient pooling operator within the uni-modal encoder to capture global intra-modal context. Moreover, to obtain temporally aligned cross-modal features, we design a plug-and-play cross-modal interaction module between the encoder and decoder, seamlessly integrating visual and textual features. Comprehensive experiments on QVHighlights, Charades-STA, Activity-Net, and TVSum datasets show that MH-DETR outperforms existing state-of-the-art methods, demonstrating its effectiveness and superiority. Our code is available at https://github.com/YoucanBaby/MH-DETR.","tags":["Source Themes"],"title":"MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer","type":"publication"},{"authors":["曹靖豪","杨雄","刘晟","唐铁健","李杨","都思丹"],"categories":null,"content":"","date":1715558400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1715558400,"objectID":"76b847d4a029314e4193337f0d215be5","permalink":"http://localhost:1313/publication/dpcalib/","publishdate":"2025-05-13T00:00:00Z","relpermalink":"/publication/dpcalib/","section":"publication","summary":"The precise calibration of a LiDAR-camera system is a crucial prerequisite for multimodal 3D information fusion in perception systems. The accuracy and robustness of existing traditional offline calibration methods are inferior to methods based on deep learning. Meanwhile, most parameter regression-based online calibration methods directly project LiDAR data onto a specific plane, leading to information loss and perceptual limitations. A novel network, DPCalib, a dual perspective view network that mitigates the aforementioned issue, is proposed in this paper. This paper proposes a novel neural network architecture to achieve the fusion and reuse of input information. We design a feature encoder that effectively extracts features from two orthogonal views using attention mechanisms. Furthermore, we propose an effective decoder that aggregates features from two views, thereby obtaining accurate extrinsic parameter estimation outputs. The experimental results demonstrate that our approach outperforms existing SOTA methods, and the ablation experiments validate the rationality and effectiveness of our work.","tags":["Source Themes"],"title":"DPCalib: Dual-Perspective View Network for LiDAR-Camera Joint Calibration","type":"publication"},{"authors":["唐铁健","曹靖豪","杨雄","刘晟","Dongsheng Zhu","都思丹","李杨"],"categories":null,"content":"","date":1713139200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1713139200,"objectID":"72a7442c22b5a0bee6c71028fee60fd0","permalink":"http://localhost:1313/publication/real-time-railway/","publishdate":"2024-04-15T00:00:00Z","relpermalink":"/publication/real-time-railway/","section":"publication","summary":"Railway track detection, which is crucial for train operational safety, faces numerous challenges such as the curved track, obstacle occlusion, and vibrations during the train’s operation. Most existing methods for railway track detection use a camera or LiDAR. However, the vision-based approach lacks essential 3D environmental information about the train, while the LiDAR-based approach tends to detect tracks of insufficient length due to the inherent limitations of LiDAR. In this study, we propose a real-time method for railway track detection and 3D fitting based on camera and LiDAR fusion sensing. Semantic segmentation of the railway track in the image is performed, followed by inverse projection to obtain 3D information of the distant railway track. Then, 3D fitting is applied to the inverse projection of the railway track for track vectorization and LiDAR railway track point segmentation. The extrinsic parameters necessary for inverse projection are continuously optimized to ensure robustness against variations in extrinsic parameters during the train’s operation. Experimental results show that the proposed method achieves desirable accuracy for railway track detection and 3D fitting with acceptable computational efficiency, and outperforms existing approaches based on LiDAR, camera, and camera–LiDAR fusion. To the best of our knowledge, our approach represents the first successful attempt to fuse camera and LiDAR data for real-time railway track detection and 3D fitting.","tags":["Source Themes"],"title":"A Real-Time Method for Railway Track Detection and 3D Fitting Based on Camera and LiDAR Fusion Sensing","type":"publication"},{"authors":["胡雪娇","王师捷","李明","李杨","都思丹"],"categories":null,"content":"","date":1710288000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1710288000,"objectID":"96a7097e3b2a78fefbae228a6f72fc45","permalink":"http://localhost:1313/publication/time-attentive-fusion-network/","publishdate":"2024-03-13T00:00:00Z","relpermalink":"/publication/time-attentive-fusion-network/","section":"publication","summary":"Online detection of action start is a significant and challenging task that requires prompt identification of action start positions and corresponding categories within streaming videos. This task presents challenges due to data imbalance, similarity in boundary content, and real-time detection requirements. Here, a novel Time-Attentive Fusion Network is introduced to address the requirements of improved action detection accuracy and operational efficiency. The time-attentive fusion module is proposed, which consists of long-term memory attention and the fusion feature learning mechanism, to improve spatial-temporal feature learning. The temporal memory attention mechanism captures more effective temporal dependencies by employing weighted linear attention. The fusion feature learning mechanism facilitates the incorporation of current moment action information with historical data, thus enhancing the representation. The proposed method exhibits linear complexity and parallelism, enabling rapid training and inference speed. This method is evaluated on two challenging datasets:THUMOS’14 and ActivityNet v1.3. The experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art methods in terms of both detection accuracy and inference speed.","tags":["Source Themes"],"title":"Time-attentive fusion network: An efficient model for online detection of action start","type":"publication"},{"authors":["胡雪娇","王师捷","李明","李杨","都思丹"],"categories":null,"content":"","date":1704844800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704844800,"objectID":"b9a4bb8bc81c9c01f82dd92e463162fb","permalink":"http://localhost:1313/publication/online-detection-of-action/","publishdate":"2024-01-10T00:00:00Z","relpermalink":"/publication/online-detection-of-action/","section":"publication","summary":"The Online Detection of Action Start (ODAS) has attracted the attention of researchers because of its practical applications in areas such as security and emergency response. However, online detection of activity boundaries remains a challenging task due to the inherent ambiguity of boundary definition and the significant imbalance in the number of boundaries and nonboundary points. To address this issue, this study proposes a novel Distribution-aware Activity Boundary Representation (DABR) method that utilizes a continuous probability density function to smooth the probability of moments near activity boundaries. The proposed DABR reduces the penalty for detecting moments near ground-truth boundary points, while increasing the number of samples related to boundary points. Additionally, we introduce a two-stage framework that incorporates class-informed information in temporal localization for more efficient activity boundary localization. Extensive experiments demonstrate that our method achieves state-of-the-art results on two standard datasets, particularly exhibiting a significant improvement of 11.5% at average p-mAP on the THUMOS'14 dataset.","tags":["Source Themes"],"title":"Distribution-aware Activity Boundary Representation for Online Detection of Action Start in Untrimmed Videos","type":"publication"},{"authors":null,"categories":null,"content":"恭喜胡雪娇！\n论文详情\n","date":1704844800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704844800,"objectID":"123638f7a92e5e1f79e758c0cb94b399","permalink":"http://localhost:1313/post/24-01-10-ieeespl-accept/","publishdate":"2024-01-10T00:00:00Z","relpermalink":"/post/24-01-10-ieeespl-accept/","section":"post","summary":"恭喜胡雪娇！\n","tags":null,"title":"我们的文章被 IEEE Signal Processing Letters 接收！","type":"post"},{"authors":["王品智","李明","曹靖豪","都思丹","李杨"],"categories":null,"content":"","date":1704499200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704499200,"objectID":"467374eb732d2f12d5e51be3f4c39b44","permalink":"http://localhost:1313/publication/casomnimvs/","publishdate":"2024-01-06T00:00:00Z","relpermalink":"/publication/casomnimvs/","section":"publication","summary":"Estimating 360° depth from multiple cameras has been a challenging problem. However, existing methods often adopt a fixed-step spherical sweeping approach with densely sampled spheres and use numerous 3D convolutions in networks, which limits the speed of algorithms in practice. Additionally, obtaining high-precision depth maps of real scenes poses a challenge for the existing algorithms. In this paper, we design a cascade architecture using a dynamic spherical sweeping method that progressively refines the depth estimation from coarse to fine over multiple stages. The proposed method adaptively adjusts sweeping intervals and ranges based on the predicted depth and the uncertainty from the previous stage, resulting in a more efficient cost aggregation performance. The experimental results demonstrated that our method achieved state-of-the-art accuracy with reduced GPU memory usage and time consumption compared to the other methods. Furthermore, we illustrate that our method achieved satisfactory performance on real-world data, despite being trained on synthetic data, indicating its generalization potential and practical applicability.","tags":["Source Themes"],"title":"CasOmniMVS: Cascade Omnidirectional Depth Estimation with Dynamic Spherical Sweeping","type":"publication"},{"authors":["Yunzhuo Sun","徐一舫","谢子恩","Yukun Shu","都思丹"],"categories":null,"content":"","date":1701648000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1701648000,"objectID":"de3c3a0640924567226463b1a32dad4c","permalink":"http://localhost:1313/publication/gptsee/","publishdate":"2023-12-04T00:00:00Z","relpermalink":"/publication/gptsee/","section":"publication","summary":"Moment retrieval (MR) and highlight detection (HD) aim to identify relevant moments and highlights in video from corresponding natural language query. Large language models (LLMs) have demonstrated proficiency in various computer vision tasks. However, existing methods for MR\u0026HD have not yet been integrated with LLMs. In this letter, we propose a novel two-stage model that takes the output of LLMs as the input to the second-stage transformer encoder-decoder. First, MiniGPT-4 is employed to generate the detailed description of the video frame and rewrite the query statement, fed into the encoder as new features. Then, semantic similarity is computed between the generated description and the rewritten queries. Finally, continuous high-similarity video frames are converted into span anchors, serving as prior position information for the decoder. Experiments demonstrate that our approach achieves a state-of-the-art result, and by using only span anchors and similarity scores as outputs, positioning accuracy outperforms traditional methods, like Moment-DETR.","tags":["Source Themes"],"title":"GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features","type":"publication"},{"authors":["帅江海","李明","冯永康","李杨","都思丹"],"categories":null,"content":"","date":1700179200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1700179200,"objectID":"fec53cb389d41152c2d9c3e94deb752b","permalink":"http://localhost:1313/publication/monocular/","publishdate":"2023-11-17T00:00:00Z","relpermalink":"/publication/monocular/","section":"publication","summary":"In the field of computer vision, monocular depth estimation has garnered significant attention as a research direction. However, current depth estimation methods often overlook the impact of depth range variations in indoor and outdoor scenes, consequently limiting the model’s generalization ability. To achieve high-precision depth estimation across different depth ranges, we propose a new method. We employ the pretrained model Dinov2 as encoder, combined with decoder based on CNN architecture, to enhance the network’s capacity for extracting global information from indoor and outdoor scenes. Also, we design a mapping module to transform diverse depth ranges into a unified 0-1 range, which can effectively adapt to indoor and outdoor scenes. We validate our method on the DIODE dataset, which comprises mixed indoor and outdoor scenes. Experimental results demonstrate that our method achieves higher depth estimation accuracy and stronger generalization performance when dealing with scenes of diverse depth ranges.","tags":["Source Themes"],"title":"A Monocular Depth Estimation Method for Indoor-Outdoor Scenes Based on Vision Transformer","type":"publication"},{"authors":["朱治亦","刘晟","帅江海","都思丹","李杨"],"categories":null,"content":"","date":1690416000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1690416000,"objectID":"9fe793a4a1424ef8e0769f12ccdf4f45","permalink":"http://localhost:1313/publication/3d-associative-embedding/","publishdate":"2023-07-27T00:00:00Z","relpermalink":"/publication/3d-associative-embedding/","section":"publication","summary":"Most of the existing multi-view multi-person 3D human pose estimation methods predict the location of each joint of one target person following a top-down paradigm after finding his region. However, these works neglect the interference of others’ joints in the region. When the scene is crowded and the target person is surrounded by others, the information of his joints tends to be disturbed which results in significant errors in 3D results. To overcome this problem, this paper takes advantage of a bottom-up method in 2D pose estimation. We incorporate the Associative Embedding method into 3D pose estimation and propose a Voxel Hourglass Network to predict 3D heatmaps along with 3D tag-maps. As a result, the adverse effects from surrounding persons can be eliminated through the difference between tags. Moreover, we design a three-stage coarse-to-fine framework which can effectively reduce the quantization error. The size of the search space drops at each stage while the resolution increases. We test our method on the CMU Panoptic dataset where it outperforms the related top-down methods.","tags":["Source Themes"],"title":"3D Associative Embedding: Multi-View 3D Human Pose Estimation in Crowded Scenes","type":"publication"},{"authors":["戴京昭","李明","胡雪娇","李杨","都思丹"],"categories":null,"content":"","date":1686873600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1686873600,"objectID":"1daabdf0a7eae17201085e12fcac5aa2","permalink":"http://localhost:1313/publication/gazefollowtr-a-method-of-gaze-following-with-reborn-mechanism/","publishdate":"2023-06-16T00:00:00Z","relpermalink":"/publication/gazefollowtr-a-method-of-gaze-following-with-reborn-mechanism/","section":"publication","summary":"Most existing 3D pose representations cannot completely decouple the overlapping two or more human joints of the same type. In this paper, the authors propose a novel 2.5 D representation of the human pose by projecting human joints in 3D space onto the three orthogonal planes. The authors apply for the first time the permutation module to a multi-person 3D human pose estimation task and use Geometric Constraints Loss (GCL) to guide the learning of the model. The authors overcome the negative effects of the inductive bias of convolutional neural networks (CNNs) by aligning the intermediate feature space with the output feature space. The effectiveness of the authors’ approach is validated on the carnegie mellon university (CMU) panoptic dataset and MuPoTS-3D dataset. The authors’ proposed representations can effectively decouple the human joints in their selected data from overlapping human joints.","tags":["Source Themes"],"title":"GazeFollowTR: A Method of Gaze Following with Reborn Mechanism","type":"publication"},{"authors":["刘晟","帅江海","李杨","都思丹"],"categories":null,"content":"","date":1678924800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1678924800,"objectID":"d5167a6a818f063476969201bc7cf6e4","permalink":"http://localhost:1313/publication/mmda/","publishdate":"2023-03-16T00:00:00Z","relpermalink":"/publication/mmda/","section":"publication","summary":"Most existing 3D pose representations cannot completely decouple the overlapping two or more human joints of the same type. In this paper, the authors propose a novel 2.5 D representation of the human pose by projecting human joints in 3D space onto the three orthogonal planes. The authors apply for the first time the permutation module to a multi-person 3D human pose estimation task and use Geometric Constraints Loss (GCL) to guide the learning of the model. The authors overcome the negative effects of the inductive bias of convolutional neural networks (CNNs) by aligning the intermediate feature space with the output feature space. The effectiveness of the authors’ approach is validated on the carnegie mellon university (CMU) panoptic dataset and MuPoTS-3D dataset. The authors’ proposed representations can effectively decouple the human joints in their selected data from overlapping human joints.","tags":["Source Themes"],"title":"MMDA: Multi-person Marginal Distribution Awareness for Monocular 3D Pose Estimation","type":"publication"},{"authors":["戴京昭","胡雪娇","李明","李杨","都思丹"],"categories":null,"content":"","date":1674691200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1674691200,"objectID":"0cd3e4d991d0f6b6a9f128f5e0b4263c","permalink":"http://localhost:1313/publication/food-analyses-in-computer-vision/","publishdate":"2023-01-26T00:00:00Z","relpermalink":"/publication/food-analyses-in-computer-vision/","section":"publication","summary":"With the rapid development of food production and health management, analyses of food samples have been essential for preventing diseases and understanding human culture. Recently, food analyses have become increasingly complex and are not limited in food categorization. They also contain many advanced tasks (e.g., nutrition estimation and recipe retrieval). From existing works, two points can be concluded. First, food features are much more comprehensive and sophisticated than general samples. Second, for food analyses, multiple learning strategies (MLSs) usually achieve outperformance over general deep learning methods. However, there are few survey papers reporting food analyses with MLSs, and the main factors lead to difficulty of operation. Therefore, we intend to conduct a survey for applications of MLSs to food analyses. In this survey paper, three types of common MLSs, which are multi-task learning (MTL), multi-view learning (MVL) and multi-scale learning (MSL) strategies, are presented in terms of their guidance, typical works, algorithms and final aggregation methods. Additionally, food characteristics are proposed to be closely related to the difficulty of food analyses. We comprehensively conclude food characteristics as nonrigid, complex in arrangement, and large (small) in intraclass (interclass) variance. Moreover, some experimental results of MLSs are also presented and analyzed in this paper. Based on these results, insightful suggestions for MLSs implementation are proposed. Finally, the promising tendency of MLSs applications in the future is discussed.","tags":["Source Themes"],"title":"The multi-learning for food analyses in computer vision: a survey","type":"publication"},{"authors":["李明","靳学乾"],"categories":null,"content":"","date":1656806400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656806400,"objectID":"11a05cf5f2d9a1da11ce003c554ac18e","permalink":"http://localhost:1313/publication/mode2022/","publishdate":"2022-07-03T00:00:00Z","relpermalink":"/publication/mode2022/","section":"publication","summary":"In this paper, we propose a two-stage omnidirectional depth estimation framework with multi-view 360◦ cameras. The framework first estimates the depth maps from different camera pairs via omnidirectional stereo matching and then fuses the depth maps to achieverobustness against mud spots, water drops on camera lenses, and glare caused by intense light. We adopt spherical feature learning to address the distortion of panoramas. In addition, a synthetic 360◦ dataset consisting of 12K road scene panoramas and 3K ground truth depth maps is presented to train and evaluate 360◦ depth estimation algorithms. Our dataset takes soiled camera lenses and glare into consideration, which is more consistent with the real-world environment. Experimental results show that the proposed framework generates reliable results in both synthetic and real-world environments, and it achieves state-of-the-art performance on different datasets.","tags":["Source Themes"],"title":"MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras","type":"publication"},{"authors":null,"categories":null,"content":"恭喜李明和靳学乾！\n论文详情\n","date":1656806400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656806400,"objectID":"8f50ba83d2799aa01a0cda15908b510e","permalink":"http://localhost:1313/post/22-07-03-eccv-paper-accept/","publishdate":"2022-07-03T00:00:00Z","relpermalink":"/post/22-07-03-eccv-paper-accept/","section":"post","summary":"恭喜李明和靳学乾！\n","tags":null,"title":"我们的文章被 ECCV 2022 接收！","type":"post"},{"authors":["胡雪娇","戴京昭","李明","彭成磊","李杨","都思丹"],"categories":null,"content":"","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656374400,"objectID":"8bade7f9269ab6a550d4308d444ceea6","permalink":"http://localhost:1313/publication/online-human-action/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/publication/online-human-action/","section":"publication","summary":"To meet the demand for powerful models for practical applications in real time, the focus of research on human actions has shifted from offline detection to online and real-time understanding, such as driver-assistance systems, surveillance analysis, and robot services. In recent years, with the development of video recording acquisition technology and deep learning, online action analysis has made significant progress. However, there is a lack of comprehensive online surveys for online human action detection. In this survey, we discuss two hot real-time concerns online action detection and action anticipation. Online action/activity detection aims to determine whether an action is currently taking place and what kind of action it is in untrimmed videos. Action anticipation aims to anticipate human actions under limited observation of videos. Online action detection and anticipation require accuracy and low latency of detection when the video is partly observed. We present a comprehensive study that includes the definition, taxonomy, comparison of state-of-the-art techniques, datasets, metrics, challenges, and future directions. We hope that it will provide readers with a detailed understanding of the topic and inspiration for new research directions.","tags":["Source Themes"],"title":"Online human action detection and anticipation in videos: A survey","type":"publication"},{"authors":["王汉镕","李明","王杰","李杨","都思丹"],"categories":null,"content":"","date":1653177600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1653177600,"objectID":"d0ae4ab3272b2f9d13f523243d6ee089","permalink":"http://localhost:1313/publication/optimization-about-stereo-image-depth-estimation/","publishdate":"2022-05-22T00:00:00Z","relpermalink":"/publication/optimization-about-stereo-image-depth-estimation/","section":"publication","summary":"The huge computational complexity and occlusion problems make stereo matching a major challenge. In this work, we use multi-baseline trinocular camera model to accelerate the stereo matching algorithms and improve the accuracy of disparity estimation. We propose a special scheme named the trinocular flexible disparity searching range (FDSR) to accelerate the stereo matching algorithms. In this scheme, we optimize stereo matching by reducing the disparity searching range. Based on FDSR, we proposed the FDSR-MCCNN for trinocular stereo matching. According to the evaluation results, the FDSR-MCCNN could not only reduce the computational complexity but also improve the accuracy. Moreover, the optimization schemes we designed can be extended to other stereo matching algorithms that possess pixel-wise matching cost calculations and aggregation steps. We proved that the proposed optimization methods for trinocular stereo matching are effective and that trinocular stereo matching is useful for either improving accuracy or reducing computational complexity.","tags":["Source Themes"],"title":" A Discussion of Optimization about Stereo Image Depth Estimation Based on Multi-baseline Trinocular Camera Model","type":"publication"},{"authors":["曹静怡","彭成磊","李杨","都思丹"],"categories":null,"content":"","date":1651795200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1651795200,"objectID":"23cabe02a7bd7fd3f3883a09f3ffbbc2","permalink":"http://localhost:1313/publication/shadow-detection/","publishdate":"2022-05-06T00:00:00Z","relpermalink":"/publication/shadow-detection/","section":"publication","summary":"The existing shadow detection methods have achieved good results on standard shadow datasets such as SBU and UCF. However, in actual large-scale scenes, key objects covered by shadows are often regarded as shadows, which may harm computer vision tasks. In the paper, we are the first to propose the Object-aware Shadow Detection Network (OSD-Net) model for computer vision tasks in complex scenes. It introduces the direction-aware spatial context (DSC) module to detect shadows, uses semantic segmentation with Mask RCNN to extract key objects in the picture, and designs a function to perform mask fusion. Qualitative experiments have been performed to test OSD-Net on three public datasets commonly used in computer vision. Compared with popular shadow detection methods, OSD-Net is able to effectively protect the key targets in the picture from being misjudged as shadows, and ensure shadow detection accuracy.","tags":["Source Themes"],"title":"A Shadow Detection Method for Retaining Key Objects in Complex Scenes","type":"publication"},{"authors":["李兆旭","刘晟","白珏","彭成磊","李杨"],"categories":null,"content":"","date":1646352000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1646352000,"objectID":"52056beaa5fc3debacb153f8456193fb","permalink":"http://localhost:1313/publication/3d-human-pose-estimation/","publishdate":"2022-03-04T00:00:00Z","relpermalink":"/publication/3d-human-pose-estimation/","section":"publication","summary":"Kinematic chain model is widely adopted in 3D human pose estimation tasks while it cannot accurately describe the curvature of the torso. This work proposes for the first time a novel model with spine curve to both express the movement of the limbs and the bending curvature of the torso. We parameterize the spine curve with the Bezier curve as it's controlled by anchor points. In this way, the estimation of the spine curve can be converted back to the estimation of points. The result shows that the anchor points of the spine curve can be estimated accurately by existing networks, similar to other joints. With the help of MOSH++ and SMPL model, the existing datasets can also be employed with our skeleton. We fit the SMPL models with the kinematic chain model and our model respectively, the fitting result shows that our model can provide richer information for the construction of human models.","tags":["Source Themes"],"title":" A Novel Skeleton-based Model with Spine for 3D Human Pose Estimation","type":"publication"},{"authors":["靳学乾","李明","彭成磊","都思丹","李杨"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1645488000,"objectID":"c2b6b8a42e68324696396eed30b1f1b4","permalink":"http://localhost:1313/publication/removal-of-thermal-reflection/","publishdate":"2022-02-22T00:00:00Z","relpermalink":"/publication/removal-of-thermal-reflection/","section":"publication","summary":"Thermal imaging is a useful imaging technique in many scenarios. It can capture the temperature distribution of scenes in the dark and see through sparse smoke and dust. However, some surfaces such as steel and glass with high reflectivity lead to a reflection problem in thermal imaging, while heavy mist and gases lead to the occlusion problem. We proposed an efficient algorithm to solve the occlusion problem in our earlier work. The reflection in thermal images causes errors in detection and temperature measurement. Therefore, the precise model and efficient algorithms to solve this problem are in high demand. In this paper, we mainly model the reflection problem in thermal imaging and propose an algorithm to deal with it. In our experiments, a thermal camera array is built to capture the thermal light-field images. We first separate a part of the reflection pixels from thermal images based on the depth information. After that, the thermal reflection is removed by optimizing a designed cost function. The experiment results show that our reflection removal method can separate the thermal reflection with high precision, retain the objects in the scene, and get better performance than existing methods.","tags":["Source Themes"],"title":"Depth-based removal of thermal reflection with the light-field theory","type":"publication"},{"authors":["王杰","彭成磊","李明","李杨","都思丹"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1645488000,"objectID":"58eae5cc7ce780ed70bdea0fb29a70cd","permalink":"http://localhost:1313/publication/study-of-stereo/","publishdate":"2022-02-22T00:00:00Z","relpermalink":"/publication/study-of-stereo/","section":"publication","summary":"The huge computational complexity, occlusion and low texture region problems make stereo matching a big challenge. In this work, we use multi-baseline trinocular camera model to study how to accelerate the stereo matching algorithms and improve the accuracy of disparity estimation. A special scheme named the trinocular dynamic disparity range (T-DDR) was designed to accelerate the stereo matching algorithms. In this scheme, we optimize matching cost calculation, cost aggregation and disparity computation steps by narrowing disparity searching range. Meanwhile, we designed another novel scheme called the trinocular disparity confidence measure (T-DCM) to improve the accuracy of the disparity map. Based on those, we proposed the semi-global matching with T-DDR (T-DDR-SGM) and T-DCM (T-DCM-SGM) algorithms for trinocular stereo matching. According to the evaluation results, the T-DDR-SGM could not only significantly reduce the computational complexity but also slightly improving the accuracy, while the T-DCM-SGM could excellently handle the occlusion and low texture region problems. Both of them achieved a better result. Moreover, the optimization schemes we designed can be extended to the other stereo matching algorithms which possesses pixel-wise matching cost calculation and aggregation steps not only the SGM. We proved that the proposed optimization methods for the trinocular stereo matching are effective and the trinocular stereo matching is useful for either improving accuracy or reducing computational complexity.","tags":["Source Themes"],"title":"The study of stereo matching optimization based on multi-baseline trinocular model","type":"publication"},{"authors":["白珏","彭成磊","李兆旭","都思丹","李杨"],"categories":null,"content":"","date":1635638400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1635638400,"objectID":"7ec6cb327dfff6077a251d89e7ddbc7f","permalink":"http://localhost:1313/publication/large-angle-head-pose-estimation/","publishdate":"2021-10-31T00:00:00Z","relpermalink":"/publication/large-angle-head-pose-estimation/","section":"publication","summary":"Predicting Euler angles of head pose using end-to-end CNN from a single RGB image is a popular application in recent years. However, the existing methods ignored the information about the rotation order contained in the Euler angles, always following the traditional pitch-yaw-roll order. They also neglected the error sources from outlier samples with large-angle poses. We analyzed current shortcomings and made suggestions for improvement from the perspective of data distribution. We studied the influence of different rotation orders on the data distribution and showed choosing an appropriate rotation order to learn head pose can significantly optimize the data distribution and improve the prediction accuracy. Then a data enhancement method was proposed to increase the large-angle poses by rotating the 2D images randomly and solving the corresponding head poses, which can improve network performance on the large-angle poses. Evaluated on two popular networks and different datasets, our methods were proved to be effective and general.","tags":["Source Themes"],"title":"A Study of General Data Improvement for Large-Angle Head Pose Estimation","type":"publication"},{"authors":["李明","胡雪娇","戴京昭","李杨","都思丹"],"categories":null,"content":"","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1629676800,"objectID":"ec4f05c39ab2aeb2ac622a1914cc37f0","permalink":"http://localhost:1313/publication/omnidirectional-stereo-depth/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/publication/omnidirectional-stereo-depth/","section":"publication","summary":"Omnidirectional depth estimation is an emerging research topic and has received significant attention in recent years. However, the existing methods were developed based on the theory of planar stereo matching; and introduce the nonlinear epipolar constraint and significant distortions of re-projections. In this paper, we propose a novel approach that use spherical CNNs and the epipolar constraint on sphere for omnidirectional depth estimation. We discuss the epipolar constraint for spherical stereo imaging and convert the nonlinear constraint on a planar projection to the linear constraint on a sphere. We then propose a Spherical Convolution Residual Network (SCRN) for omnidirectional depth estimation via the spherical linear epipolar constraint. The input equirectangular projection (ERP) images are sampled to spherical meshes and fed into SCRN to calculate spherical depth maps. For 2D visualization, we design a Planar Refinement Network (PRN) and adopt the cascade learning scheme to improve the accuracy of depth maps. This scheme reduces the errors caused by projection, interpolation, and the limitation of spherical representation. The experiment shows that our full scheme Cascade Spherical Depth Network (CSDNet) results in more accurate and detailed depth maps with lower errors, as compared to recent seminal works. Our approach yields the comparable performance to the other state-of-the-art works on the omnidirectional stereo datasets with less number of parameters. The effectiveness of the spherical network and the cascade learning scheme is validated, and the influence of spherical sampling density is also discussed.","tags":["Source Themes"],"title":"Omnidirectional stereo depth estimation based on spherical deep network","type":"publication"},{"authors":["徐一舫","彭成磊","李明","李杨","都思丹"],"categories":null,"content":"","date":1625443200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1625443200,"objectID":"f210674663ab81d122743f163bcda1c3","permalink":"http://localhost:1313/publication/pyramid-feature-attention-network-for-monocular-depth-prediction/","publishdate":"2021-07-05T00:00:00Z","relpermalink":"/publication/pyramid-feature-attention-network-for-monocular-depth-prediction/","section":"publication","summary":"Deep convolutional neural networks (DCNNs) have achieved great success in monocular depth estimation (MDE). However, few existing works take the contributions for MDE of different levels feature maps into account, leading to inaccurate spatial layout, ambiguous boundaries and discontinuous object surface in the prediction. To better tackle these problems, we propose a Pyramid Feature Attention Network (PFANet) to improve the high-level context features and lowlevel spatial features. In the proposed PFANet, we design a Dual-scale Channel Attention Module (DCAM) to employ channel attention in different scales, which aggregate global context and local information from the high-level feature maps. To exploit the spatial relationship of visual features, we design a Spatial Pyramid Attention Module (SPAM) which can guide the network attention to multi-scale detailed information in the low-level feature maps. Finally, we introduce scale-invariant gradient loss to increase the penalty on errors in depth-wise discontinuous regions. Experimental results show that our method outperforms state-of-the-art methods on the KITTI dataset.","tags":["Source Themes"],"title":"Pyramid Feature Attention Network for Monocular Depth Prediction","type":"publication"},{"authors":["陈佟","彭成磊","李明","陈旭东","都思丹","李杨"],"categories":null,"content":"","date":1617840000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1617840000,"objectID":"c0ff3174772a708789929ffb77806c9e","permalink":"http://localhost:1313/publication/quantitative-analyzing/","publishdate":"2021-04-08T00:00:00Z","relpermalink":"/publication/quantitative-analyzing/","section":"publication","summary":"The movements of mitochondrial are especially critical for neuronal growth and function. However, to analyze and quantify this process is technically challenging. Different from traditional hand-drawn method which lacks efficiency, we focus on automatic methods, which consist three key aspects (image enhancement, trajectories tracking and quantitative analyzing) and provide a discussion about different issues in these steps.","tags":["Source Themes"],"title":"A Review on Quantitative Analyzing Axonal Transport of Mitochondria","type":"publication"},{"authors":["黎琪","Ma Yazhen","彭成磊","Guo Bin","都思丹","李杨"],"categories":null,"content":"","date":1617840000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1617840000,"objectID":"1e38330cc0ce8865a3bee6791e28dd2a","permalink":"http://localhost:1313/publication/diabetic-retinopathy-lesion-detection/","publishdate":"2021-04-08T00:00:00Z","relpermalink":"/publication/diabetic-retinopathy-lesion-detection/","section":"publication","summary":"Diabetic retinopathy (DR) is one of the leading causes of preventable blindness. It's urgent to develop reliable methods for auto DR screening, the key of which is the detection of lesions. This paper presents an innovative method to detect DR lesions in pixel-level. We design a multi-scale Convolution Neural Network (CNN) that make the full use of multiple different scales with complementary image information. Experiments are carried out on both private and public datasets. Results show that multi-scale CNN model outperforms single-scale CNN model and other state-of-the-art approaches.","tags":["Source Themes"],"title":"Pixel-level Diabetic Retinopathy Lesion Detection Using Multi-scale Convolutional Neural Network","type":"publication"},{"authors":["周子豪","李杨","彭成磊","王汉镕","都思丹"],"categories":null,"content":"","date":1613952000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1613952000,"objectID":"ccd2f774b5273192913294d51d170c85","permalink":"http://localhost:1313/publication/image-processing-facilitating-retinanet-for-detecting-small-objects/","publishdate":"2021-02-22T00:00:00Z","relpermalink":"/publication/image-processing-facilitating-retinanet-for-detecting-small-objects/","section":"publication","summary":"Detecting small objects is a challenging task in object detection due to low spatial resolution and interference by background. Specifically, one-stage detectors struggle with small objects for they generate worse candidate bounding boxes. In this paper, several modifications are made to the original Retinanet to tackle the problem. Dilated convolutional layers are added to the backbone to get fined-grained features along with semantic information. The gradient of loss function is increased near the origin to enhance the quality of candidate boxes for small objects. A novel feature fusion method is also proposed to directly guide low-level features with semantic information. Significant improvement of 5.1 mAP can be seen when evaluating on MOCOD small object dataset, which contains a large amount of small objects. Our method can be easily migrated to other backbone networks with feature pyramids for detecting small objects.","tags":["Source Themes"],"title":"Image Processing: Facilitating Retinanet for Detecting Small Objects","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"http://localhost:1313/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"http://localhost:1313/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"NJU-EE Autonomous Driving Research Group专注于以视觉为基础的感知算法 深度估计 我们研究了多摄像头系统的深度估算，以获取自动驾驶系统周围环境的结构信息。下面是一段演示视频。 感算一体 我们提出了一种基于近传感器计算架构的全向深度估计系统。所提出的工作通过任务分区实现了负载平衡，同时通过特征投影和可学习编解码器降低了传输带宽。\n占据预测网络 基于我们实验室的深度估计网络提供的深度信息，我们提出了一个基于圆柱体素的“Sketch-Coloring“框架。 下面是一段演示视频。实验结果表明，我们的“Sketch-Coloring”网络能显著提高三维感知性能，尤其是在邻近区域，这使我们的方法成为自动驾驶感知的一个有前途的解决方案。\n更多相关资料\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"b75c4ef86e2b546bd4d7028b3b0610c6","permalink":"http://localhost:1313/direction/autonomous/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/direction/autonomous/","section":"direction","summary":"NJU-EE Autonomous Driving Research Group专注于以视觉为基础的感知算法 深度估计 我们研究了多摄像头系统的深度估算，以获取自动驾驶系统周围环境的结构信息。下面是一段演示视频。 感算一体 我们提出了一种基于近传感器计算架构的全向深度估计系统。所提出的工作通过任务分区实现了负载平衡，同时通过特征投影和可学习编解码器降低了传输带宽。\n占据预测网络 基于我们实验室的深度估计网络提供的深度信息，我们提出了一个基于圆柱体素的“Sketch-Coloring“框架。 下面是一段演示视频。实验结果表明，我们的“Sketch-Coloring”网络能显著提高三维感知性能，尤其是在邻近区域，这使我们的方法成为自动驾驶感知的一个有前途的解决方案。\n更多相关资料","tags":null,"title":"NJU-EE Autonomous Driving Research Group","type":"direction"}]