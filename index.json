[{"authors":["翟本祥"],"categories":null,"content":"","date":1757376000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1757376000,"objectID":"4a375a3af28d91dd09d2955f42507cf1","permalink":"https://nju-ee.github.io/author/benxiang-zhai%E7%BF%9F%E6%9C%AC%E7%A5%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/benxiang-zhai%E7%BF%9F%E6%9C%AC%E7%A5%A5/","section":"authors","summary":"","tags":null,"title":"Benxiang Zhai(翟本祥)","type":"authors"},{"authors":["都思丹"],"categories":null,"content":"个人简历： 1997年毕业于南京大学，获博士学位。南京大学电子科学与工程学院电子工程系教授，图像工程实验室主任。电路与系统学科带头人，南京市321工程有突出贡献中青年专家。先后在日本富士通、美国西北大学访问研究， 曾挂职宜兴市科技副市长。主持承担了自然科学基金、国际合作重大项目，以及江苏省科技支撑计划等项目。发表SCI、EI学术论文100余篇。获授权国家发明专利20余项，江苏省科学技术三等奖、江苏省优秀软件产品奖，中国计算机行业发展成就奖（团体），日本富士通优秀业绩奖，南京大学青年骨干教师奖。\n研究方向： 图像处理与控制 机器学习与算法 密码与信息安全 主要课程： 本科生：概率论与随机过程，图像处理 研究生：信号处理中的数学方法，现代图像工程 ","date":1757376000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1757376000,"objectID":"aec4f54067fa5320f4f03fe6a507c3f7","permalink":"https://nju-ee.github.io/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/","section":"authors","summary":"个人简历： 1997年毕业于南京大学，获博士学位。南京大学电子科学与工程学院电子工程系教授，图像工程实验室主任。电路与系统学科带头人，南京市321工程有突出贡献中青年专家。先后在日本富士通、美国西北大学访问研究， 曾挂职宜兴市科技副市长。主持承担了自然科学基金、国际合作重大项目，以及江苏省科技支撑计划等项目。发表SCI、EI学术论文100余篇。获授权国家发明专利20余项，江苏省科学技术三等奖、江苏省优秀软件产品奖，中国计算机行业发展成就奖（团体），日本富士通优秀业绩奖，南京大学青年骨干教师奖。\n研究方向： 图像处理与控制 机器学习与算法 密码与信息安全 主要课程： 本科生：概率论与随机过程，图像处理 研究生：信号处理中的数学方法，现代图像工程 ","tags":null,"title":"Sidan Du(都思丹)","type":"authors"},{"authors":["李杨"],"categories":null,"content":"在计算机视觉领域。我们的研究兴趣集中利用计算机视觉技术对环境进行三维感知和对环境图像进行语义理解。我们围绕着机器人导航、自动驾驶等应用场景下的多种三维感知技术进行研究。在双目及多目立体视觉被动感知技术方面，我们首次提出了利用帧间相关性加速视频序列中的立体匹配问题，该方法在不降低视差计算精度的前提可以显著提升基于深度学习和非深度学习方法的立体匹配速度，我们在嵌入式系统中实现的双目立体视觉系统达到了国际先进水平。我们同时在激光雷达点云的语义分割、配准上也取得众多成果，利用算法优化和GPU加速我们实现在低功耗嵌入式平台上的实时三维重建。针对机器人导航、自动驾驶等应用场景，我们进一步发展和完善了双目视觉的同步定位与地图构建问题，提出并实现了利用深度图像加速地图构建。此外，我们在目标识别、图像语义分析等领域也取得若干成果，例如我们针对多模态图像中的目标识别问题提出了全新的深度网络架构和数据增强及训练方法，显著提升了算法的性能。\n","date":1757376000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1757376000,"objectID":"f288ec240bf05ad2f6ed15f7234831d0","permalink":"https://nju-ee.github.io/author/yang-li%E6%9D%8E%E6%9D%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yang-li%E6%9D%8E%E6%9D%A8/","section":"authors","summary":"在计算机视觉领域。我们的研究兴趣集中利用计算机视觉技术对环境进行三维感知和对环境图像进行语义理解。我们围绕着机器人导航、自动驾驶等应用场景下的多种三维感知技术进行研究。在双目及多目立体视觉被动感知技术方面，我们首次提出了利用帧间相关性加速视频序列中的立体匹配问题，该方法在不降低视差计算精度的前提可以显著提升基于深度学习和非深度学习方法的立体匹配速度，我们在嵌入式系统中实现的双目立体视觉系统达到了国际先进水平。我们同时在激光雷达点云的语义分割、配准上也取得众多成果，利用算法优化和GPU加速我们实现在低功耗嵌入式平台上的实时三维重建。针对机器人导航、自动驾驶等应用场景，我们进一步发展和完善了双目视觉的同步定位与地图构建问题，提出并实现了利用深度图像加速地图构建。此外，我们在目标识别、图像语义分析等领域也取得若干成果，例如我们针对多模态图像中的目标识别问题提出了全新的深度网络架构和数据增强及训练方法，显著提升了算法的性能。","tags":null,"title":"Yang Li(李杨)","type":"authors"},{"authors":["徐一舫"],"categories":null,"content":"这里是个人介绍\n","date":1757376000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1757376000,"objectID":"bce2f47b2f7cdd5915fb051fcf786371","permalink":"https://nju-ee.github.io/author/yifang-xu%E5%BE%90%E4%B8%80%E8%88%AB/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yifang-xu%E5%BE%90%E4%B8%80%E8%88%AB/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Yifang Xu(徐一舫)","type":"authors"},{"authors":["李明"],"categories":null,"content":"","date":1749600000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1749600000,"objectID":"926f2044e1048ef84ac37cc73a1bf8ff","permalink":"https://nju-ee.github.io/author/ming-li%E6%9D%8E%E6%98%8E/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ming-li%E6%9D%8E%E6%98%8E/","section":"authors","summary":"","tags":null,"title":"Ming Li(李明)","type":"authors"},{"authors":["武超凡"],"categories":null,"content":"","date":1744243200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1744243200,"objectID":"b8b1ecd5414e7585506248eb6669b394","permalink":"https://nju-ee.github.io/author/chaofan-wu%E6%AD%A6%E8%B6%85%E5%87%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chaofan-wu%E6%AD%A6%E8%B6%85%E5%87%A1/","section":"authors","summary":"","tags":null,"title":"Chaofan Wu(武超凡)","type":"authors"},{"authors":["曹靖豪"],"categories":null,"content":"这里是个人介绍\n","date":1744243200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1744243200,"objectID":"da4ca16fe10d4dfa0e9d04f2315b9cb8","permalink":"https://nju-ee.github.io/author/jinghao-cao%E6%9B%B9%E9%9D%96%E8%B1%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jinghao-cao%E6%9B%B9%E9%9D%96%E8%B1%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jinghao Cao(曹靖豪)","type":"authors"},{"authors":["刘晟"],"categories":null,"content":"这里是个人介绍\n","date":1744243200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1744243200,"objectID":"0e126dad231a8b33624f1ff0ad18c1a4","permalink":"https://nju-ee.github.io/author/sheng-liu%E5%88%98%E6%99%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sheng-liu%E5%88%98%E6%99%9F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Sheng Liu(刘晟)","type":"authors"},{"authors":["戴京昭"],"categories":null,"content":"这里是个人介绍\n","date":1738195200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1738195200,"objectID":"3cfa55795f69a5a08c4fddfe7da5203f","permalink":"https://nju-ee.github.io/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jingzhao Dai(戴京昭)","type":"authors"},{"authors":["王师捷"],"categories":null,"content":"","date":1704844800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704844800,"objectID":"4de19d0a657045458a77ad2bcde5977f","permalink":"https://nju-ee.github.io/author/shijie-wang%E7%8E%8B%E5%B8%88%E6%8D%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shijie-wang%E7%8E%8B%E5%B8%88%E6%8D%B7/","section":"authors","summary":"","tags":null,"title":"Shijie Wang(王师捷)","type":"authors"},{"authors":["胡雪娇"],"categories":null,"content":"","date":1704844800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704844800,"objectID":"c1943d631536dada07c9deecb391dee1","permalink":"https://nju-ee.github.io/author/xuejiao-hu%E8%83%A1%E9%9B%AA%E5%A8%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xuejiao-hu%E8%83%A1%E9%9B%AA%E5%A8%87/","section":"authors","summary":"","tags":null,"title":"Xuejiao Hu(胡雪娇)","type":"authors"},{"authors":["王品智"],"categories":null,"content":"这里是个人介绍\n","date":1704499200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704499200,"objectID":"73214478b073efac6dd77142f312f503","permalink":"https://nju-ee.github.io/author/pinzhi-wang%E7%8E%8B%E5%93%81%E6%99%BA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pinzhi-wang%E7%8E%8B%E5%93%81%E6%99%BA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Pinzhi Wang(王品智)","type":"authors"},{"authors":["帅江海"],"categories":null,"content":"这里是个人介绍\n","date":1700179200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1700179200,"objectID":"046f43ce15a0832fa8303702f4caa410","permalink":"https://nju-ee.github.io/author/jianghai-shuai%E5%B8%85%E6%B1%9F%E6%B5%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianghai-shuai%E5%B8%85%E6%B1%9F%E6%B5%B7/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jianghai Shuai(帅江海)","type":"authors"},{"authors":["冯永康"],"categories":null,"content":"Feel free to contact me.\n","date":1700179200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1700179200,"objectID":"b48fa1359440151b6c79c3ea1b02bec2","permalink":"https://nju-ee.github.io/author/yongkang-feng%E5%86%AF%E6%B0%B8%E5%BA%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yongkang-feng%E5%86%AF%E6%B0%B8%E5%BA%B7/","section":"authors","summary":"Feel free to contact me.","tags":null,"title":"Yongkang Feng(冯永康)","type":"authors"},{"authors":["朱治亦"],"categories":null,"content":"这里是个人介绍\n","date":1690416000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1690416000,"objectID":"262c22709d80c22fea8d72432065c3f0","permalink":"https://nju-ee.github.io/author/zhiyi-zhu%E6%9C%B1%E6%B2%BB%E4%BA%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiyi-zhu%E6%9C%B1%E6%B2%BB%E4%BA%A6/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhiyi Zhu(朱治亦)","type":"authors"},{"authors":["靳学乾"],"categories":null,"content":"这里是个人介绍\n","date":1656806400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1656806400,"objectID":"d3f1f0cdfcb9e2e97fc2490fb6509516","permalink":"https://nju-ee.github.io/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Xueqian Jin(靳学乾)","type":"authors"},{"authors":["王汉镕"],"categories":null,"content":"这里是个人介绍\n","date":1653177600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1653177600,"objectID":"4813d0567e2be557ca0e7e9adc939800","permalink":"https://nju-ee.github.io/author/hanrong-wang%E7%8E%8B%E6%B1%89%E9%95%95/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hanrong-wang%E7%8E%8B%E6%B1%89%E9%95%95/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Hanrong Wang(王汉镕)","type":"authors"},{"authors":["王杰"],"categories":null,"content":"","date":1653177600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1653177600,"objectID":"5a4344f2cc5135e24172554221ed61e0","permalink":"https://nju-ee.github.io/author/jie-wang%E7%8E%8B%E6%9D%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jie-wang%E7%8E%8B%E6%9D%B0/","section":"authors","summary":"","tags":null,"title":"Jie Wang(王杰)","type":"authors"},{"authors":["曹静怡"],"categories":null,"content":"这里是个人介绍\n","date":1651795200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1651795200,"objectID":"f6ff1b89fdfdcc22f3c6363d4db78b29","permalink":"https://nju-ee.github.io/author/jingyi-cao%E6%9B%B9%E9%9D%99%E6%80%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingyi-cao%E6%9B%B9%E9%9D%99%E6%80%A1/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jingyi Cao(曹静怡)","type":"authors"},{"authors":["白珏"],"categories":null,"content":"这里是个人介绍\n","date":1646352000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1646352000,"objectID":"9bf3914ddb531aa38f7f52088e14376e","permalink":"https://nju-ee.github.io/author/jue-bai%E7%99%BD%E7%8F%8F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jue-bai%E7%99%BD%E7%8F%8F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jue Bai(白珏)","type":"authors"},{"authors":["李兆旭"],"categories":null,"content":"这里是个人介绍\n","date":1646352000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1646352000,"objectID":"c7b764560c4c9d4d73658b276b5e454b","permalink":"https://nju-ee.github.io/author/zhaoxu-li%E6%9D%8E%E5%85%86%E6%97%AD/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhaoxu-li%E6%9D%8E%E5%85%86%E6%97%AD/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhaoxu Li(李兆旭)","type":"authors"},{"authors":["黎琪"],"categories":null,"content":"这里是个人介绍\n","date":1617840000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1617840000,"objectID":"f1ca98276535d17eb6f055f3c436b962","permalink":"https://nju-ee.github.io/author/qi-li%E9%BB%8E%E7%90%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qi-li%E9%BB%8E%E7%90%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Qi Li(黎琪)","type":"authors"},{"authors":["陈佟"],"categories":null,"content":"这里是个人介绍\n","date":1617840000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1617840000,"objectID":"2b82ab97fce568f00e121b3821dc3ca8","permalink":"https://nju-ee.github.io/author/tong-chen%E9%99%88%E4%BD%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tong-chen%E9%99%88%E4%BD%9F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Tong Chen(陈佟)","type":"authors"},{"authors":["陈旭东"],"categories":null,"content":"经历了紫荆 pt、GoAgent 时代，开眼看世界。 经过几年不务正业，精通各式语言和框架，如 C++、Makefile、CMake、Bash、Python、Latex、Markdown、HTML、CSS、JavaScript、Lua、Java、Hive SQL，以及各样系统和软件，如 Windows、Ubuntu、Microsoft Office、Microsoft Visio、Adobe Acrobat、Adobe Photoshop、SAI，的擅长领域和文档查询。\n最近觉得 NAS 和 pt 有点意思，可惜紫荆已经关站。 强烈安利 BYRPT ，这是我有账号的几个 pt 站里还没关的，欢迎大家找我玩~~\n","date":1617840000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1617840000,"objectID":"4426c4f7800d9d6ba95dcc000a955da9","permalink":"https://nju-ee.github.io/author/xudong-chen%E9%99%88%E6%97%AD%E4%B8%9C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xudong-chen%E9%99%88%E6%97%AD%E4%B8%9C/","section":"authors","summary":"经历了紫荆 pt、GoAgent 时代，开眼看世界。 经过几年不务正业，精通各式语言和框架，如 C++、Makefile、CMake、Bash、Python、Latex、Markdown、HTML、CSS、JavaScript、Lua、Java、Hive SQL，以及各样系统和软件，如 Windows、Ubuntu、Microsoft Office、Microsoft Visio、Adobe Acrobat、Adobe Photoshop、SAI，的擅长领域和文档查询。\n最近觉得 NAS 和 pt 有点意思，可惜紫荆已经关站。 强烈安利 BYRPT ，这是我有账号的几个 pt 站里还没关的，欢迎大家找我玩~~","tags":null,"title":"Xudong Chen(陈旭东)","type":"authors"},{"authors":["周子豪"],"categories":null,"content":"","date":1613952000,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1613952000,"objectID":"0029828d3e9d8497ac33da8f4419d0e1","permalink":"https://nju-ee.github.io/author/zihao-zhou%E5%91%A8%E5%AD%90%E8%B1%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zihao-zhou%E5%91%A8%E5%AD%90%E8%B1%AA/","section":"authors","summary":"","tags":null,"title":"Zihao Zhou(周子豪)","type":"authors"},{"authors":["董晨"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"a7697d83fcea15e78896b80c27fe2534","permalink":"https://nju-ee.github.io/author/chen-dong%E8%91%A3%E6%99%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chen-dong%E8%91%A3%E6%99%A8/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Chen Dong(董晨)","type":"authors"},{"authors":["杨帆"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"08232687745d8a4d0349d036c8c5fcf3","permalink":"https://nju-ee.github.io/author/fan-yang%E6%9D%A8%E5%B8%86/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/fan-yang%E6%9D%A8%E5%B8%86/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Fan Yang(杨帆)","type":"authors"},{"authors":["李嘉恒"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"27879f078458565628243f2aaf610067","permalink":"https://nju-ee.github.io/author/jiaheng-li%E6%9D%8E%E5%98%89%E6%81%92/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaheng-li%E6%9D%8E%E5%98%89%E6%81%92/","section":"authors","summary":"","tags":null,"title":"Jiaheng Li(李嘉恒)","type":"authors"},{"authors":["郑嘉璇"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"c9ef1dac8c4b1e372ccc0cde72333de4","permalink":"https://nju-ee.github.io/author/jiaxuan-zheng%E9%83%91%E5%98%89%E7%92%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaxuan-zheng%E9%83%91%E5%98%89%E7%92%87/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jiaxuan Zheng(郑嘉璇)","type":"authors"},{"authors":["吴佳昱"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"65834053bfc801d06ba84965cabacfc0","permalink":"https://nju-ee.github.io/author/jiayu-wu%E5%90%B4%E4%BD%B3%E6%98%B1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiayu-wu%E5%90%B4%E4%BD%B3%E6%98%B1/","section":"authors","summary":"","tags":null,"title":"Jiayu Wu(吴佳昱)","type":"authors"},{"authors":["石立"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"7dc83e81e202bdc33b04b4500b2f77ab","permalink":"https://nju-ee.github.io/author/li-shi%E7%9F%B3%E7%AB%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-shi%E7%9F%B3%E7%AB%8B/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Li Shi(石立)","type":"authors"},{"authors":["赵芮"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b3cc0ba2cc650ae09b0712998d97a0c3","permalink":"https://nju-ee.github.io/author/rui-zhao%E8%B5%B5%E8%8A%AE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rui-zhao%E8%B5%B5%E8%8A%AE/","section":"authors","summary":"","tags":null,"title":"Rui Zhao(赵芮)","type":"authors"},{"authors":["陆胜"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"3e403ff20ed614d1376c068f35edcafb","permalink":"https://nju-ee.github.io/author/sheng-lu%E9%99%86%E8%83%9C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sheng-lu%E9%99%86%E8%83%9C/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Sheng Lu(陆胜)","type":"authors"},{"authors":["许薯文"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"56100b1701554d958fb2e4677d931f4c","permalink":"https://nju-ee.github.io/author/shuwen-xu%E8%AE%B8%E8%96%AF%E6%96%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuwen-xu%E8%AE%B8%E8%96%AF%E6%96%87/","section":"authors","summary":"","tags":null,"title":"Shuwen Xu(许薯文)","type":"authors"},{"authors":["陆天昊"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"0700c0266630f9e8f4f9e155cd550b42","permalink":"https://nju-ee.github.io/author/tianhao-lu%E9%99%86%E5%A4%A9%E6%98%8A/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianhao-lu%E9%99%86%E5%A4%A9%E6%98%8A/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Tianhao Lu(陆天昊)","type":"authors"},{"authors":["唐铁健"],"categories":null,"content":"喜欢羽毛球，游泳，看动漫，湖南伢子，特别能吃辣\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"e7488a3910324de9929d24ca2288bbc7","permalink":"https://nju-ee.github.io/author/tiejian-tang%E5%94%90%E9%93%81%E5%81%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tiejian-tang%E5%94%90%E9%93%81%E5%81%A5/","section":"authors","summary":"喜欢羽毛球，游泳，看动漫，湖南伢子，特别能吃辣","tags":null,"title":"Tiejian Tang(唐铁健)","type":"authors"},{"authors":["蔡文聪"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"47e5a4405627ad6402085ce1bf9b43b1","permalink":"https://nju-ee.github.io/author/wencong-cai%E8%94%A1%E6%96%87%E8%81%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wencong-cai%E8%94%A1%E6%96%87%E8%81%AA/","section":"authors","summary":"","tags":null,"title":"Wencong Cai(蔡文聪)","type":"authors"},{"authors":["王祥祥"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"6c3e5110295a92172e68ef64afccbd71","permalink":"https://nju-ee.github.io/author/xiangxiang-wang%E7%8E%8B%E7%A5%A5%E7%A5%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiangxiang-wang%E7%8E%8B%E7%A5%A5%E7%A5%A5/","section":"authors","summary":"","tags":null,"title":"Xiangxiang Wang(王祥祥)","type":"authors"},{"authors":["杨雄"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"cf8d3df0151b6542686b80091698c9a0","permalink":"https://nju-ee.github.io/author/xiong-yang%E6%9D%A8%E9%9B%84/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiong-yang%E6%9D%A8%E9%9B%84/","section":"authors","summary":"","tags":null,"title":"Xiong Yang(杨雄)","type":"authors"},{"authors":["陈叶朦"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"3d41a0d98761fb8f5c83f8dfe252ef6b","permalink":"https://nju-ee.github.io/author/yemeng-chen%E9%99%88%E5%8F%B6%E6%9C%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yemeng-chen%E9%99%88%E5%8F%B6%E6%9C%A6/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"yemeng Chen(陈叶朦)","type":"authors"},{"authors":["马依航"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"70eff90bc88e20830640ad5e398071bd","permalink":"https://nju-ee.github.io/author/yihang-ma%E9%A9%AC%E4%BE%9D%E8%88%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yihang-ma%E9%A9%AC%E4%BE%9D%E8%88%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Yihang Ma(马依航)","type":"authors"},{"authors":["侯悦"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"d9c52025fb4cd7669dca17895012e9be","permalink":"https://nju-ee.github.io/author/yue-hou%E4%BE%AF%E6%82%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yue-hou%E4%BE%AF%E6%82%A6/","section":"authors","summary":"","tags":null,"title":"Yue Hou(侯悦)","type":"authors"},{"authors":["凌煜清"],"categories":null,"content":"本人性格开朗，热爱探索，从事视频AI算法工作数年，对视觉AI方面的前沿技术抱有浓厚兴趣。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"40ddcfad620fcd4960329a95986e940d","permalink":"https://nju-ee.github.io/author/yuqing-ling%E5%87%8C%E7%85%9C%E6%B8%85/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuqing-ling%E5%87%8C%E7%85%9C%E6%B8%85/","section":"authors","summary":"本人性格开朗，热爱探索，从事视频AI算法工作数年，对视觉AI方面的前沿技术抱有浓厚兴趣。","tags":null,"title":"Yuqing Ling(凌煜清)","type":"authors"},{"authors":["朱雨秋"],"categories":null,"content":"南京大学电子科学与工程学院2024级研究生\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"2a8cdc1f77113e2dab663bdc2a6857c5","permalink":"https://nju-ee.github.io/author/yuqiu-zhu%E6%9C%B1%E9%9B%A8%E7%A7%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuqiu-zhu%E6%9C%B1%E9%9B%A8%E7%A7%8B/","section":"authors","summary":"南京大学电子科学与工程学院2024级研究生","tags":null,"title":"Yuqiu Zhu(朱雨秋)","type":"authors"},{"authors":["王之渊"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b6bb3ab316146419e470e248d397a13f","permalink":"https://nju-ee.github.io/author/zhiyuan-wang%E7%8E%8B%E4%B9%8B%E6%B8%8A/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiyuan-wang%E7%8E%8B%E4%B9%8B%E6%B8%8A/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhiyuan Wang(王之渊)","type":"authors"},{"authors":["谢子恩"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b44123dade96c394f3042dcca6ad619e","permalink":"https://nju-ee.github.io/author/zien-xie%E8%B0%A2%E5%AD%90%E6%81%A9/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zien-xie%E8%B0%A2%E5%AD%90%E6%81%A9/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zien Xie(谢子恩)","type":"authors"},{"authors":["高梓航"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"01d774f2f2748cdea7dc9aa1e5e9f3e1","permalink":"https://nju-ee.github.io/author/zihang-gao%E9%AB%98%E6%A2%93%E8%88%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zihang-gao%E9%AB%98%E6%A2%93%E8%88%AA/","section":"authors","summary":"","tags":null,"title":"Zihang Gao(高梓航)","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://nju-ee.github.io/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["翟本祥","徐一舫","Guofeng Zhang","李杨","都思丹"],"categories":null,"content":"","date":1757376000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1757376000,"objectID":"fe00340b955da7bce0f382e53dc6d90c","permalink":"https://nju-ee.github.io/publication/facesnap/","publishdate":"2025-09-09T00:00:00Z","relpermalink":"/publication/facesnap/","section":"publication","summary":"Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.","tags":["Source Themes"],"title":"FaceSnap: Enhanced ID-Fidelity Network forTuning-Free Portrait Customization","type":"publication"},{"authors":["徐一舫","翟本祥","孙运卓","李明","李杨","都思丹"],"categories":null,"content":"","date":1749600000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1749600000,"objectID":"2c6e33b9c3604a3e5ee30e3db7d4cd0d","permalink":"https://nju-ee.github.io/publication/hifi-portrait/","publishdate":"2025-06-11T00:00:00Z","relpermalink":"/publication/hifi-portrait/","section":"publication","summary":"Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.","tags":["Source Themes"],"title":"HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion","type":"publication"},{"authors":["曹靖豪","刘晟","武超凡","李杨","都思丹"],"categories":null,"content":"","date":1744243200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1744243200,"objectID":"74c435639e41ec56db2b3b2f39feb5a0","permalink":"https://nju-ee.github.io/publication/athena/","publishdate":"2025-04-10T00:00:00Z","relpermalink":"/publication/athena/","section":"publication","summary":"Large language models have brought revolutionary changes to autonomous driving algorithms, ushering them into the era of multimodality. However, existing vehicle trajectory planning methods primarily focus on obstacle avoidance in autonomous driving scenarios, overlooking interactions with entities within the scene, such as humans. In this letter, we propose a new research direction:vehicle trajectory planning that takes into account human actions. We establish ATHENA, the first autonomous driving dataset that integrates multimodal human actions, comprising 33,855 scenarios. Each scenario contains status information about ego vehicle, as well as pedestrian actions that actively or passively interact with the vehicle, such as signaling the vehicle to proceed by waving and the unexpected falls by pedestrians that force the vehicle to stop. Based on each type of interaction, ATHENA also provides the corresponding driving suggestions and the reasons behind them. Moreover, we present an LLM-based baseline that consists of two agents:the Action Understanding Agent and the Vehicle Control Agent. Our baseline implements the generation of driving recommendations and vehicle control functions, which are guided by pedestrian actions. Experiments demonstrate the effectiveness and strong performance of our method.","tags":["Source Themes"],"title":"ATHENA - Autonomous Vehicle Trajectory Planning Considered Human Action Awareness","type":"publication"},{"authors":["徐一舫","孙运卓","翟本祥","Wenxin Liang","李杨","都思丹"],"categories":null,"content":"","date":1740441600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1740441600,"objectID":"c6e1d619e1c4c6b09fcdc6bdd5331e77","permalink":"https://nju-ee.github.io/publication/zero-shot/","publishdate":"2025-02-25T00:00:00Z","relpermalink":"/publication/zero-shot/","section":"publication","summary":"The target of video moment retrieval (VMR) is predicting temporal spans within a video that semantically match a given linguistic query. Existing VMR methods based on multimodal large language models (MLLMs) overly rely on expensive high-quality datasets and time-consuming fine-tuning. Although some recent studies introduce a zero-shot setting to avoid fine-tuning, they overlook inherent language bias in the query, leading to erroneous localization. To tackle the aforementioned challenges, this paper proposes Moment-GPT, a tuning-free pipeline for zero-shot VMR utilizing frozen MLLMs. Specifically, we first employ LLaMA-3 to correct and rephrase the query to mitigate language bias. Subsequently, we design a span generator combined with MiniGPT-v2 to produce candidate spans adaptively. Finally, to leverage the video comprehension capabilities of MLLMs, we apply VideoChatGPT and span scorer to select the most appropriate spans. Our proposed method substantially outperforms the state-ofthe-art MLLM-based and zero-shot models on several public datasets, including QVHighlights, ActivityNet-Captions, and Charades-STA.","tags":["Source Themes"],"title":" Zero-shot Video Moment Retrieval via Off-the-shelf Multimodal Large Language Models","type":"publication"},{"authors":["戴京昭","李杨","都思丹"],"categories":null,"content":"","date":1738195200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1738195200,"objectID":"4bd4f7ec81892a8dcb54b4c45b44b26a","permalink":"https://nju-ee.github.io/publication/the-patch-based-multi-task/","publishdate":"2025-01-30T00:00:00Z","relpermalink":"/publication/the-patch-based-multi-task/","section":"publication","summary":"In this paper, we propose a global gaze following method using the patched‐based multi‐task multi‐scale reborn network (MMRGaze360) specifically designed for panorama images. Unlike existing approaches that rely on spherical networks or process only local regions, our architecture thoroughly accounts for the distortions introduced by the sphere‐to‐plane projection, enabling gaze following in comprehensive 360‐degree images. MMRGaze360 incorporates field‐of‐view (360‐FoV) and sight line (360‐Gaze) generators to model gaze behaviours and scene information in 360‐degree images. A multi‐task multi‐scale module is introduced to capture features from multiple patches centred around the estimated points located in the 360‐Gaze, using multi‐scale attention maps. These features, along with the 360‐FoV, are fused to produce a final heatmap. Additionally, we employ multi‐layer perceptions and convolutional networks using the reborn mechanism to enhance information usage and feature representation. Moreover, we establish a novel dataset, SRGaze360, which contains more conditions of the sphere‐to‐plane distortion. Experimental results on the GazeFollow360 and SRGaze360 datasets demonstrate the superiority of our method over previous works. It can be validated that our approach effectively addresses the limitations of 2D gaze following in handling out‐of‐frame gaze positions and distortions in 360‐degree images.","tags":["Source Themes"],"title":"HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion","type":"publication"},{"authors":["徐一舫","Chenyu Zhang","翟本祥","都思丹"],"categories":null,"content":"","date":1737936000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1737936000,"objectID":"2a4d51cacd71875b26183cffed344d74","permalink":"https://nju-ee.github.io/publication/hp3/","publishdate":"2025-01-27T00:00:00Z","relpermalink":"/publication/hp3/","section":"publication","summary":"Portrait personalization (PP) has garnered considerable attention recently due to its potential applications. However, existing methods only preserve the face region, with limited capability to customize head attributes, which restricts their practicality. To address these challenges, we introduce HP3, the first head-preserving framework for zero-shot PP. It can generate realistic portraits that preserve the source head, while controlling head expression and pose through the driving image. To accomplish this, we first design the head encoder and 3D reconstruction module to obtain head features and 3D priors. Next, the head controller is devised to align them, producing 3D-aware head conditions. These conditions are injected into UNet via the adaptive connector to achieve superior head preservation and control. Qualitative and quantitative experiments demonstrate that HP3 significantly outperforms SOTA methods. Our project is available at https://github.com/YoucanBaby/HP3.","tags":["Source Themes"],"title":"HP3: Tuning-Free Head-Preserving Portrait Personalization Via 3D-Controlled Diffusion Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1730160000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1730160000,"objectID":"efb4f93d86e7ae8f84dfe52fdffa3267","permalink":"https://nju-ee.github.io/post/25-01-03-nianhui/","publishdate":"2024-10-29T00:00:00Z","relpermalink":"/post/25-01-03-nianhui/","section":"post","summary":"","tags":null,"title":"我们实验室在2025年1月3日举办实验室年会！","type":"post"},{"authors":null,"categories":null,"content":"科研之余注意身体健康\n","date":1730160000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1730160000,"objectID":"d5d983bf2b8dd158db0968e24a189f87","permalink":"https://nju-ee.github.io/post/24-10-29-sports-meeting/","publishdate":"2024-10-29T00:00:00Z","relpermalink":"/post/24-10-29-sports-meeting/","section":"post","summary":"科研之余注意身体健康\n","tags":null,"title":"我们实验室在方肇周体育场举办趣味运动会！","type":"post"},{"authors":["胡雪娇","王师捷","李明","李杨","都思丹"],"categories":null,"content":"","date":1704844800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704844800,"objectID":"b9a4bb8bc81c9c01f82dd92e463162fb","permalink":"https://nju-ee.github.io/publication/online-detection-of-action/","publishdate":"2024-01-10T00:00:00Z","relpermalink":"/publication/online-detection-of-action/","section":"publication","summary":"The Online Detection of Action Start (ODAS) has attracted the attention of researchers because of its practical applications in areas such as security and emergency response. However, online detection of activity boundaries remains a challenging task due to the inherent ambiguity of boundary definition and the significant imbalance in the number of boundaries and nonboundary points. To address this issue, this study proposes a novel Distribution-aware Activity Boundary Representation (DABR) method that utilizes a continuous probability density function to smooth the probability of moments near activity boundaries. The proposed DABR reduces the penalty for detecting moments near ground-truth boundary points, while increasing the number of samples related to boundary points. Additionally, we introduce a two-stage framework that incorporates class-informed information in temporal localization for more efficient activity boundary localization. Extensive experiments demonstrate that our method achieves state-of-the-art results on two standard datasets, particularly exhibiting a significant improvement of 11.5% at average p-mAP on the THUMOS'14 dataset.","tags":["Source Themes"],"title":"Distribution-aware Activity Boundary Representation for Online Detection of Action Start in Untrimmed Videos","type":"publication"},{"authors":null,"categories":null,"content":"恭喜胡雪娇！\n论文详情\n","date":1704844800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704844800,"objectID":"123638f7a92e5e1f79e758c0cb94b399","permalink":"https://nju-ee.github.io/post/24-01-10-ieeespl-accept/","publishdate":"2024-01-10T00:00:00Z","relpermalink":"/post/24-01-10-ieeespl-accept/","section":"post","summary":"恭喜胡雪娇！\n","tags":null,"title":"我们的文章被 IEEE Signal Processing Letters 接收！","type":"post"},{"authors":["王品智","李明","曹靖豪","都思丹","李杨"],"categories":null,"content":"","date":1704499200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704499200,"objectID":"467374eb732d2f12d5e51be3f4c39b44","permalink":"https://nju-ee.github.io/publication/casomnimvs/","publishdate":"2024-01-06T00:00:00Z","relpermalink":"/publication/casomnimvs/","section":"publication","summary":"Estimating 360° depth from multiple cameras has been a challenging problem. However, existing methods often adopt a fixed-step spherical sweeping approach with densely sampled spheres and use numerous 3D convolutions in networks, which limits the speed of algorithms in practice. Additionally, obtaining high-precision depth maps of real scenes poses a challenge for the existing algorithms. In this paper, we design a cascade architecture using a dynamic spherical sweeping method that progressively refines the depth estimation from coarse to fine over multiple stages. The proposed method adaptively adjusts sweeping intervals and ranges based on the predicted depth and the uncertainty from the previous stage, resulting in a more efficient cost aggregation performance. The experimental results demonstrated that our method achieved state-of-the-art accuracy with reduced GPU memory usage and time consumption compared to the other methods. Furthermore, we illustrate that our method achieved satisfactory performance on real-world data, despite being trained on synthetic data, indicating its generalization potential and practical applicability.","tags":["Source Themes"],"title":"CasOmniMVS: Cascade Omnidirectional Depth Estimation with Dynamic Spherical Sweeping","type":"publication"},{"authors":["帅江海","李明","冯永康","李杨","都思丹"],"categories":null,"content":"","date":1700179200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1700179200,"objectID":"fec53cb389d41152c2d9c3e94deb752b","permalink":"https://nju-ee.github.io/publication/monocular/","publishdate":"2023-11-17T00:00:00Z","relpermalink":"/publication/monocular/","section":"publication","summary":"In the field of computer vision, monocular depth estimation has garnered significant attention as a research direction. However, current depth estimation methods often overlook the impact of depth range variations in indoor and outdoor scenes, consequently limiting the model’s generalization ability. To achieve high-precision depth estimation across different depth ranges, we propose a new method. We employ the pretrained model Dinov2 as encoder, combined with decoder based on CNN architecture, to enhance the network’s capacity for extracting global information from indoor and outdoor scenes. Also, we design a mapping module to transform diverse depth ranges into a unified 0-1 range, which can effectively adapt to indoor and outdoor scenes. We validate our method on the DIODE dataset, which comprises mixed indoor and outdoor scenes. Experimental results demonstrate that our method achieves higher depth estimation accuracy and stronger generalization performance when dealing with scenes of diverse depth ranges.","tags":["Source Themes"],"title":"A Monocular Depth Estimation Method for Indoor-Outdoor Scenes Based on Vision Transformer","type":"publication"},{"authors":["朱治亦","刘晟","帅江海","都思丹","李杨"],"categories":null,"content":"","date":1690416000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1690416000,"objectID":"9fe793a4a1424ef8e0769f12ccdf4f45","permalink":"https://nju-ee.github.io/publication/3d-associative-embedding/","publishdate":"2023-07-27T00:00:00Z","relpermalink":"/publication/3d-associative-embedding/","section":"publication","summary":"Most of the existing multi-view multi-person 3D human pose estimation methods predict the location of each joint of one target person following a top-down paradigm after finding his region. However, these works neglect the interference of others’ joints in the region. When the scene is crowded and the target person is surrounded by others, the information of his joints tends to be disturbed which results in significant errors in 3D results. To overcome this problem, this paper takes advantage of a bottom-up method in 2D pose estimation. We incorporate the Associative Embedding method into 3D pose estimation and propose a Voxel Hourglass Network to predict 3D heatmaps along with 3D tag-maps. As a result, the adverse effects from surrounding persons can be eliminated through the difference between tags. Moreover, we design a three-stage coarse-to-fine framework which can effectively reduce the quantization error. The size of the search space drops at each stage while the resolution increases. We test our method on the CMU Panoptic dataset where it outperforms the related top-down methods.","tags":["Source Themes"],"title":"3D Associative Embedding: Multi-View 3D Human Pose Estimation in Crowded Scenes","type":"publication"},{"authors":["戴京昭","李明","胡雪娇","李杨","都思丹"],"categories":null,"content":"","date":1686873600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1686873600,"objectID":"1daabdf0a7eae17201085e12fcac5aa2","permalink":"https://nju-ee.github.io/publication/gazefollowtr-a-method-of-gaze-following-with-reborn-mechanism/","publishdate":"2023-06-16T00:00:00Z","relpermalink":"/publication/gazefollowtr-a-method-of-gaze-following-with-reborn-mechanism/","section":"publication","summary":"Most existing 3D pose representations cannot completely decouple the overlapping two or more human joints of the same type. In this paper, the authors propose a novel 2.5 D representation of the human pose by projecting human joints in 3D space onto the three orthogonal planes. The authors apply for the first time the permutation module to a multi-person 3D human pose estimation task and use Geometric Constraints Loss (GCL) to guide the learning of the model. The authors overcome the negative effects of the inductive bias of convolutional neural networks (CNNs) by aligning the intermediate feature space with the output feature space. The effectiveness of the authors’ approach is validated on the carnegie mellon university (CMU) panoptic dataset and MuPoTS-3D dataset. The authors’ proposed representations can effectively decouple the human joints in their selected data from overlapping human joints.","tags":["Source Themes"],"title":"GazeFollowTR: A Method of Gaze Following with Reborn Mechanism","type":"publication"},{"authors":["刘晟","帅江海","李杨","都思丹"],"categories":null,"content":"","date":1678924800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1678924800,"objectID":"d5167a6a818f063476969201bc7cf6e4","permalink":"https://nju-ee.github.io/publication/mmda/","publishdate":"2023-03-16T00:00:00Z","relpermalink":"/publication/mmda/","section":"publication","summary":"Most existing 3D pose representations cannot completely decouple the overlapping two or more human joints of the same type. In this paper, the authors propose a novel 2.5 D representation of the human pose by projecting human joints in 3D space onto the three orthogonal planes. The authors apply for the first time the permutation module to a multi-person 3D human pose estimation task and use Geometric Constraints Loss (GCL) to guide the learning of the model. The authors overcome the negative effects of the inductive bias of convolutional neural networks (CNNs) by aligning the intermediate feature space with the output feature space. The effectiveness of the authors’ approach is validated on the carnegie mellon university (CMU) panoptic dataset and MuPoTS-3D dataset. The authors’ proposed representations can effectively decouple the human joints in their selected data from overlapping human joints.","tags":["Source Themes"],"title":"MMDA: Multi-person Marginal Distribution Awareness for Monocular 3D Pose Estimation","type":"publication"},{"authors":["戴京昭","胡雪娇","李明","李杨","都思丹"],"categories":null,"content":"","date":1674691200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1674691200,"objectID":"0cd3e4d991d0f6b6a9f128f5e0b4263c","permalink":"https://nju-ee.github.io/publication/food-analyses-in-computer-vision/","publishdate":"2023-01-26T00:00:00Z","relpermalink":"/publication/food-analyses-in-computer-vision/","section":"publication","summary":"With the rapid development of food production and health management, analyses of food samples have been essential for preventing diseases and understanding human culture. Recently, food analyses have become increasingly complex and are not limited in food categorization. They also contain many advanced tasks (e.g., nutrition estimation and recipe retrieval). From existing works, two points can be concluded. First, food features are much more comprehensive and sophisticated than general samples. Second, for food analyses, multiple learning strategies (MLSs) usually achieve outperformance over general deep learning methods. However, there are few survey papers reporting food analyses with MLSs, and the main factors lead to difficulty of operation. Therefore, we intend to conduct a survey for applications of MLSs to food analyses. In this survey paper, three types of common MLSs, which are multi-task learning (MTL), multi-view learning (MVL) and multi-scale learning (MSL) strategies, are presented in terms of their guidance, typical works, algorithms and final aggregation methods. Additionally, food characteristics are proposed to be closely related to the difficulty of food analyses. We comprehensively conclude food characteristics as nonrigid, complex in arrangement, and large (small) in intraclass (interclass) variance. Moreover, some experimental results of MLSs are also presented and analyzed in this paper. Based on these results, insightful suggestions for MLSs implementation are proposed. Finally, the promising tendency of MLSs applications in the future is discussed.","tags":["Source Themes"],"title":"The multi-learning for food analyses in computer vision: a survey","type":"publication"},{"authors":["李明","靳学乾"],"categories":null,"content":"","date":1656806400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656806400,"objectID":"11a05cf5f2d9a1da11ce003c554ac18e","permalink":"https://nju-ee.github.io/publication/mode2022/","publishdate":"2022-07-03T00:00:00Z","relpermalink":"/publication/mode2022/","section":"publication","summary":"In this paper, we propose a two-stage omnidirectional depth estimation framework with multi-view 360◦ cameras. The framework first estimates the depth maps from different camera pairs via omnidirectional stereo matching and then fuses the depth maps to achieverobustness against mud spots, water drops on camera lenses, and glare caused by intense light. We adopt spherical feature learning to address the distortion of panoramas. In addition, a synthetic 360◦ dataset consisting of 12K road scene panoramas and 3K ground truth depth maps is presented to train and evaluate 360◦ depth estimation algorithms. Our dataset takes soiled camera lenses and glare into consideration, which is more consistent with the real-world environment. Experimental results show that the proposed framework generates reliable results in both synthetic and real-world environments, and it achieves state-of-the-art performance on different datasets.","tags":["Source Themes"],"title":"MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras","type":"publication"},{"authors":null,"categories":null,"content":"恭喜李明和靳学乾！\n论文详情\n","date":1656806400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656806400,"objectID":"8f50ba83d2799aa01a0cda15908b510e","permalink":"https://nju-ee.github.io/post/22-07-03-eccv-paper-accept/","publishdate":"2022-07-03T00:00:00Z","relpermalink":"/post/22-07-03-eccv-paper-accept/","section":"post","summary":"恭喜李明和靳学乾！\n","tags":null,"title":"我们的文章被 ECCV 2022 接收！","type":"post"},{"authors":["胡雪娇","戴京昭","李明","彭成磊","李杨","都思丹"],"categories":null,"content":"","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656374400,"objectID":"8bade7f9269ab6a550d4308d444ceea6","permalink":"https://nju-ee.github.io/publication/online-human-action/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/publication/online-human-action/","section":"publication","summary":"To meet the demand for powerful models for practical applications in real time, the focus of research on human actions has shifted from offline detection to online and real-time understanding, such as driver-assistance systems, surveillance analysis, and robot services. In recent years, with the development of video recording acquisition technology and deep learning, online action analysis has made significant progress. However, there is a lack of comprehensive online surveys for online human action detection. In this survey, we discuss two hot real-time concerns online action detection and action anticipation. Online action/activity detection aims to determine whether an action is currently taking place and what kind of action it is in untrimmed videos. Action anticipation aims to anticipate human actions under limited observation of videos. Online action detection and anticipation require accuracy and low latency of detection when the video is partly observed. We present a comprehensive study that includes the definition, taxonomy, comparison of state-of-the-art techniques, datasets, metrics, challenges, and future directions. We hope that it will provide readers with a detailed understanding of the topic and inspiration for new research directions.","tags":["Source Themes"],"title":"Online human action detection and anticipation in videos: A survey","type":"publication"},{"authors":["王汉镕","李明","王杰","李杨","都思丹"],"categories":null,"content":"","date":1653177600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1653177600,"objectID":"d0ae4ab3272b2f9d13f523243d6ee089","permalink":"https://nju-ee.github.io/publication/optimization-about-stereo-image-depth-estimation/","publishdate":"2022-05-22T00:00:00Z","relpermalink":"/publication/optimization-about-stereo-image-depth-estimation/","section":"publication","summary":"The huge computational complexity and occlusion problems make stereo matching a major challenge. In this work, we use multi-baseline trinocular camera model to accelerate the stereo matching algorithms and improve the accuracy of disparity estimation. We propose a special scheme named the trinocular flexible disparity searching range (FDSR) to accelerate the stereo matching algorithms. In this scheme, we optimize stereo matching by reducing the disparity searching range. Based on FDSR, we proposed the FDSR-MCCNN for trinocular stereo matching. According to the evaluation results, the FDSR-MCCNN could not only reduce the computational complexity but also improve the accuracy. Moreover, the optimization schemes we designed can be extended to other stereo matching algorithms that possess pixel-wise matching cost calculations and aggregation steps. We proved that the proposed optimization methods for trinocular stereo matching are effective and that trinocular stereo matching is useful for either improving accuracy or reducing computational complexity.","tags":["Source Themes"],"title":" A Discussion of Optimization about Stereo Image Depth Estimation Based on Multi-baseline Trinocular Camera Model","type":"publication"},{"authors":["曹静怡","彭成磊","李杨","都思丹"],"categories":null,"content":"","date":1651795200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1651795200,"objectID":"23cabe02a7bd7fd3f3883a09f3ffbbc2","permalink":"https://nju-ee.github.io/publication/shadow-detection/","publishdate":"2022-05-06T00:00:00Z","relpermalink":"/publication/shadow-detection/","section":"publication","summary":"The existing shadow detection methods have achieved good results on standard shadow datasets such as SBU and UCF. However, in actual large-scale scenes, key objects covered by shadows are often regarded as shadows, which may harm computer vision tasks. In the paper, we are the first to propose the Object-aware Shadow Detection Network (OSD-Net) model for computer vision tasks in complex scenes. It introduces the direction-aware spatial context (DSC) module to detect shadows, uses semantic segmentation with Mask RCNN to extract key objects in the picture, and designs a function to perform mask fusion. Qualitative experiments have been performed to test OSD-Net on three public datasets commonly used in computer vision. Compared with popular shadow detection methods, OSD-Net is able to effectively protect the key targets in the picture from being misjudged as shadows, and ensure shadow detection accuracy.","tags":["Source Themes"],"title":"A Shadow Detection Method for Retaining Key Objects in Complex Scenes","type":"publication"},{"authors":["李兆旭","刘晟","白珏","彭成磊","李杨"],"categories":null,"content":"","date":1646352000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1646352000,"objectID":"52056beaa5fc3debacb153f8456193fb","permalink":"https://nju-ee.github.io/publication/3d-human-pose-estimation/","publishdate":"2022-03-04T00:00:00Z","relpermalink":"/publication/3d-human-pose-estimation/","section":"publication","summary":"Kinematic chain model is widely adopted in 3D human pose estimation tasks while it cannot accurately describe the curvature of the torso. This work proposes for the first time a novel model with spine curve to both express the movement of the limbs and the bending curvature of the torso. We parameterize the spine curve with the Bezier curve as it's controlled by anchor points. In this way, the estimation of the spine curve can be converted back to the estimation of points. The result shows that the anchor points of the spine curve can be estimated accurately by existing networks, similar to other joints. With the help of MOSH++ and SMPL model, the existing datasets can also be employed with our skeleton. We fit the SMPL models with the kinematic chain model and our model respectively, the fitting result shows that our model can provide richer information for the construction of human models.","tags":["Source Themes"],"title":" A Novel Skeleton-based Model with Spine for 3D Human Pose Estimation","type":"publication"},{"authors":["靳学乾","李明","彭成磊","都思丹","李杨"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1645488000,"objectID":"c2b6b8a42e68324696396eed30b1f1b4","permalink":"https://nju-ee.github.io/publication/removal-of-thermal-reflection/","publishdate":"2022-02-22T00:00:00Z","relpermalink":"/publication/removal-of-thermal-reflection/","section":"publication","summary":"Thermal imaging is a useful imaging technique in many scenarios. It can capture the temperature distribution of scenes in the dark and see through sparse smoke and dust. However, some surfaces such as steel and glass with high reflectivity lead to a reflection problem in thermal imaging, while heavy mist and gases lead to the occlusion problem. We proposed an efficient algorithm to solve the occlusion problem in our earlier work. The reflection in thermal images causes errors in detection and temperature measurement. Therefore, the precise model and efficient algorithms to solve this problem are in high demand. In this paper, we mainly model the reflection problem in thermal imaging and propose an algorithm to deal with it. In our experiments, a thermal camera array is built to capture the thermal light-field images. We first separate a part of the reflection pixels from thermal images based on the depth information. After that, the thermal reflection is removed by optimizing a designed cost function. The experiment results show that our reflection removal method can separate the thermal reflection with high precision, retain the objects in the scene, and get better performance than existing methods.","tags":["Source Themes"],"title":"Depth-based removal of thermal reflection with the light-field theory","type":"publication"},{"authors":["王杰","彭成磊","李明","李杨","都思丹"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1645488000,"objectID":"58eae5cc7ce780ed70bdea0fb29a70cd","permalink":"https://nju-ee.github.io/publication/study-of-stereo/","publishdate":"2022-02-22T00:00:00Z","relpermalink":"/publication/study-of-stereo/","section":"publication","summary":"The huge computational complexity, occlusion and low texture region problems make stereo matching a big challenge. In this work, we use multi-baseline trinocular camera model to study how to accelerate the stereo matching algorithms and improve the accuracy of disparity estimation. A special scheme named the trinocular dynamic disparity range (T-DDR) was designed to accelerate the stereo matching algorithms. In this scheme, we optimize matching cost calculation, cost aggregation and disparity computation steps by narrowing disparity searching range. Meanwhile, we designed another novel scheme called the trinocular disparity confidence measure (T-DCM) to improve the accuracy of the disparity map. Based on those, we proposed the semi-global matching with T-DDR (T-DDR-SGM) and T-DCM (T-DCM-SGM) algorithms for trinocular stereo matching. According to the evaluation results, the T-DDR-SGM could not only significantly reduce the computational complexity but also slightly improving the accuracy, while the T-DCM-SGM could excellently handle the occlusion and low texture region problems. Both of them achieved a better result. Moreover, the optimization schemes we designed can be extended to the other stereo matching algorithms which possesses pixel-wise matching cost calculation and aggregation steps not only the SGM. We proved that the proposed optimization methods for the trinocular stereo matching are effective and the trinocular stereo matching is useful for either improving accuracy or reducing computational complexity.","tags":["Source Themes"],"title":"The study of stereo matching optimization based on multi-baseline trinocular model","type":"publication"},{"authors":["白珏","彭成磊","李兆旭","都思丹","李杨"],"categories":null,"content":"","date":1635638400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1635638400,"objectID":"7ec6cb327dfff6077a251d89e7ddbc7f","permalink":"https://nju-ee.github.io/publication/large-angle-head-pose-estimation/","publishdate":"2021-10-31T00:00:00Z","relpermalink":"/publication/large-angle-head-pose-estimation/","section":"publication","summary":"Predicting Euler angles of head pose using end-to-end CNN from a single RGB image is a popular application in recent years. However, the existing methods ignored the information about the rotation order contained in the Euler angles, always following the traditional pitch-yaw-roll order. They also neglected the error sources from outlier samples with large-angle poses. We analyzed current shortcomings and made suggestions for improvement from the perspective of data distribution. We studied the influence of different rotation orders on the data distribution and showed choosing an appropriate rotation order to learn head pose can significantly optimize the data distribution and improve the prediction accuracy. Then a data enhancement method was proposed to increase the large-angle poses by rotating the 2D images randomly and solving the corresponding head poses, which can improve network performance on the large-angle poses. Evaluated on two popular networks and different datasets, our methods were proved to be effective and general.","tags":["Source Themes"],"title":"A Study of General Data Improvement for Large-Angle Head Pose Estimation","type":"publication"},{"authors":["李明","胡雪娇","戴京昭","李杨","都思丹"],"categories":null,"content":"","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1629676800,"objectID":"ec4f05c39ab2aeb2ac622a1914cc37f0","permalink":"https://nju-ee.github.io/publication/omnidirectional-stereo-depth/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/publication/omnidirectional-stereo-depth/","section":"publication","summary":"Omnidirectional depth estimation is an emerging research topic and has received significant attention in recent years. However, the existing methods were developed based on the theory of planar stereo matching; and introduce the nonlinear epipolar constraint and significant distortions of re-projections. In this paper, we propose a novel approach that use spherical CNNs and the epipolar constraint on sphere for omnidirectional depth estimation. We discuss the epipolar constraint for spherical stereo imaging and convert the nonlinear constraint on a planar projection to the linear constraint on a sphere. We then propose a Spherical Convolution Residual Network (SCRN) for omnidirectional depth estimation via the spherical linear epipolar constraint. The input equirectangular projection (ERP) images are sampled to spherical meshes and fed into SCRN to calculate spherical depth maps. For 2D visualization, we design a Planar Refinement Network (PRN) and adopt the cascade learning scheme to improve the accuracy of depth maps. This scheme reduces the errors caused by projection, interpolation, and the limitation of spherical representation. The experiment shows that our full scheme Cascade Spherical Depth Network (CSDNet) results in more accurate and detailed depth maps with lower errors, as compared to recent seminal works. Our approach yields the comparable performance to the other state-of-the-art works on the omnidirectional stereo datasets with less number of parameters. The effectiveness of the spherical network and the cascade learning scheme is validated, and the influence of spherical sampling density is also discussed.","tags":["Source Themes"],"title":"Omnidirectional stereo depth estimation based on spherical deep network","type":"publication"},{"authors":["徐一舫","彭成磊","李明","李杨","都思丹"],"categories":null,"content":"","date":1625443200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1625443200,"objectID":"f210674663ab81d122743f163bcda1c3","permalink":"https://nju-ee.github.io/publication/pyramid-feature-attention-network-for-monocular-depth-prediction/","publishdate":"2021-07-05T00:00:00Z","relpermalink":"/publication/pyramid-feature-attention-network-for-monocular-depth-prediction/","section":"publication","summary":"Deep convolutional neural networks (DCNNs) have achieved great success in monocular depth estimation (MDE). However, few existing works take the contributions for MDE of different levels feature maps into account, leading to inaccurate spatial layout, ambiguous boundaries and discontinuous object surface in the prediction. To better tackle these problems, we propose a Pyramid Feature Attention Network (PFANet) to improve the high-level context features and lowlevel spatial features. In the proposed PFANet, we design a Dual-scale Channel Attention Module (DCAM) to employ channel attention in different scales, which aggregate global context and local information from the high-level feature maps. To exploit the spatial relationship of visual features, we design a Spatial Pyramid Attention Module (SPAM) which can guide the network attention to multi-scale detailed information in the low-level feature maps. Finally, we introduce scale-invariant gradient loss to increase the penalty on errors in depth-wise discontinuous regions. Experimental results show that our method outperforms state-of-the-art methods on the KITTI dataset.","tags":["Source Themes"],"title":"Pyramid Feature Attention Network for Monocular Depth Prediction","type":"publication"},{"authors":["陈佟","彭成磊","李明","陈旭东","都思丹","李杨"],"categories":null,"content":"","date":1617840000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1617840000,"objectID":"c0ff3174772a708789929ffb77806c9e","permalink":"https://nju-ee.github.io/publication/quantitative-analyzing/","publishdate":"2021-04-08T00:00:00Z","relpermalink":"/publication/quantitative-analyzing/","section":"publication","summary":"The movements of mitochondrial are especially critical for neuronal growth and function. However, to analyze and quantify this process is technically challenging. Different from traditional hand-drawn method which lacks efficiency, we focus on automatic methods, which consist three key aspects (image enhancement, trajectories tracking and quantitative analyzing) and provide a discussion about different issues in these steps.","tags":["Source Themes"],"title":"A Review on Quantitative Analyzing Axonal Transport of Mitochondria","type":"publication"},{"authors":["黎琪","Ma Yazhen","彭成磊","Guo Bin","都思丹","李杨"],"categories":null,"content":"","date":1617840000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1617840000,"objectID":"1e38330cc0ce8865a3bee6791e28dd2a","permalink":"https://nju-ee.github.io/publication/diabetic-retinopathy-lesion-detection/","publishdate":"2021-04-08T00:00:00Z","relpermalink":"/publication/diabetic-retinopathy-lesion-detection/","section":"publication","summary":"Diabetic retinopathy (DR) is one of the leading causes of preventable blindness. It's urgent to develop reliable methods for auto DR screening, the key of which is the detection of lesions. This paper presents an innovative method to detect DR lesions in pixel-level. We design a multi-scale Convolution Neural Network (CNN) that make the full use of multiple different scales with complementary image information. Experiments are carried out on both private and public datasets. Results show that multi-scale CNN model outperforms single-scale CNN model and other state-of-the-art approaches.","tags":["Source Themes"],"title":"Pixel-level Diabetic Retinopathy Lesion Detection Using Multi-scale Convolutional Neural Network","type":"publication"},{"authors":["周子豪","李杨","彭成磊","王汉镕","都思丹"],"categories":null,"content":"","date":1613952000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1613952000,"objectID":"ccd2f774b5273192913294d51d170c85","permalink":"https://nju-ee.github.io/publication/image-processing-facilitating-retinanet-for-detecting-small-objects/","publishdate":"2021-02-22T00:00:00Z","relpermalink":"/publication/image-processing-facilitating-retinanet-for-detecting-small-objects/","section":"publication","summary":"Detecting small objects is a challenging task in object detection due to low spatial resolution and interference by background. Specifically, one-stage detectors struggle with small objects for they generate worse candidate bounding boxes. In this paper, several modifications are made to the original Retinanet to tackle the problem. Dilated convolutional layers are added to the backbone to get fined-grained features along with semantic information. The gradient of loss function is increased near the origin to enhance the quality of candidate boxes for small objects. A novel feature fusion method is also proposed to directly guide low-level features with semantic information. Significant improvement of 5.1 mAP can be seen when evaluating on MOCOD small object dataset, which contains a large amount of small objects. Our method can be easily migrated to other backbone networks with feature pyramids for detecting small objects.","tags":["Source Themes"],"title":"Image Processing: Facilitating Retinanet for Detecting Small Objects","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://nju-ee.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://nju-ee.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"NJU-EE Autonomous Driving Research Group专注于以视觉为基础的感知算法 深度估计 我们研究了多摄像头系统的深度估算，以获取自动驾驶系统周围环境的结构信息。下面是一段演示视频。 感算一体 我们提出了一种基于近传感器计算架构的全向深度估计系统。所提出的工作通过任务分区实现了负载平衡，同时通过特征投影和可学习编解码器降低了传输带宽。\n占据预测网络 基于我们实验室的深度估计网络提供的深度信息，我们提出了一个基于圆柱体素的“Sketch-Coloring“框架。 下面是一段演示视频。实验结果表明，我们的“Sketch-Coloring”网络能显著提高三维感知性能，尤其是在邻近区域，这使我们的方法成为自动驾驶感知的一个有前途的解决方案。\n更多相关资料\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"b75c4ef86e2b546bd4d7028b3b0610c6","permalink":"https://nju-ee.github.io/direction/autonomous/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/direction/autonomous/","section":"direction","summary":"NJU-EE Autonomous Driving Research Group专注于以视觉为基础的感知算法 深度估计 我们研究了多摄像头系统的深度估算，以获取自动驾驶系统周围环境的结构信息。下面是一段演示视频。 感算一体 我们提出了一种基于近传感器计算架构的全向深度估计系统。所提出的工作通过任务分区实现了负载平衡，同时通过特征投影和可学习编解码器降低了传输带宽。\n占据预测网络 基于我们实验室的深度估计网络提供的深度信息，我们提出了一个基于圆柱体素的“Sketch-Coloring“框架。 下面是一段演示视频。实验结果表明，我们的“Sketch-Coloring”网络能显著提高三维感知性能，尤其是在邻近区域，这使我们的方法成为自动驾驶感知的一个有前途的解决方案。\n更多相关资料","tags":null,"title":"NJU-EE Autonomous Driving Research Group","type":"direction"}]