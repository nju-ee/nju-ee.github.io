[{"authors":["李明"],"categories":null,"content":"","date":1704844800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704844800,"objectID":"926f2044e1048ef84ac37cc73a1bf8ff","permalink":"https://nju-ee.github.io/author/ming-li%E6%9D%8E%E6%98%8E/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ming-li%E6%9D%8E%E6%98%8E/","section":"authors","summary":"","tags":null,"title":"Ming Li(李明)","type":"authors"},{"authors":["王师捷"],"categories":null,"content":"","date":1704844800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704844800,"objectID":"4de19d0a657045458a77ad2bcde5977f","permalink":"https://nju-ee.github.io/author/shijie-wang%E7%8E%8B%E5%B8%88%E6%8D%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shijie-wang%E7%8E%8B%E5%B8%88%E6%8D%B7/","section":"authors","summary":"","tags":null,"title":"Shijie Wang(王师捷)","type":"authors"},{"authors":["都思丹"],"categories":null,"content":"个人简历： 1997年毕业于南京大学，获博士学位。南京大学电子科学与工程学院电子工程系教授，图像工程实验室主任。电路与系统学科带头人，南京市321工程有突出贡献中青年专家。先后在日本富士通、美国西北大学访问研究， 曾挂职宜兴市科技副市长。主持承担了自然科学基金、国际合作重大项目，以及江苏省科技支撑计划等项目。发表SCI、EI学术论文100余篇。获授权国家发明专利20余项，江苏省科学技术三等奖、江苏省优秀软件产品奖，中国计算机行业发展成就奖（团体），日本富士通优秀业绩奖，南京大学青年骨干教师奖。\n研究方向：  图像处理与控制 机器学习与算法 密码与信息安全  主要课程：  本科生：概率论与随机过程，图像处理 研究生：信号处理中的数学方法，现代图像工程  ","date":1704844800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704844800,"objectID":"aec4f54067fa5320f4f03fe6a507c3f7","permalink":"https://nju-ee.github.io/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/","section":"authors","summary":"个人简历： 1997年毕业于南京大学，获博士学位。南京大学电子科学与工程学院电子工程系教授，图像工程实验室主任。电路与系统学科带头人，南京市321工程有突出贡献中青年专家。先后在日本富士通、美国西北大学访问研究， 曾挂职宜兴市科技副市长。主持承担了自然科学基金、国际合作重大项目，以及江苏省科技支撑计划等项目。发表SCI、EI学术论文100余篇。获授权国家发明专利20余项，江苏省科学技术三等奖、江苏省优秀软件产品奖，中国计算机行业发展成就奖（团体），日本富士通优秀业绩奖，南京大学青年骨干教师奖。\n研究方向：  图像处理与控制 机器学习与算法 密码与信息安全  主要课程：  本科生：概率论与随机过程，图像处理 研究生：信号处理中的数学方法，现代图像工程  ","tags":null,"title":"Sidan Du(都思丹)","type":"authors"},{"authors":["胡雪娇"],"categories":null,"content":"","date":1704844800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704844800,"objectID":"c1943d631536dada07c9deecb391dee1","permalink":"https://nju-ee.github.io/author/xuejiao-hu%E8%83%A1%E9%9B%AA%E5%A8%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xuejiao-hu%E8%83%A1%E9%9B%AA%E5%A8%87/","section":"authors","summary":"","tags":null,"title":"Xuejiao Hu(胡雪娇)","type":"authors"},{"authors":["李杨"],"categories":null,"content":"在计算机视觉领域。我们的研究兴趣集中利用计算机视觉技术对环境进行三维感知和对环境图像进行语义理解。我们围绕着机器人导航、自动驾驶等应用场景下的多种三维感知技术进行研究。在双目及多目立体视觉被动感知技术方面，我们首次提出了利用帧间相关性加速视频序列中的立体匹配问题，该方法在不降低视差计算精度的前提可以显著提升基于深度学习和非深度学习方法的立体匹配速度，我们在嵌入式系统中实现的双目立体视觉系统达到了国际先进水平。我们同时在激光雷达点云的语义分割、配准上也取得众多成果，利用算法优化和GPU加速我们实现在低功耗嵌入式平台上的实时三维重建。针对机器人导航、自动驾驶等应用场景，我们进一步发展和完善了双目视觉的同步定位与地图构建问题，提出并实现了利用深度图像加速地图构建。此外，我们在目标识别、图像语义分析等领域也取得若干成果，例如我们针对多模态图像中的目标识别问题提出了全新的深度网络架构和数据增强及训练方法，显著提升了算法的性能。\n","date":1704844800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704844800,"objectID":"f288ec240bf05ad2f6ed15f7234831d0","permalink":"https://nju-ee.github.io/author/yang-li%E6%9D%8E%E6%9D%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yang-li%E6%9D%8E%E6%9D%A8/","section":"authors","summary":"在计算机视觉领域。我们的研究兴趣集中利用计算机视觉技术对环境进行三维感知和对环境图像进行语义理解。我们围绕着机器人导航、自动驾驶等应用场景下的多种三维感知技术进行研究。在双目及多目立体视觉被动感知技术方面，我们首次提出了利用帧间相关性加速视频序列中的立体匹配问题，该方法在不降低视差计算精度的前提可以显著提升基于深度学习和非深度学习方法的立体匹配速度，我们在嵌入式系统中实现的双目立体视觉系统达到了国际先进水平。我们同时在激光雷达点云的语义分割、配准上也取得众多成果，利用算法优化和GPU加速我们实现在低功耗嵌入式平台上的实时三维重建。针对机器人导航、自动驾驶等应用场景，我们进一步发展和完善了双目视觉的同步定位与地图构建问题，提出并实现了利用深度图像加速地图构建。此外，我们在目标识别、图像语义分析等领域也取得若干成果，例如我们针对多模态图像中的目标识别问题提出了全新的深度网络架构和数据增强及训练方法，显著提升了算法的性能。","tags":null,"title":"Yang Li(李杨)","type":"authors"},{"authors":["曹靖豪"],"categories":null,"content":"这里是个人介绍\n","date":1704499200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704499200,"objectID":"da4ca16fe10d4dfa0e9d04f2315b9cb8","permalink":"https://nju-ee.github.io/author/jinghao-cao%E6%9B%B9%E9%9D%96%E8%B1%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jinghao-cao%E6%9B%B9%E9%9D%96%E8%B1%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jinghao Cao(曹靖豪)","type":"authors"},{"authors":["王品智"],"categories":null,"content":"这里是个人介绍\n","date":1704499200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704499200,"objectID":"73214478b073efac6dd77142f312f503","permalink":"https://nju-ee.github.io/author/pinzhi-wang%E7%8E%8B%E5%93%81%E6%99%BA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pinzhi-wang%E7%8E%8B%E5%93%81%E6%99%BA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Pinzhi Wang(王品智)","type":"authors"},{"authors":["帅江海"],"categories":null,"content":"这里是个人介绍\n","date":1700179200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1700179200,"objectID":"046f43ce15a0832fa8303702f4caa410","permalink":"https://nju-ee.github.io/author/jianghai-shuai%E5%B8%85%E6%B1%9F%E6%B5%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianghai-shuai%E5%B8%85%E6%B1%9F%E6%B5%B7/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jianghai Shuai(帅江海)","type":"authors"},{"authors":["冯永康"],"categories":null,"content":"Feel free to contact me.\n","date":1700179200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1700179200,"objectID":"b48fa1359440151b6c79c3ea1b02bec2","permalink":"https://nju-ee.github.io/author/yongkang-feng%E5%86%AF%E6%B0%B8%E5%BA%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yongkang-feng%E5%86%AF%E6%B0%B8%E5%BA%B7/","section":"authors","summary":"Feel free to contact me.","tags":null,"title":"Yongkang Feng(冯永康)","type":"authors"},{"authors":["刘晟"],"categories":null,"content":"这里是个人介绍\n","date":1690416e3,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1690416e3,"objectID":"0e126dad231a8b33624f1ff0ad18c1a4","permalink":"https://nju-ee.github.io/author/sheng-liu%E5%88%98%E6%99%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sheng-liu%E5%88%98%E6%99%9F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Sheng Liu(刘晟)","type":"authors"},{"authors":["朱治亦"],"categories":null,"content":"这里是个人介绍\n","date":1690416e3,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1690416e3,"objectID":"262c22709d80c22fea8d72432065c3f0","permalink":"https://nju-ee.github.io/author/zhiyi-zhu%E6%9C%B1%E6%B2%BB%E4%BA%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiyi-zhu%E6%9C%B1%E6%B2%BB%E4%BA%A6/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhiyi Zhu(朱治亦)","type":"authors"},{"authors":["靳学乾"],"categories":null,"content":"这里是个人介绍\n","date":1656806400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1656806400,"objectID":"d3f1f0cdfcb9e2e97fc2490fb6509516","permalink":"https://nju-ee.github.io/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Xueqian Jin(靳学乾)","type":"authors"},{"authors":["戴京昭"],"categories":null,"content":"这里是个人介绍\n","date":1656374400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1656374400,"objectID":"3cfa55795f69a5a08c4fddfe7da5203f","permalink":"https://nju-ee.github.io/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jingzhao Dai(戴京昭)","type":"authors"},{"authors":["王汉镕"],"categories":null,"content":"这里是个人介绍\n","date":1653177600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1653177600,"objectID":"4813d0567e2be557ca0e7e9adc939800","permalink":"https://nju-ee.github.io/author/hanrong-wang%E7%8E%8B%E6%B1%89%E9%95%95/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hanrong-wang%E7%8E%8B%E6%B1%89%E9%95%95/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Hanrong Wang(王汉镕)","type":"authors"},{"authors":["王杰"],"categories":null,"content":"","date":1653177600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1653177600,"objectID":"5a4344f2cc5135e24172554221ed61e0","permalink":"https://nju-ee.github.io/author/jie-wang%E7%8E%8B%E6%9D%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jie-wang%E7%8E%8B%E6%9D%B0/","section":"authors","summary":"","tags":null,"title":"Jie Wang(王杰)","type":"authors"},{"authors":["曹静怡"],"categories":null,"content":"这里是个人介绍\n","date":1651795200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1651795200,"objectID":"f6ff1b89fdfdcc22f3c6363d4db78b29","permalink":"https://nju-ee.github.io/author/jingyi-cao%E6%9B%B9%E9%9D%99%E6%80%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingyi-cao%E6%9B%B9%E9%9D%99%E6%80%A1/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jingyi Cao(曹静怡)","type":"authors"},{"authors":["白珏"],"categories":null,"content":"这里是个人介绍\n","date":1646352e3,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1646352e3,"objectID":"9bf3914ddb531aa38f7f52088e14376e","permalink":"https://nju-ee.github.io/author/jue-bai%E7%99%BD%E7%8F%8F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jue-bai%E7%99%BD%E7%8F%8F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jue Bai(白珏)","type":"authors"},{"authors":["李兆旭"],"categories":null,"content":"这里是个人介绍\n","date":1646352e3,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1646352e3,"objectID":"c7b764560c4c9d4d73658b276b5e454b","permalink":"https://nju-ee.github.io/author/zhaoxu-li%E6%9D%8E%E5%85%86%E6%97%AD/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhaoxu-li%E6%9D%8E%E5%85%86%E6%97%AD/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhaoxu Li(李兆旭)","type":"authors"},{"authors":["黎琪"],"categories":null,"content":"这里是个人介绍\n","date":161784e4,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":161784e4,"objectID":"f1ca98276535d17eb6f055f3c436b962","permalink":"https://nju-ee.github.io/author/qi-li%E9%BB%8E%E7%90%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qi-li%E9%BB%8E%E7%90%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Qi Li(黎琪)","type":"authors"},{"authors":["陈佟"],"categories":null,"content":"这里是个人介绍\n","date":161784e4,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":161784e4,"objectID":"2b82ab97fce568f00e121b3821dc3ca8","permalink":"https://nju-ee.github.io/author/tong-chen%E9%99%88%E4%BD%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tong-chen%E9%99%88%E4%BD%9F/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Tong Chen(陈佟)","type":"authors"},{"authors":["陈旭东"],"categories":null,"content":"经历了紫荆 pt、GoAgent 时代，开眼看世界。 经过几年不务正业，精通各式语言和框架，如 C++、Makefile、CMake、Bash、Python、Latex、Markdown、HTML、CSS、JavaScript、Lua、Java、Hive SQL，以及各样系统和软件，如 Windows、Ubuntu、Microsoft Office、Microsoft Visio、Adobe Acrobat、Adobe Photoshop、SAI，的擅长领域和文档查询。\n最近觉得 NAS 和 pt 有点意思，可惜紫荆已经关站。 强烈安利 BYRPT ，这是我有账号的几个 pt 站里还没关的，欢迎大家找我玩~~\n","date":161784e4,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":161784e4,"objectID":"4426c4f7800d9d6ba95dcc000a955da9","permalink":"https://nju-ee.github.io/author/xudong-chen%E9%99%88%E6%97%AD%E4%B8%9C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xudong-chen%E9%99%88%E6%97%AD%E4%B8%9C/","section":"authors","summary":"经历了紫荆 pt、GoAgent 时代，开眼看世界。 经过几年不务正业，精通各式语言和框架，如 C++、Makefile、CMake、Bash、Python、Latex、Markdown、HTML、CSS、JavaScript、Lua、Java、Hive SQL，以及各样系统和软件，如 Windows、Ubuntu、Microsoft Office、Microsoft Visio、Adobe Acrobat、Adobe Photoshop、SAI，的擅长领域和文档查询。\n最近觉得 NAS 和 pt 有点意思，可惜紫荆已经关站。 强烈安利 BYRPT ，这是我有账号的几个 pt 站里还没关的，欢迎大家找我玩~~","tags":null,"title":"Xudong Chen(陈旭东)","type":"authors"},{"authors":["翟本祥"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"4a375a3af28d91dd09d2955f42507cf1","permalink":"https://nju-ee.github.io/author/benxiang-zhai%E7%BF%9F%E6%9C%AC%E7%A5%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/benxiang-zhai%E7%BF%9F%E6%9C%AC%E7%A5%A5/","section":"authors","summary":"","tags":null,"title":"Benxiang Zhai(翟本祥)","type":"authors"},{"authors":["武超凡"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b8b1ecd5414e7585506248eb6669b394","permalink":"https://nju-ee.github.io/author/chaofan-wu%E6%AD%A6%E8%B6%85%E5%87%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chaofan-wu%E6%AD%A6%E8%B6%85%E5%87%A1/","section":"authors","summary":"","tags":null,"title":"Chaofan Wu(武超凡)","type":"authors"},{"authors":["董晨"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"a7697d83fcea15e78896b80c27fe2534","permalink":"https://nju-ee.github.io/author/chen-dong%E8%91%A3%E6%99%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chen-dong%E8%91%A3%E6%99%A8/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Chen Dong(董晨)","type":"authors"},{"authors":["杨帆"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"08232687745d8a4d0349d036c8c5fcf3","permalink":"https://nju-ee.github.io/author/fan-yang%E6%9D%A8%E5%B8%86/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/fan-yang%E6%9D%A8%E5%B8%86/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Fan Yang(杨帆)","type":"authors"},{"authors":["李嘉恒"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"27879f078458565628243f2aaf610067","permalink":"https://nju-ee.github.io/author/jiaheng-li%E6%9D%8E%E5%98%89%E6%81%92/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaheng-li%E6%9D%8E%E5%98%89%E6%81%92/","section":"authors","summary":"","tags":null,"title":"Jiaheng Li(李嘉恒)","type":"authors"},{"authors":["郑嘉璇"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"c9ef1dac8c4b1e372ccc0cde72333de4","permalink":"https://nju-ee.github.io/author/jiaxuan-zheng%E9%83%91%E5%98%89%E7%92%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaxuan-zheng%E9%83%91%E5%98%89%E7%92%87/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Jiaxuan Zheng(郑嘉璇)","type":"authors"},{"authors":["吴佳昱"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"65834053bfc801d06ba84965cabacfc0","permalink":"https://nju-ee.github.io/author/jiayu-wu%E5%90%B4%E4%BD%B3%E6%98%B1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiayu-wu%E5%90%B4%E4%BD%B3%E6%98%B1/","section":"authors","summary":"","tags":null,"title":"Jiayu Wu(吴佳昱)","type":"authors"},{"authors":["石立"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"7dc83e81e202bdc33b04b4500b2f77ab","permalink":"https://nju-ee.github.io/author/li-shi%E7%9F%B3%E7%AB%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-shi%E7%9F%B3%E7%AB%8B/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Li Shi(石立)","type":"authors"},{"authors":["赵芮"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b3cc0ba2cc650ae09b0712998d97a0c3","permalink":"https://nju-ee.github.io/author/rui-zhao%E8%B5%B5%E8%8A%AE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rui-zhao%E8%B5%B5%E8%8A%AE/","section":"authors","summary":"","tags":null,"title":"Rui Zhao(赵芮)","type":"authors"},{"authors":["陆胜"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"3e403ff20ed614d1376c068f35edcafb","permalink":"https://nju-ee.github.io/author/sheng-lu%E9%99%86%E8%83%9C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sheng-lu%E9%99%86%E8%83%9C/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Sheng Lu(陆胜)","type":"authors"},{"authors":["许薯文"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"56100b1701554d958fb2e4677d931f4c","permalink":"https://nju-ee.github.io/author/shuwen-xu%E8%AE%B8%E8%96%AF%E6%96%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuwen-xu%E8%AE%B8%E8%96%AF%E6%96%87/","section":"authors","summary":"","tags":null,"title":"Shuwen Xu(许薯文)","type":"authors"},{"authors":["陆天昊"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"0700c0266630f9e8f4f9e155cd550b42","permalink":"https://nju-ee.github.io/author/tianhao-lu%E9%99%86%E5%A4%A9%E6%98%8A/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianhao-lu%E9%99%86%E5%A4%A9%E6%98%8A/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Tianhao Lu(陆天昊)","type":"authors"},{"authors":["唐铁健"],"categories":null,"content":"喜欢羽毛球，游泳，看动漫，湖南伢子，特别能吃辣\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"e7488a3910324de9929d24ca2288bbc7","permalink":"https://nju-ee.github.io/author/tiejian-tang%E5%94%90%E9%93%81%E5%81%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tiejian-tang%E5%94%90%E9%93%81%E5%81%A5/","section":"authors","summary":"喜欢羽毛球，游泳，看动漫，湖南伢子，特别能吃辣","tags":null,"title":"Tiejian Tang(唐铁健)","type":"authors"},{"authors":["蔡文聪"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"47e5a4405627ad6402085ce1bf9b43b1","permalink":"https://nju-ee.github.io/author/wencong-cai%E8%94%A1%E6%96%87%E8%81%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wencong-cai%E8%94%A1%E6%96%87%E8%81%AA/","section":"authors","summary":"","tags":null,"title":"Wencong Cai(蔡文聪)","type":"authors"},{"authors":["王祥祥"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"6c3e5110295a92172e68ef64afccbd71","permalink":"https://nju-ee.github.io/author/xiangxiang-wang%E7%8E%8B%E7%A5%A5%E7%A5%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiangxiang-wang%E7%8E%8B%E7%A5%A5%E7%A5%A5/","section":"authors","summary":"","tags":null,"title":"Xiangxiang Wang(王祥祥)","type":"authors"},{"authors":["杨雄"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"cf8d3df0151b6542686b80091698c9a0","permalink":"https://nju-ee.github.io/author/xiong-yang%E6%9D%A8%E9%9B%84/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiong-yang%E6%9D%A8%E9%9B%84/","section":"authors","summary":"","tags":null,"title":"Xiong Yang(杨雄)","type":"authors"},{"authors":["陈叶朦"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"3d41a0d98761fb8f5c83f8dfe252ef6b","permalink":"https://nju-ee.github.io/author/yemeng-chen%E9%99%88%E5%8F%B6%E6%9C%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yemeng-chen%E9%99%88%E5%8F%B6%E6%9C%A6/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"yemeng Chen(陈叶朦)","type":"authors"},{"authors":["徐一舫"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"bce2f47b2f7cdd5915fb051fcf786371","permalink":"https://nju-ee.github.io/author/yifang-xu%E5%BE%90%E4%B8%80%E8%88%AB/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yifang-xu%E5%BE%90%E4%B8%80%E8%88%AB/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Yifang Xu(徐一舫)","type":"authors"},{"authors":["马依航"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"70eff90bc88e20830640ad5e398071bd","permalink":"https://nju-ee.github.io/author/yihang-ma%E9%A9%AC%E4%BE%9D%E8%88%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yihang-ma%E9%A9%AC%E4%BE%9D%E8%88%AA/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Yihang Ma(马依航)","type":"authors"},{"authors":["侯悦"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"d9c52025fb4cd7669dca17895012e9be","permalink":"https://nju-ee.github.io/author/yue-hou%E4%BE%AF%E6%82%A6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yue-hou%E4%BE%AF%E6%82%A6/","section":"authors","summary":"","tags":null,"title":"Yue Hou(侯悦)","type":"authors"},{"authors":["凌煜清"],"categories":null,"content":"本人性格开朗，热爱探索，从事视频AI算法工作数年，对视觉AI方面的前沿技术抱有浓厚兴趣。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"40ddcfad620fcd4960329a95986e940d","permalink":"https://nju-ee.github.io/author/yuqing-ling%E5%87%8C%E7%85%9C%E6%B8%85/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuqing-ling%E5%87%8C%E7%85%9C%E6%B8%85/","section":"authors","summary":"本人性格开朗，热爱探索，从事视频AI算法工作数年，对视觉AI方面的前沿技术抱有浓厚兴趣。","tags":null,"title":"Yuqing Ling(凌煜清)","type":"authors"},{"authors":["朱雨秋"],"categories":null,"content":"南京大学电子科学与工程学院2024级研究生\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"2a8cdc1f77113e2dab663bdc2a6857c5","permalink":"https://nju-ee.github.io/author/yuqiu-zhu%E6%9C%B1%E9%9B%A8%E7%A7%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuqiu-zhu%E6%9C%B1%E9%9B%A8%E7%A7%8B/","section":"authors","summary":"南京大学电子科学与工程学院2024级研究生","tags":null,"title":"Yuqiu Zhu(朱雨秋)","type":"authors"},{"authors":["王之渊"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b6bb3ab316146419e470e248d397a13f","permalink":"https://nju-ee.github.io/author/zhiyuan-wang%E7%8E%8B%E4%B9%8B%E6%B8%8A/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiyuan-wang%E7%8E%8B%E4%B9%8B%E6%B8%8A/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zhiyuan Wang(王之渊)","type":"authors"},{"authors":["谢子恩"],"categories":null,"content":"这里是个人介绍\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b44123dade96c394f3042dcca6ad619e","permalink":"https://nju-ee.github.io/author/zien-xie%E8%B0%A2%E5%AD%90%E6%81%A9/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zien-xie%E8%B0%A2%E5%AD%90%E6%81%A9/","section":"authors","summary":"这里是个人介绍","tags":null,"title":"Zien Xie(谢子恩)","type":"authors"},{"authors":["高梓航"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"01d774f2f2748cdea7dc9aa1e5e9f3e1","permalink":"https://nju-ee.github.io/author/zihang-gao%E9%AB%98%E6%A2%93%E8%88%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zihang-gao%E9%AB%98%E6%A2%93%E8%88%AA/","section":"authors","summary":"","tags":null,"title":"Zihang Gao(高梓航)","type":"authors"},{"authors":["周子豪"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"0029828d3e9d8497ac33da8f4419d0e1","permalink":"https://nju-ee.github.io/author/zihao-zhou%E5%91%A8%E5%AD%90%E8%B1%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zihao-zhou%E5%91%A8%E5%AD%90%E8%B1%AA/","section":"authors","summary":"","tags":null,"title":"Zihao Zhou(周子豪)","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://nju-ee.github.io/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":null,"categories":null,"content":"科研之余注意身体健康\n","date":173016e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":173016e4,"objectID":"d5d983bf2b8dd158db0968e24a189f87","permalink":"https://nju-ee.github.io/post/24-10-29-sports-meeting/","publishdate":"2024-10-29T00:00:00Z","relpermalink":"/post/24-10-29-sports-meeting/","section":"post","summary":"科研之余注意身体健康\n","tags":null,"title":"我们实验室在方肇周体育场举办趣味运动会！","type":"post"},{"authors":["胡雪娇","王师捷","李明","李杨","都思丹"],"categories":null,"content":"","date":1704844800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704844800,"objectID":"b9a4bb8bc81c9c01f82dd92e463162fb","permalink":"https://nju-ee.github.io/publication/online-detection-of-action/","publishdate":"2024-01-10T00:00:00Z","relpermalink":"/publication/online-detection-of-action/","section":"publication","summary":"The Online Detection of Action Start (ODAS) has attracted the attention of researchers because of its practical applications in areas such as security and emergency response. However, online detection of activity boundaries remains a challenging task due to the inherent ambiguity of boundary definition and the significant imbalance in the number of boundaries and nonboundary points. To address this issue, this study proposes a novel Distribution-aware Activity Boundary Representation (DABR) method that utilizes a continuous probability density function to smooth the probability of moments near activity boundaries. The proposed DABR reduces the penalty for detecting moments near ground-truth boundary points, while increasing the number of samples related to boundary points. Additionally, we introduce a two-stage framework that incorporates class-informed information in temporal localization for more efficient activity boundary localization. Extensive experiments demonstrate that our method achieves state-of-the-art results on two standard datasets, particularly exhibiting a significant improvement of 11.5% at average p-mAP on the THUMOS'14 dataset.","tags":["Source Themes"],"title":"Distribution-aware Activity Boundary Representation for Online Detection of Action Start in Untrimmed Videos","type":"publication"},{"authors":null,"categories":null,"content":"恭喜胡雪娇！\n论文详情\n","date":1704844800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704844800,"objectID":"123638f7a92e5e1f79e758c0cb94b399","permalink":"https://nju-ee.github.io/post/24-01-10-ieeespl-accept/","publishdate":"2024-01-10T00:00:00Z","relpermalink":"/post/24-01-10-ieeespl-accept/","section":"post","summary":"恭喜胡雪娇！\n","tags":null,"title":"我们的文章被 IEEE Signal Processing Letters 接收！","type":"post"},{"authors":["王品智","李明","曹靖豪","都思丹","李杨"],"categories":null,"content":"","date":1704499200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704499200,"objectID":"467374eb732d2f12d5e51be3f4c39b44","permalink":"https://nju-ee.github.io/publication/casomnimvs/","publishdate":"2024-01-06T00:00:00Z","relpermalink":"/publication/casomnimvs/","section":"publication","summary":"Estimating 360° depth from multiple cameras has been a challenging problem. However, existing methods often adopt a fixed-step spherical sweeping approach with densely sampled spheres and use numerous 3D convolutions in networks, which limits the speed of algorithms in practice. Additionally, obtaining high-precision depth maps of real scenes poses a challenge for the existing algorithms. In this paper, we design a cascade architecture using a dynamic spherical sweeping method that progressively refines the depth estimation from coarse to fine over multiple stages. The proposed method adaptively adjusts sweeping intervals and ranges based on the predicted depth and the uncertainty from the previous stage, resulting in a more efficient cost aggregation performance. The experimental results demonstrated that our method achieved state-of-the-art accuracy with reduced GPU memory usage and time consumption compared to the other methods. Furthermore, we illustrate that our method achieved satisfactory performance on real-world data, despite being trained on synthetic data, indicating its generalization potential and practical applicability.","tags":["Source Themes"],"title":"CasOmniMVS: Cascade Omnidirectional Depth Estimation with Dynamic Spherical Sweeping","type":"publication"},{"authors":["帅江海","李明","冯永康","李杨","都思丹"],"categories":null,"content":"","date":1700179200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1700179200,"objectID":"fec53cb389d41152c2d9c3e94deb752b","permalink":"https://nju-ee.github.io/publication/monocular/","publishdate":"2023-11-17T00:00:00Z","relpermalink":"/publication/monocular/","section":"publication","summary":"In the field of computer vision, monocular depth estimation has garnered significant attention as a research direction. However, current depth estimation methods often overlook the impact of depth range variations in indoor and outdoor scenes, consequently limiting the model’s generalization ability. To achieve high-precision depth estimation across different depth ranges, we propose a new method. We employ the pretrained model Dinov2 as encoder, combined with decoder based on CNN architecture, to enhance the network’s capacity for extracting global information from indoor and outdoor scenes. Also, we design a mapping module to transform diverse depth ranges into a unified 0-1 range, which can effectively adapt to indoor and outdoor scenes. We validate our method on the DIODE dataset, which comprises mixed indoor and outdoor scenes. Experimental results demonstrate that our method achieves higher depth estimation accuracy and stronger generalization performance when dealing with scenes of diverse depth ranges.","tags":["Source Themes"],"title":"A Monocular Depth Estimation Method for Indoor-Outdoor Scenes Based on Vision Transformer","type":"publication"},{"authors":["朱治亦","刘晟","帅江海","都思丹","李杨"],"categories":null,"content":"","date":1690416e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1690416e3,"objectID":"9fe793a4a1424ef8e0769f12ccdf4f45","permalink":"https://nju-ee.github.io/publication/3d-associative-embedding/","publishdate":"2023-07-27T00:00:00Z","relpermalink":"/publication/3d-associative-embedding/","section":"publication","summary":"Most of the existing multi-view multi-person 3D human pose estimation methods predict the location of each joint of one target person following a top-down paradigm after finding his region. However, these works neglect the interference of others’ joints in the region. When the scene is crowded and the target person is surrounded by others, the information of his joints tends to be disturbed which results in significant errors in 3D results. To overcome this problem, this paper takes advantage of a bottom-up method in 2D pose estimation. We incorporate the Associative Embedding method into 3D pose estimation and propose a Voxel Hourglass Network to predict 3D heatmaps along with 3D tag-maps. As a result, the adverse effects from surrounding persons can be eliminated through the difference between tags. Moreover, we design a three-stage coarse-to-fine framework which can effectively reduce the quantization error. The size of the search space drops at each stage while the resolution increases. We test our method on the CMU Panoptic dataset where it outperforms the related top-down methods.","tags":["Source Themes"],"title":"3D Associative Embedding: Multi-View 3D Human Pose Estimation in Crowded Scenes","type":"publication"},{"authors":["李明","靳学乾"],"categories":null,"content":"","date":1656806400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656806400,"objectID":"11a05cf5f2d9a1da11ce003c554ac18e","permalink":"https://nju-ee.github.io/publication/mode2022/","publishdate":"2022-07-03T00:00:00Z","relpermalink":"/publication/mode2022/","section":"publication","summary":"In this paper, we propose a two-stage omnidirectional depth estimation framework with multi-view 360◦ cameras. The framework first estimates the depth maps from different camera pairs via omnidirectional stereo matching and then fuses the depth maps to achieverobustness against mud spots, water drops on camera lenses, and glare caused by intense light. We adopt spherical feature learning to address the distortion of panoramas. In addition, a synthetic 360◦ dataset consisting of 12K road scene panoramas and 3K ground truth depth maps is presented to train and evaluate 360◦ depth estimation algorithms. Our dataset takes soiled camera lenses and glare into consideration, which is more consistent with the real-world environment. Experimental results show that the proposed framework generates reliable results in both synthetic and real-world environments, and it achieves state-of-the-art performance on different datasets.","tags":["Source Themes"],"title":"MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras","type":"publication"},{"authors":null,"categories":null,"content":"恭喜李明和靳学乾！\n论文详情\n","date":1656806400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656806400,"objectID":"8f50ba83d2799aa01a0cda15908b510e","permalink":"https://nju-ee.github.io/post/22-07-03-eccv-paper-accept/","publishdate":"2022-07-03T00:00:00Z","relpermalink":"/post/22-07-03-eccv-paper-accept/","section":"post","summary":"恭喜李明和靳学乾！\n","tags":null,"title":"我们的文章被 ECCV 2022 接收！","type":"post"},{"authors":["胡雪娇","戴京昭","李明","彭成磊","李杨","都思丹"],"categories":null,"content":"","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656374400,"objectID":"8bade7f9269ab6a550d4308d444ceea6","permalink":"https://nju-ee.github.io/publication/online-human-action/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/publication/online-human-action/","section":"publication","summary":"To meet the demand for powerful models for practical applications in real time, the focus of research on human actions has shifted from offline detection to online and real-time understanding, such as driver-assistance systems, surveillance analysis, and robot services. In recent years, with the development of video recording acquisition technology and deep learning, online action analysis has made significant progress. However, there is a lack of comprehensive online surveys for online human action detection. In this survey, we discuss two hot real-time concerns online action detection and action anticipation. Online action/activity detection aims to determine whether an action is currently taking place and what kind of action it is in untrimmed videos. Action anticipation aims to anticipate human actions under limited observation of videos. Online action detection and anticipation require accuracy and low latency of detection when the video is partly observed. We present a comprehensive study that includes the definition, taxonomy, comparison of state-of-the-art techniques, datasets, metrics, challenges, and future directions. We hope that it will provide readers with a detailed understanding of the topic and inspiration for new research directions.","tags":["Source Themes"],"title":"Online human action detection and anticipation in videos: A survey","type":"publication"},{"authors":["王汉镕","李明","王杰","李杨","都思丹"],"categories":null,"content":"","date":1653177600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1653177600,"objectID":"d0ae4ab3272b2f9d13f523243d6ee089","permalink":"https://nju-ee.github.io/publication/optimization-about-stereo-image-depth-estimation/","publishdate":"2022-05-22T00:00:00Z","relpermalink":"/publication/optimization-about-stereo-image-depth-estimation/","section":"publication","summary":"The huge computational complexity and occlusion problems make stereo matching a major challenge. In this work, we use multi-baseline trinocular camera model to accelerate the stereo matching algorithms and improve the accuracy of disparity estimation. We propose a special scheme named the trinocular flexible disparity searching range (FDSR) to accelerate the stereo matching algorithms. In this scheme, we optimize stereo matching by reducing the disparity searching range. Based on FDSR, we proposed the FDSR-MCCNN for trinocular stereo matching. According to the evaluation results, the FDSR-MCCNN could not only reduce the computational complexity but also improve the accuracy. Moreover, the optimization schemes we designed can be extended to other stereo matching algorithms that possess pixel-wise matching cost calculations and aggregation steps. We proved that the proposed optimization methods for trinocular stereo matching are effective and that trinocular stereo matching is useful for either improving accuracy or reducing computational complexity.","tags":["Source Themes"],"title":" A Discussion of Optimization about Stereo Image Depth Estimation Based on Multi-baseline Trinocular Camera Model","type":"publication"},{"authors":["曹静怡","彭成磊","李杨","都思丹"],"categories":null,"content":"","date":1651795200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1651795200,"objectID":"23cabe02a7bd7fd3f3883a09f3ffbbc2","permalink":"https://nju-ee.github.io/publication/shadow-detection/","publishdate":"2022-05-06T00:00:00Z","relpermalink":"/publication/shadow-detection/","section":"publication","summary":"The existing shadow detection methods have achieved good results on standard shadow datasets such as SBU and UCF. However, in actual large-scale scenes, key objects covered by shadows are often regarded as shadows, which may harm computer vision tasks. In the paper, we are the first to propose the Object-aware Shadow Detection Network (OSD-Net) model for computer vision tasks in complex scenes. It introduces the direction-aware spatial context (DSC) module to detect shadows, uses semantic segmentation with Mask RCNN to extract key objects in the picture, and designs a function to perform mask fusion. Qualitative experiments have been performed to test OSD-Net on three public datasets commonly used in computer vision. Compared with popular shadow detection methods, OSD-Net is able to effectively protect the key targets in the picture from being misjudged as shadows, and ensure shadow detection accuracy.","tags":["Source Themes"],"title":"A Shadow Detection Method for Retaining Key Objects in Complex Scenes","type":"publication"},{"authors":["李兆旭","刘晟","白珏","彭成磊","李杨"],"categories":null,"content":"","date":1646352e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1646352e3,"objectID":"52056beaa5fc3debacb153f8456193fb","permalink":"https://nju-ee.github.io/publication/3d-human-pose-estimation/","publishdate":"2022-03-04T00:00:00Z","relpermalink":"/publication/3d-human-pose-estimation/","section":"publication","summary":"Kinematic chain model is widely adopted in 3D human pose estimation tasks while it cannot accurately describe the curvature of the torso. This work proposes for the first time a novel model with spine curve to both express the movement of the limbs and the bending curvature of the torso. We parameterize the spine curve with the Bezier curve as it's controlled by anchor points. In this way, the estimation of the spine curve can be converted back to the estimation of points. The result shows that the anchor points of the spine curve can be estimated accurately by existing networks, similar to other joints. With the help of MOSH++ and SMPL model, the existing datasets can also be employed with our skeleton. We fit the SMPL models with the kinematic chain model and our model respectively, the fitting result shows that our model can provide richer information for the construction of human models.","tags":["Source Themes"],"title":" A Novel Skeleton-based Model with Spine for 3D Human Pose Estimation","type":"publication"},{"authors":["靳学乾","李明","彭成磊","都思丹","李杨"],"categories":null,"content":"","date":1645488e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1645488e3,"objectID":"c2b6b8a42e68324696396eed30b1f1b4","permalink":"https://nju-ee.github.io/publication/removal-of-thermal-reflection/","publishdate":"2022-02-22T00:00:00Z","relpermalink":"/publication/removal-of-thermal-reflection/","section":"publication","summary":"Thermal imaging is a useful imaging technique in many scenarios. It can capture the temperature distribution of scenes in the dark and see through sparse smoke and dust. However, some surfaces such as steel and glass with high reflectivity lead to a reflection problem in thermal imaging, while heavy mist and gases lead to the occlusion problem. We proposed an efficient algorithm to solve the occlusion problem in our earlier work. The reflection in thermal images causes errors in detection and temperature measurement. Therefore, the precise model and efficient algorithms to solve this problem are in high demand. In this paper, we mainly model the reflection problem in thermal imaging and propose an algorithm to deal with it. In our experiments, a thermal camera array is built to capture the thermal light-field images. We first separate a part of the reflection pixels from thermal images based on the depth information. After that, the thermal reflection is removed by optimizing a designed cost function. The experiment results show that our reflection removal method can separate the thermal reflection with high precision, retain the objects in the scene, and get better performance than existing methods.","tags":["Source Themes"],"title":"Depth-based removal of thermal reflection with the light-field theory","type":"publication"},{"authors":["王杰","彭成磊","李明","李杨","都思丹"],"categories":null,"content":"","date":1645488e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1645488e3,"objectID":"58eae5cc7ce780ed70bdea0fb29a70cd","permalink":"https://nju-ee.github.io/publication/study-of-stereo/","publishdate":"2022-02-22T00:00:00Z","relpermalink":"/publication/study-of-stereo/","section":"publication","summary":"The huge computational complexity, occlusion and low texture region problems make stereo matching a big challenge. In this work, we use multi-baseline trinocular camera model to study how to accelerate the stereo matching algorithms and improve the accuracy of disparity estimation. A special scheme named the trinocular dynamic disparity range (T-DDR) was designed to accelerate the stereo matching algorithms. In this scheme, we optimize matching cost calculation, cost aggregation and disparity computation steps by narrowing disparity searching range. Meanwhile, we designed another novel scheme called the trinocular disparity confidence measure (T-DCM) to improve the accuracy of the disparity map. Based on those, we proposed the semi-global matching with T-DDR (T-DDR-SGM) and T-DCM (T-DCM-SGM) algorithms for trinocular stereo matching. According to the evaluation results, the T-DDR-SGM could not only significantly reduce the computational complexity but also slightly improving the accuracy, while the T-DCM-SGM could excellently handle the occlusion and low texture region problems. Both of them achieved a better result. Moreover, the optimization schemes we designed can be extended to the other stereo matching algorithms which possesses pixel-wise matching cost calculation and aggregation steps not only the SGM. We proved that the proposed optimization methods for the trinocular stereo matching are effective and the trinocular stereo matching is useful for either improving accuracy or reducing computational complexity.","tags":["Source Themes"],"title":"The study of stereo matching optimization based on multi-baseline trinocular model","type":"publication"},{"authors":["白珏","彭成磊","李兆旭","都思丹","李杨"],"categories":null,"content":"","date":1635638400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1635638400,"objectID":"7ec6cb327dfff6077a251d89e7ddbc7f","permalink":"https://nju-ee.github.io/publication/large-angle-head-pose-estimation/","publishdate":"2021-10-31T00:00:00Z","relpermalink":"/publication/large-angle-head-pose-estimation/","section":"publication","summary":"Predicting Euler angles of head pose using end-to-end CNN from a single RGB image is a popular application in recent years. However, the existing methods ignored the information about the rotation order contained in the Euler angles, always following the traditional pitch-yaw-roll order. They also neglected the error sources from outlier samples with large-angle poses. We analyzed current shortcomings and made suggestions for improvement from the perspective of data distribution. We studied the influence of different rotation orders on the data distribution and showed choosing an appropriate rotation order to learn head pose can significantly optimize the data distribution and improve the prediction accuracy. Then a data enhancement method was proposed to increase the large-angle poses by rotating the 2D images randomly and solving the corresponding head poses, which can improve network performance on the large-angle poses. Evaluated on two popular networks and different datasets, our methods were proved to be effective and general.","tags":["Source Themes"],"title":"A Study of General Data Improvement for Large-Angle Head Pose Estimation","type":"publication"},{"authors":["李明","胡雪娇","戴京昭","李杨","都思丹"],"categories":null,"content":"","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1629676800,"objectID":"ec4f05c39ab2aeb2ac622a1914cc37f0","permalink":"https://nju-ee.github.io/publication/omnidirectional-stereo-depth/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/publication/omnidirectional-stereo-depth/","section":"publication","summary":"Omnidirectional depth estimation is an emerging research topic and has received significant attention in recent years. However, the existing methods were developed based on the theory of planar stereo matching; and introduce the nonlinear epipolar constraint and significant distortions of re-projections. In this paper, we propose a novel approach that use spherical CNNs and the epipolar constraint on sphere for omnidirectional depth estimation. We discuss the epipolar constraint for spherical stereo imaging and convert the nonlinear constraint on a planar projection to the linear constraint on a sphere. We then propose a Spherical Convolution Residual Network (SCRN) for omnidirectional depth estimation via the spherical linear epipolar constraint. The input equirectangular projection (ERP) images are sampled to spherical meshes and fed into SCRN to calculate spherical depth maps. For 2D visualization, we design a Planar Refinement Network (PRN) and adopt the cascade learning scheme to improve the accuracy of depth maps. This scheme reduces the errors caused by projection, interpolation, and the limitation of spherical representation. The experiment shows that our full scheme Cascade Spherical Depth Network (CSDNet) results in more accurate and detailed depth maps with lower errors, as compared to recent seminal works. Our approach yields the comparable performance to the other state-of-the-art works on the omnidirectional stereo datasets with less number of parameters. The effectiveness of the spherical network and the cascade learning scheme is validated, and the influence of spherical sampling density is also discussed.","tags":["Source Themes"],"title":"Omnidirectional stereo depth estimation based on spherical deep network","type":"publication"},{"authors":["陈佟","彭成磊","李明","陈旭东","都思丹","李杨"],"categories":null,"content":"","date":161784e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":161784e4,"objectID":"c0ff3174772a708789929ffb77806c9e","permalink":"https://nju-ee.github.io/publication/quantitative-analyzing/","publishdate":"2021-04-08T00:00:00Z","relpermalink":"/publication/quantitative-analyzing/","section":"publication","summary":"The movements of mitochondrial are especially critical for neuronal growth and function. However, to analyze and quantify this process is technically challenging. Different from traditional hand-drawn method which lacks efficiency, we focus on automatic methods, which consist three key aspects (image enhancement, trajectories tracking and quantitative analyzing) and provide a discussion about different issues in these steps.","tags":["Source Themes"],"title":"A Review on Quantitative Analyzing Axonal Transport of Mitochondria","type":"publication"},{"authors":["黎琪","Ma Yazhen","彭成磊","Guo Bin","都思丹","李杨"],"categories":null,"content":"","date":161784e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":161784e4,"objectID":"1e38330cc0ce8865a3bee6791e28dd2a","permalink":"https://nju-ee.github.io/publication/diabetic-retinopathy-lesion-detection/","publishdate":"2021-04-08T00:00:00Z","relpermalink":"/publication/diabetic-retinopathy-lesion-detection/","section":"publication","summary":"Diabetic retinopathy (DR) is one of the leading causes of preventable blindness. It's urgent to develop reliable methods for auto DR screening, the key of which is the detection of lesions. This paper presents an innovative method to detect DR lesions in pixel-level. We design a multi-scale Convolution Neural Network (CNN) that make the full use of multiple different scales with complementary image information. Experiments are carried out on both private and public datasets. Results show that multi-scale CNN model outperforms single-scale CNN model and other state-of-the-art approaches.","tags":["Source Themes"],"title":"Pixel-level Diabetic Retinopathy Lesion Detection Using Multi-scale Convolutional Neural Network","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://nju-ee.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://nju-ee.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"NJU-EE Autonomous Driving Research Group专注于以视觉为基础的感知算法  深度估计 \n我们研究了多摄像头系统的深度估算，以获取自动驾驶系统周围环境的结构信息。下面是一段演示视频。   感算一体 \n我们提出了一种基于近传感器计算架构的全向深度估计系统。所提出的工作通过任务分区实现了负载平衡，同时通过特征投影和可学习编解码器降低了传输带宽。\n 占据预测网络 \n基于我们实验室的深度估计网络提供的深度信息，我们提出了一个基于圆柱体素的“Sketch-Coloring“框架。 下面是一段演示视频。实验结果表明，我们的“Sketch-Coloring”网络能显著提高三维感知性能，尤其是在邻近区域，这使我们的方法成为自动驾驶感知的一个有前途的解决方案。\n 更多相关资料\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"b75c4ef86e2b546bd4d7028b3b0610c6","permalink":"https://nju-ee.github.io/direction/autonomous/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/direction/autonomous/","section":"direction","summary":"","tags":null,"title":"NJU-EE Autonomous Driving Research Group","type":"direction"}]