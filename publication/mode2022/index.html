<!doctype html><html lang=zh-hans><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=description content="In this paper, we propose a two-stage omnidirectional depth estimation framework with multi-view 360◦ cameras. The framework first estimates the depth maps from different camera pairs via omnidirectional stereo matching and then fuses the depth maps to achieverobustness against mud spots, water drops on camera lenses, and glare caused by intense light. We adopt spherical feature learning to address the distortion of panoramas. In addition, a synthetic 360◦ dataset consisting of 12K road scene panoramas and 3K ground truth depth maps is presented to train and evaluate 360◦ depth estimation algorithms. Our dataset takes soiled camera lenses and glare into consideration, which is more consistent with the real-world environment. Experimental results show that the proposed framework generates reliable results in both synthetic and real-world environments, and it achieves state-of-the-art performance on different datasets."><link rel=alternate hreflang=zh-hans href=https://nju-ee.github.io/publication/mode2022/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.6d31e9cd41a84afcca84bc68494717ec.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://nju-ee.github.io/publication/mode2022/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="emAI @ NJU"><meta property="og:url" content="https://nju-ee.github.io/publication/mode2022/"><meta property="og:title" content="MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras | emAI @ NJU"><meta property="og:description" content="In this paper, we propose a two-stage omnidirectional depth estimation framework with multi-view 360◦ cameras. The framework first estimates the depth maps from different camera pairs via omnidirectional stereo matching and then fuses the depth maps to achieverobustness against mud spots, water drops on camera lenses, and glare caused by intense light. We adopt spherical feature learning to address the distortion of panoramas. In addition, a synthetic 360◦ dataset consisting of 12K road scene panoramas and 3K ground truth depth maps is presented to train and evaluate 360◦ depth estimation algorithms. Our dataset takes soiled camera lenses and glare into consideration, which is more consistent with the real-world environment. Experimental results show that the proposed framework generates reliable results in both synthetic and real-world environments, and it achieves state-of-the-art performance on different datasets."><meta property="og:image" content="https://nju-ee.github.io/publication/mode2022/featured.jpg"><meta property="twitter:image" content="https://nju-ee.github.io/publication/mode2022/featured.jpg"><meta property="og:locale" content="zh-Hans"><meta property="article:published_time" content="2022-07-03T00:00:00+00:00"><meta property="article:modified_time" content="2022-07-03T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://nju-ee.github.io/publication/mode2022/"},"headline":"MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras","image":["https://nju-ee.github.io/publication/mode2022/featured.jpg"],"datePublished":"2022-07-03T00:00:00Z","dateModified":"2022-07-03T00:00:00Z","author":{"@type":"Person","name":"李明"},"publisher":{"@type":"Organization","name":"emAI @ NJU","logo":{"@type":"ImageObject","url":"https://nju-ee.github.io/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_192x192_fill_lanczos_center_3.png"}},"description":"In this paper, we propose a two-stage omnidirectional depth estimation framework with multi-view 360◦ cameras. The framework first estimates the depth maps from different camera pairs via omnidirectional stereo matching and then fuses the depth maps to achieverobustness against mud spots, water drops on camera lenses, and glare caused by intense light. We adopt spherical feature learning to address the distortion of panoramas. In addition, a synthetic 360◦ dataset consisting of 12K road scene panoramas and 3K ground truth depth maps is presented to train and evaluate 360◦ depth estimation algorithms. Our dataset takes soiled camera lenses and glare into consideration, which is more consistent with the real-world environment. Experimental results show that the proposed framework generates reliable results in both synthetic and real-world environments, and it achieves state-of-the-art performance on different datasets."}</script><title>MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras | emAI @ NJU</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=11a05cf5f2d9a1da11ce003c554ac18e><script src=/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>搜索</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=搜索... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=搜索...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>emAI @ NJU</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>emAI @ NJU</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/><span>首页</span></a></li><li class=nav-item><a class=nav-link href=/post><span>新闻</span></a></li><li class=nav-item><a class=nav-link href=/people><span>人员</span></a></li><li class=nav-item><a class=nav-link href=/direction><span>研究方向</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>论文出版</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>联系我们</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>浅色</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>深色</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>自动</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras</h1><div class=article-metadata><div><span><a href=/author/ming-li%E6%9D%8E%E6%98%8E/>Ming Li(李明)</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span><a href=/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/>Xueqian Jin(靳学乾)</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i></div><span class=article-date>七月 2022</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=http://eprints.soton.ac.uk/352095/1/Cushen-IMV2013.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/nju-ee/MODE-2022 target=_blank rel=noopener>代码</a>
<a class="btn btn-outline-primary btn-page-header" href="https://www.youtube.com/watch?v=Fw-KR35UWgQ" target=_blank rel=noopener>视频</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:436px><div style=position:relative><img src=/publication/mode2022/featured_hu3d03a01dcc18bc5be0e67db3d8d209a6_214168_720x2500_fit_q75_h2_lanczos.webp width=720 height=436 alt class=featured-image></div></div><div class=article-container><h3>摘要</h3><p class=pub-abstract>In this paper, we propose a two-stage omnidirectional depth estimation framework with multi-view 360◦ cameras. The framework first estimates the depth maps from different camera pairs via omnidirectional stereo matching and then fuses the depth maps to achieverobustness against mud spots, water drops on camera lenses, and glare caused by intense light. We adopt spherical feature learning to address the distortion of panoramas. In addition, a synthetic 360◦ dataset consisting of 12K road scene panoramas and 3K ground truth depth maps is presented to train and evaluate 360◦ depth estimation algorithms. Our dataset takes soiled camera lenses and glare into consideration, which is more consistent with the real-world environment. Experimental results show that the proposed framework generates reliable results in both synthetic and real-world environments, and it achieves state-of-the-art performance on different datasets.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">类型</div><div class="col-12 col-md-9"><a href=/publication/#1>会议文章</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">出版物</div><div class="col-12 col-md-9">In <em>European Conference on Computer Vision</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tag/source-themes/>Source Themes</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://nju-ee.github.io/publication/mode2022/&text=MODE:%20Multi-view%20Omnidirectional%20Depth%20Estimation%20with%20360%c2%b0%20Cameras" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://nju-ee.github.io/publication/mode2022/&t=MODE:%20Multi-view%20Omnidirectional%20Depth%20Estimation%20with%20360%c2%b0%20Cameras" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=MODE:%20Multi-view%20Omnidirectional%20Depth%20Estimation%20with%20360%c2%b0%20Cameras&body=https://nju-ee.github.io/publication/mode2022/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://nju-ee.github.io/publication/mode2022/&title=MODE:%20Multi-view%20Omnidirectional%20Depth%20Estimation%20with%20360%c2%b0%20Cameras" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=MODE:%20Multi-view%20Omnidirectional%20Depth%20Estimation%20with%20360%c2%b0%20Cameras%20https://nju-ee.github.io/publication/mode2022/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://nju-ee.github.io/publication/mode2022/&title=MODE:%20Multi-view%20Omnidirectional%20Depth%20Estimation%20with%20360%c2%b0%20Cameras" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><div class=media-body><h5 class=card-title><a href=/author/ming-li%E6%9D%8E%E6%98%8E/>Ming Li(李明)</a></h5><h6 class=card-subtitle>硕博连读(2017-2024)</h6><p class=card-text>简略介绍</p><ul class=network-icon aria-hidden=true><li><a href=mailto:mingli@smail.nju.edu.cn><i class="fas fa-envelope"></i></a></li><li><a href="https://scholar.google.com/citations?user=0zLyGOUAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/AndyLiming target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><div class=media-body><h5 class=card-title><a href=/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/>Xueqian Jin(靳学乾)</a></h5><h6 class=card-subtitle>硕士(2020-2023)</h6><p class=card-text>简略介绍</p><ul class=network-icon aria-hidden=true><li><a href=mailto:jcboxq@126.com><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/jcboxq target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=/files/cv.pdf><i class="ai ai-cv"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 emAI, Nanjing University</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.fab8b449b814cc9f95b22fcf2e45f05b.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/zh/js/wowchemy.min.a6bdfe971dd13a950afddcdcb7287e53.js></script>
<script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>引用</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> 复制</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> 下载</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>