<!doctype html><html lang=zh-hans><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=description content="To meet the demand for powerful models for practical applications in real time, the focus of research on human actions has shifted from offline detection to online and real-time understanding, such as driver-assistance systems, surveillance analysis, and robot services. In recent years, with the development of video recording acquisition technology and deep learning, online action analysis has made significant progress. However, there is a lack of comprehensive online surveys for online human action detection. In this survey, we discuss two hot real-time concerns online action detection and action anticipation. Online action/activity detection aims to determine whether an action is currently taking place and what kind of action it is in untrimmed videos. Action anticipation aims to anticipate human actions under limited observation of videos. Online action detection and anticipation require accuracy and low latency of detection when the video is partly observed. We present a comprehensive study that includes the definition, taxonomy, comparison of state-of-the-art techniques, datasets, metrics, challenges, and future directions. We hope that it will provide readers with a detailed understanding of the topic and inspiration for new research directions."><link rel=alternate hreflang=zh-hans href=https://nju-ee.github.io/publication/online-human-action/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.6d31e9cd41a84afcca84bc68494717ec.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://nju-ee.github.io/publication/online-human-action/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="emAI @ NJU"><meta property="og:url" content="https://nju-ee.github.io/publication/online-human-action/"><meta property="og:title" content="Online human action detection and anticipation in videos: A survey | emAI @ NJU"><meta property="og:description" content="To meet the demand for powerful models for practical applications in real time, the focus of research on human actions has shifted from offline detection to online and real-time understanding, such as driver-assistance systems, surveillance analysis, and robot services. In recent years, with the development of video recording acquisition technology and deep learning, online action analysis has made significant progress. However, there is a lack of comprehensive online surveys for online human action detection. In this survey, we discuss two hot real-time concerns online action detection and action anticipation. Online action/activity detection aims to determine whether an action is currently taking place and what kind of action it is in untrimmed videos. Action anticipation aims to anticipate human actions under limited observation of videos. Online action detection and anticipation require accuracy and low latency of detection when the video is partly observed. We present a comprehensive study that includes the definition, taxonomy, comparison of state-of-the-art techniques, datasets, metrics, challenges, and future directions. We hope that it will provide readers with a detailed understanding of the topic and inspiration for new research directions."><meta property="og:image" content="https://nju-ee.github.io/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://nju-ee.github.io/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="zh-Hans"><meta property="article:published_time" content="2022-06-28T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-28T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://nju-ee.github.io/publication/online-human-action/"},"headline":"Online human action detection and anticipation in videos: A survey","datePublished":"2022-06-28T00:00:00Z","dateModified":"2022-06-28T00:00:00Z","author":{"@type":"Person","name":"胡雪娇"},"publisher":{"@type":"Organization","name":"emAI @ NJU","logo":{"@type":"ImageObject","url":"https://nju-ee.github.io/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_192x192_fill_lanczos_center_3.png"}},"description":"To meet the demand for powerful models for practical applications in real time, the focus of research on human actions has shifted from offline detection to online and real-time understanding, such as driver-assistance systems, surveillance analysis, and robot services. In recent years, with the development of video recording acquisition technology and deep learning, online action analysis has made significant progress. However, there is a lack of comprehensive online surveys for online human action detection. In this survey, we discuss two hot real-time concerns online action detection and action anticipation. Online action/activity detection aims to determine whether an action is currently taking place and what kind of action it is in untrimmed videos. Action anticipation aims to anticipate human actions under limited observation of videos. Online action detection and anticipation require accuracy and low latency of detection when the video is partly observed. We present a comprehensive study that includes the definition, taxonomy, comparison of state-of-the-art techniques, datasets, metrics, challenges, and future directions. We hope that it will provide readers with a detailed understanding of the topic and inspiration for new research directions."}</script><title>Online human action detection and anticipation in videos: A survey | emAI @ NJU</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=8bade7f9269ab6a550d4308d444ceea6><script src=/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>搜索</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=搜索... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=搜索...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>emAI @ NJU</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>emAI @ NJU</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/><span>首页</span></a></li><li class=nav-item><a class=nav-link href=/post><span>新闻</span></a></li><li class=nav-item><a class=nav-link href=/people><span>人员</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>研究方向</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/direction><span>总览</span></a>
<a class=dropdown-item href=/Autonomous_Driving_Research_Group.page/><span>自动驾驶感知</span></a></div></li><li class=nav-item><a class=nav-link href=/publication><span>论文出版</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>联系我们</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Online human action detection and anticipation in videos: A survey</h1><div class=article-metadata><div><span><a href=/author/xuejiao-hu%E8%83%A1%E9%9B%AA%E5%A8%87/>Xuejiao Hu(胡雪娇)</a></span>, <span><a href=/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/>Jingzhao Dai(戴京昭)</a></span>, <span><a href=/author/ming-li%E6%9D%8E%E6%98%8E/>Ming Li(李明)</a></span>, <span><a href=/author/%E5%BD%AD%E6%88%90%E7%A3%8A/>彭成磊</a></span>, <span><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></span>, <span><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/>Sidan Du(都思丹)</a></span></div><span class=article-date>六月 2022</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://doi.org/10.1016/j.neucom.2022.03.069 target=_blank rel=noopener>PDF</a></div></div><div class=article-container><h3>摘要</h3><p class=pub-abstract>To meet the demand for powerful models for practical applications in real time, the focus of research on human actions has shifted from offline detection to online and real-time understanding, such as driver-assistance systems, surveillance analysis, and robot services. In recent years, with the development of video recording acquisition technology and deep learning, online action analysis has made significant progress. However, there is a lack of comprehensive online surveys for online human action detection. In this survey, we discuss two hot real-time concerns online action detection and action anticipation. Online action/activity detection aims to determine whether an action is currently taking place and what kind of action it is in untrimmed videos. Action anticipation aims to anticipate human actions under limited observation of videos. Online action detection and anticipation require accuracy and low latency of detection when the video is partly observed. We present a comprehensive study that includes the definition, taxonomy, comparison of state-of-the-art techniques, datasets, metrics, challenges, and future directions. We hope that it will provide readers with a detailed understanding of the topic and inspiration for new research directions.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">类型</div><div class="col-12 col-md-9"><a href=/publication/#2>期刊文章</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">出版物</div><div class="col-12 col-md-9">In <em>Neurocomputing</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tag/source-themes/>Source Themes</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://nju-ee.github.io/publication/online-human-action/&text=Online%20human%20action%20detection%20and%20anticipation%20in%20videos:%20A%20survey" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://nju-ee.github.io/publication/online-human-action/&t=Online%20human%20action%20detection%20and%20anticipation%20in%20videos:%20A%20survey" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Online%20human%20action%20detection%20and%20anticipation%20in%20videos:%20A%20survey&body=https://nju-ee.github.io/publication/online-human-action/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://nju-ee.github.io/publication/online-human-action/&title=Online%20human%20action%20detection%20and%20anticipation%20in%20videos:%20A%20survey" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Online%20human%20action%20detection%20and%20anticipation%20in%20videos:%20A%20survey%20https://nju-ee.github.io/publication/online-human-action/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://nju-ee.github.io/publication/online-human-action/&title=Online%20human%20action%20detection%20and%20anticipation%20in%20videos:%20A%20survey" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><div class=media-body><h5 class=card-title><a href=/author/xuejiao-hu%E8%83%A1%E9%9B%AA%E5%A8%87/>Xuejiao Hu(胡雪娇)</a></h5><h6 class=card-subtitle>博士(2019-2024)</h6><p class=card-text>简略介绍</p><ul class=network-icon aria-hidden=true></ul></div></div><div class="media author-card content-widget-hr"><a href=/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/><img class="avatar mr-3 avatar-circle" src=/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/avatar_hu17ef6455205ece084fb1ea65c6faf3d4_95341_270x270_fill_q75_lanczos_center.jpg alt="Jingzhao Dai(戴京昭)"></a><div class=media-body><h5 class=card-title><a href=/author/jingzhao-dai%E6%88%B4%E4%BA%AC%E6%98%AD/>Jingzhao Dai(戴京昭)</a></h5><h6 class=card-subtitle>博士(2020-)</h6><p class=card-text>简略介绍</p><ul class=network-icon aria-hidden=true></ul></div></div><div class="media author-card content-widget-hr"><div class=media-body><h5 class=card-title><a href=/author/ming-li%E6%9D%8E%E6%98%8E/>Ming Li(李明)</a></h5><h6 class=card-subtitle>硕博连读(2017-2024)</h6><p class=card-text>简略介绍</p><ul class=network-icon aria-hidden=true><li><a href=mailto:mingli@smail.nju.edu.cn><i class="fas fa-envelope"></i></a></li><li><a href="https://scholar.google.com/citations?user=0zLyGOUAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/AndyLiming target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/><img class="avatar mr-3 avatar-circle" src=/author/yang-li%E6%9D%8E%E6%9D%A8/avatar_hu9edce20d0a5dfa53d0e6865a73c16c78_73185_270x270_fill_q75_lanczos_center.jpg alt="Yang Li(李杨)"></a><div class=media-body><h5 class=card-title><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></h5><h6 class=card-subtitle>副教授</h6><p class=card-text>简略介绍</p><ul class=network-icon aria-hidden=true><li><a href=mailto:yogo@nju.edu.cn><i class="fas fa-envelope"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/><img class="avatar mr-3 avatar-circle" src=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/avatar_hu3b0dc4d878f0896a0572ceec7b7182e7_46370_270x270_fill_q75_lanczos_center.jpg alt="Sidan Du(都思丹)"></a><div class=media-body><h5 class=card-title><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/>Sidan Du(都思丹)</a></h5><h6 class=card-subtitle>教授</h6><p class=card-text>简略介绍</p><ul class=network-icon aria-hidden=true><li><a href=mailto:coff128@nju.edu.cn><i class="fas fa-envelope"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 emAI, Nanjing University</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.fab8b449b814cc9f95b22fcf2e45f05b.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/zh/js/wowchemy.min.a6bdfe971dd13a950afddcdcb7287e53.js></script>
<script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>引用</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> 复制</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> 下载</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>