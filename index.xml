<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>emAI @ NJU</title><link>https://nju-ee.github.io/</link><atom:link href="https://nju-ee.github.io/index.xml" rel="self" type="application/rss+xml"/><description>emAI @ NJU</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh-Hans</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate><image><url>https://nju-ee.github.io/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_512x512_fill_lanczos_center_3.png</url><title>emAI @ NJU</title><link>https://nju-ee.github.io/</link></image><item><title>Example Event</title><link>https://nju-ee.github.io/event/example/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://nju-ee.github.io/event/example/</guid><description>&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Wowchemy&amp;rsquo;s &lt;a href="https://wowchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further event details, including page elements such as image galleries, can be added to the body of this page.&lt;/p></description></item><item><title>Distribution-aware Activity Boundary Representation for Online Detection of Action Start in Untrimmed Videos</title><link>https://nju-ee.github.io/publication/online-detection-of-action/</link><pubDate>Wed, 10 Jan 2024 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/online-detection-of-action/</guid><description/></item><item><title>我们的文章被 IEEE Signal Processing Letters 接收！</title><link>https://nju-ee.github.io/post/24-01-10-ieeespl-accept/</link><pubDate>Wed, 10 Jan 2024 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/post/24-01-10-ieeespl-accept/</guid><description>&lt;p>恭喜胡雪娇！&lt;/p>
&lt;p>&lt;a href="https://nju-ee.github.io/publication/online-detection-of-action/">论文详情&lt;/a>&lt;/p></description></item><item><title>CasOmniMVS: Cascade Omnidirectional Depth Estimation with Dynamic Spherical Sweeping</title><link>https://nju-ee.github.io/publication/casomnimvs/</link><pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/casomnimvs/</guid><description/></item><item><title>A Monocular Depth Estimation Method for Indoor-Outdoor Scenes Based on Vision Transformer</title><link>https://nju-ee.github.io/publication/monocular/</link><pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/monocular/</guid><description/></item><item><title>3D Associative Embedding: Multi-View 3D Human Pose Estimation in Crowded Scenes</title><link>https://nju-ee.github.io/publication/3d-associative-embedding/</link><pubDate>Thu, 27 Jul 2023 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/3d-associative-embedding/</guid><description/></item><item><title>MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras</title><link>https://nju-ee.github.io/publication/mode2022/</link><pubDate>Sun, 03 Jul 2022 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/mode2022/</guid><description/></item><item><title>我们的文章被 ECCV 2022 接收！</title><link>https://nju-ee.github.io/post/22-07-03-eccv-paper-accept/</link><pubDate>Sun, 03 Jul 2022 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/post/22-07-03-eccv-paper-accept/</guid><description>&lt;p>恭喜李明和靳学乾！&lt;/p>
&lt;p>&lt;a href="https://nju-ee.github.io/publication/mode2022/">论文详情&lt;/a>&lt;/p></description></item><item><title>Online human action detection and anticipation in videos: A survey</title><link>https://nju-ee.github.io/publication/online-human-action/</link><pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/online-human-action/</guid><description/></item><item><title> A Discussion of Optimization about Stereo Image Depth Estimation Based on Multi-baseline Trinocular Camera Model</title><link>https://nju-ee.github.io/publication/optimization-about-stereo-image-depth-estimation/</link><pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/optimization-about-stereo-image-depth-estimation/</guid><description/></item><item><title>A Shadow Detection Method for Retaining Key Objects in Complex Scenes</title><link>https://nju-ee.github.io/publication/shadow-detection/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/shadow-detection/</guid><description/></item><item><title> A Novel Skeleton-based Model with Spine for 3D Human Pose Estimation</title><link>https://nju-ee.github.io/publication/3d-human-pose-estimation/</link><pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/3d-human-pose-estimation/</guid><description/></item><item><title>Depth-based removal of thermal reflection with the light-field theory</title><link>https://nju-ee.github.io/publication/removal-of-thermal-reflection/</link><pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/removal-of-thermal-reflection/</guid><description/></item><item><title>The study of stereo matching optimization based on multi-baseline trinocular model</title><link>https://nju-ee.github.io/publication/study-of-stereo/</link><pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/study-of-stereo/</guid><description/></item><item><title>A Study of General Data Improvement for Large-Angle Head Pose Estimation</title><link>https://nju-ee.github.io/publication/large-angle-head-pose-estimation/</link><pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/large-angle-head-pose-estimation/</guid><description/></item><item><title>Omnidirectional stereo depth estimation based on spherical deep network</title><link>https://nju-ee.github.io/publication/omnidirectional-stereo-depth/</link><pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/omnidirectional-stereo-depth/</guid><description/></item><item><title>A Review on Quantitative Analyzing Axonal Transport of Mitochondria</title><link>https://nju-ee.github.io/publication/quantitative-analyzing/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/quantitative-analyzing/</guid><description/></item><item><title>Pixel-level Diabetic Retinopathy Lesion Detection Using Multi-scale Convolutional Neural Network</title><link>https://nju-ee.github.io/publication/diabetic-retinopathy-lesion-detection/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/publication/diabetic-retinopathy-lesion-detection/</guid><description/></item><item><title/><link>https://nju-ee.github.io/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/contact/</guid><description/></item><item><title/><link>https://nju-ee.github.io/people/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/people/</guid><description/></item><item><title>NJU-EE Autonomous Driving Research Group</title><link>https://nju-ee.github.io/direction/autonomous/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nju-ee.github.io/direction/autonomous/</guid><description>&lt;center>NJU-EE Autonomous Driving Research Group专注于以视觉为基础的感知算法&lt;center>
&lt;p>&lt;center>&lt;big>&lt;b> 深度估计 &lt;/center>&lt;/big>&lt;/b>&lt;/p> &lt;br> 我们研究了多摄像头系统的深度估算，以获取自动驾驶系统周围环境的结构信息。下面是一段演示视频。 &lt;br>&lt;br> &lt;video src="ground1.mp4" autoplay="autoplay" loop="loop" controls="controls">&lt;/video>
&lt;p>&lt;center>&lt;big>&lt;b> 感算一体 &lt;/center>&lt;/big>&lt;/b>&lt;/p> &lt;br> 我们提出了一种基于近传感器计算架构的全向深度估计系统。所提出的工作通过任务分区实现了负载平衡，同时通过特征投影和可学习编解码器降低了传输带宽。&lt;br>&lt;br> &lt;p>&lt;img src="Fig2.drawio.png" align="center" width="1250" /> &lt;/p>
&lt;p>&lt;center>&lt;big>&lt;b> 占据预测网络 &lt;/center>&lt;/big>&lt;/b>&lt;/p> &lt;br> 基于我们实验室的深度估计网络提供的深度信息，我们提出了一个基于圆柱体素的“Sketch-Coloring“框架。 下面是一段演示视频。实验结果表明，我们的“Sketch-Coloring”网络能显著提高三维感知性能，尤其是在邻近区域，这使我们的方法成为自动驾驶感知的一个有前途的解决方案。&lt;br>&lt;br> &lt;video src="occ_output_subtitle.mp4" loop="loop" controls="controls">&lt;/video>
&lt;p>&lt;center>&lt;big>&lt;b>&lt;a href="https://nju-ee.github.io/Autonomous_Driving_Research_Group.page" title="NJU-EE Autonomous Driving Research Group">更多相关资料&lt;/a>&lt;/center>&lt;/big>&lt;/b>&lt;/p></description></item></channel></rss>