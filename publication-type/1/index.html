<!doctype html><html lang=zh-hans><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=description content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><link rel=alternate hreflang=en href=https://nju-ee.github.io/en/publication-type/1/><link rel=alternate hreflang=zh-hans href=https://nju-ee.github.io/publication-type/1/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.6d31e9cd41a84afcca84bc68494717ec.css><link rel=alternate href=/publication-type/1/index.xml type=application/rss+xml title="emAI @ NJU"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://nju-ee.github.io/publication-type/1/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="emAI @ NJU"><meta property="og:url" content="https://nju-ee.github.io/publication-type/1/"><meta property="og:title" content="1 | emAI @ NJU"><meta property="og:description" content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><meta property="og:image" content="https://nju-ee.github.io/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://nju-ee.github.io/media/icon_huaf24208aa2613587e2d3d60da524e188_42332_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2023-11-17T00:00:00+00:00"><title>1 | emAI @ NJU</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>搜索</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=搜索... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=搜索...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>emAI @ NJU</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>emAI @ NJU</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/><span>首页</span></a></li><li class=nav-item><a class=nav-link href=/post><span>新闻</span></a></li><li class=nav-item><a class=nav-link href=/people><span>人员</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>研究方向</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/Autonomous_Driving_Research_Group.page/><span>Autonomous Driving Perception</span></a></div></li><li class=nav-item><a class=nav-link href=/publication><span>论文出版</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>联系我们</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>浅色</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>深色</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>自动</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-label=语言><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">中文 (简体)</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>中文 (简体)</span></div><a class=dropdown-item href=https://nju-ee.github.io/en/publication-type/1/><span>English</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>1</h1></div><div class=universal-wrapper><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/monocular/>A Monocular Depth Estimation Method for Indoor-Outdoor Scenes Based on Vision Transformer</a></div><a href=/publication/monocular/ class=summary-link><div class=article-style>In the field of computer vision, monocular depth estimation has garnered significant attention as a research direction. However, …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/jianghai-shuai%E5%B8%85%E6%B1%9F%E6%B5%B7/>Jianghai Shuai(帅江海)</a></span>, <span><a href=/author/ming-li%E6%9D%8E%E6%98%8E/>Ming Li(李明)</a></span>, <span><a href=/author/yongkang-feng%E5%86%AF%E6%B0%B8%E5%BA%B7/>Yongkang Feng(冯永康)</a></span>, <span><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></span>, <span><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/>Sidan Du(都思丹)</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10316039" target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/3d-associative-embedding/>3D Associative Embedding: Multi-View 3D Human Pose Estimation in Crowded Scenes</a></div><a href=/publication/3d-associative-embedding/ class=summary-link><div class=article-style>Most of the existing multi-view multi-person 3D human pose estimation methods predict the location of each joint of one target person …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/zhiyi-zhu%E6%9C%B1%E6%B2%BB%E4%BA%A6/>Zhiyi Zhu(朱治亦)</a></span>, <span><a href=/author/sheng-liu%E5%88%98%E6%99%9F/>Sheng Liu(刘晟)</a></span>, <span><a href=/author/jianghai-shuai%E5%B8%85%E6%B1%9F%E6%B5%B7/>Jianghai Shuai(帅江海)</a></span>, <span><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/>Sidan Du(都思丹)</a></span>, <span><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/pdf/10.1145/3603781.3603804 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/mode2022/>MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras</a></div><a href=/publication/mode2022/ class=summary-link><div class=article-style>In this paper, we propose a two-stage omnidirectional depth estimation framework with multi-view 360◦ cameras. The framework first …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/ming-li%E6%9D%8E%E6%98%8E/>Ming Li(李明)</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span><a href=/author/xueqian-jin%E9%9D%B3%E5%AD%A6%E4%B9%BE/>Xueqian Jin(靳学乾)</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://eprints.soton.ac.uk/352095/1/Cushen-IMV2013.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nju-ee/MODE-2022 target=_blank rel=noopener>代码</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=Fw-KR35UWgQ" target=_blank rel=noopener>视频</a></div></div><div class=ml-3><a href=/publication/mode2022/><img src=/publication/mode2022/featured_hu3d03a01dcc18bc5be0e67db3d8d209a6_214168_150x0_resize_q75_h2_lanczos.webp height=91 width=150 alt="MODE: Multi-view Omnidirectional Depth Estimation with 360° Cameras" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/optimization-about-stereo-image-depth-estimation/>A Discussion of Optimization about Stereo Image Depth Estimation Based on Multi-baseline Trinocular Camera Model</a></div><a href=/publication/optimization-about-stereo-image-depth-estimation/ class=summary-link><div class=article-style>The huge computational complexity and occlusion problems make stereo matching a major challenge. In this work, we use multi-baseline …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/hanrong-wang%E7%8E%8B%E6%B1%89%E9%95%95/>Hanrong Wang(王汉镕)</a></span>, <span><a href=/author/ming-li%E6%9D%8E%E6%98%8E/>Ming Li(李明)</a></span>, <span><a href=/author/jie-wang%E7%8E%8B%E6%9D%B0/>Jie Wang(王杰)</a></span>, <span><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></span>, <span><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/>Sidan Du(都思丹)</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CSCI54926.2021.00325 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/shadow-detection/>A Shadow Detection Method for Retaining Key Objects in Complex Scenes</a></div><a href=/publication/shadow-detection/ class=summary-link><div class=article-style>The existing shadow detection methods have achieved good results on standard shadow datasets such as SBU and UCF. However, in actual …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/jingyi-cao%E6%9B%B9%E9%9D%99%E6%80%A1/>Jingyi Cao(曹静怡)</a></span>, <span><a href=/author/%E5%BD%AD%E6%88%90%E7%A3%8A/>彭成磊</a></span>, <span><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></span>, <span><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/>Sidan Du(都思丹)</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/KST51265.2021.9415766 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/3d-human-pose-estimation/>A Novel Skeleton-based Model with Spine for 3D Human Pose Estimation</a></div><a href=/publication/3d-human-pose-estimation/ class=summary-link><div class=article-style>Kinematic chain model is widely adopted in 3D human pose estimation tasks while it cannot accurately describe the curvature of the …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/zhaoxu-li%E6%9D%8E%E5%85%86%E6%97%AD/>Zhaoxu Li(李兆旭)</a></span>, <span><a href=/author/sheng-liu%E5%88%98%E6%99%9F/>Sheng Liu(刘晟)</a></span>, <span><a href=/author/jue-bai%E7%99%BD%E7%8F%8F/>Jue Bai(白珏)</a></span>, <span><a href=/author/%E5%BD%AD%E6%88%90%E7%A3%8A/>彭成磊</a></span>, <span><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/abstract/document/9720811 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/large-angle-head-pose-estimation/>A Study of General Data Improvement for Large-Angle Head Pose Estimation</a></div><a href=/publication/large-angle-head-pose-estimation/ class=summary-link><div class=article-style>Predicting Euler angles of head pose using end-to-end CNN from a single RGB image is a popular application in recent years. However, …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/jue-bai%E7%99%BD%E7%8F%8F/>Jue Bai(白珏)</a></span>, <span><a href=/author/%E5%BD%AD%E6%88%90%E7%A3%8A/>彭成磊</a></span>, <span><a href=/author/zhaoxu-li%E6%9D%8E%E5%85%86%E6%97%AD/>Zhaoxu Li(李兆旭)</a></span>, <span><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/>Sidan Du(都思丹)</a></span>, <span><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-89131-2_18 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/diabetic-retinopathy-lesion-detection/>Pixel-level Diabetic Retinopathy Lesion Detection Using Multi-scale Convolutional Neural Network</a></div><a href=/publication/diabetic-retinopathy-lesion-detection/ class=summary-link><div class=article-style>Diabetic retinopathy (DR) is one of the leading causes of preventable blindness. It&rsquo;s urgent to develop reliable methods for auto …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/qi-li%E9%BB%8E%E7%90%AA/>Qi Li(黎琪)</a></span>, <span><a href=/author/ma-yazhen/>Ma Yazhen</a></span>, <span><a href=/author/%E5%BD%AD%E6%88%90%E7%A3%8A/>彭成磊</a></span>, <span><a href=/author/guo-bin/>Guo Bin</a></span>, <span><a href=/author/sidan-du%E9%83%BD%E6%80%9D%E4%B8%B9/>Sidan Du(都思丹)</a></span>, <span><a href=/author/yang-li%E6%9D%8E%E6%9D%A8/>Yang Li(李杨)</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/abstract/document/9391891 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 emAI, Nanjing University</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.fab8b449b814cc9f95b22fcf2e45f05b.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/zh/js/wowchemy.min.a6bdfe971dd13a950afddcdcb7287e53.js></script>
<script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>引用</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> 复制</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> 下载</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>